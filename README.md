# Transformer-in-Vision
Recent Transformer-based CV and related works. Welcome to comment/contribute!

Keep updated.

## Resource

- SCENIC: A JAX Library for Computer Vision Research and Beyond, [[Code]](https://github.com/google-research/scenic)

- V-L joint learning study (with good tables): [[METER]](https://arxiv.org/pdf/2111.02387.pdf), [[Kaleido-BERT]](https://arxiv.org/pdf/2103.16110.pdf)

- Attention is all you need, [[Paper]](https://arxiv.org/pdf/1706.03762.pdf)

- OpenAI CLIP [[Page]](https://openai.com/blog/clip/), [[Paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf), [[Code]](https://github.com/openai/CLIP), [[arXiv]](https://arxiv.org/pdf/2103.00020.pdf)

- OpenAI DALL·E [[Page]](https://openai.com/blog/dall-e/), [[Code]](https://github.com/openai/DALL-E), [[Paper]](https://arxiv.org/pdf/2102.12092.pdf)

- [huggingface/transformers](https://github.com/huggingface/transformers)

- [Kyubyong/transformer](https://github.com/Kyubyong/transformer), TF

- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch), Torch

- [krasserm/fairseq-image-captioning](https://github.com/krasserm/fairseq-image-captioning)

- [PyTorch Transformers Tutorials](https://github.com/abhimishra91/transformers-tutorials)

- [ictnlp/awesome-transformer](https://github.com/ictnlp/awesome-transformer)

- [basicv8vc/awesome-transformer](https://github.com/basicv8vc/awesome-transformer)

- [dk-liang/Awesome-Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer)

- [yuewang-cuhk/awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)

## Survey

- (arXiv 2022.02) Transformer for **Graphs**: An Overview from Architecture Perspective, [[Paper]](https://arxiv.org/pdf/2202.08455.pdf)

- (arXiv 2022.01) **Video** Transformers: A Survey, [[Paper]](https://arxiv.org/pdf/2201.05991.pdf)

- (arXiv 2021.11) ARE WE READY FOR A NEW PARADIGM SHIFT? A SURVEY ON VISUAL DEEP **MLP**, [[Paper]](https://arxiv.org/pdf/2111.04060.pdf)

- (arXiv 2021.11) A Survey of **Visual** Transformers, [[Paper]](https://arxiv.org/pdf/2111.06091.pdf)

- (arXiv 2021.09) Survey: Transformer based **Video-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2109.09920.pdf)

- (arXiv 2021.06) A Survey of **Transformers**, [[Paper]](https://arxiv.org/pdf/2106.04554.pdf)

- (arXiv 2021.06) **Attention** mechanisms and deep learning for machine vision: A survey of the state of the art, [[Paper]](https://arxiv.org/pdf/2106.07550.pdf)

- (arXiv 2021.06) **Pre-Trained Models**: Past, Present and Future, [[Paper]](https://arxiv.org/pdf/2106.07139.pdf)

- (arXiv 2021.05) Can Attention Enable **MLPs** To Catch Up With CNNs? [[Paper]](https://arxiv.org/pdf/2105.15078.pdf)

- (arXiv 2021.03) A Practical Survey on **Faster** and **Lighter** Transformers, [[Paper]](https://arxiv.org/pdf/2103.14636.pdf)

- (arXiv 2021.03) Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with **Language and Vision**, [[Paper]](https://arxiv.org/pdf/2103.04037.pdf)

- (arXiv 2021.01) A Survey on **Visual** Transformer, [[Paper]](https://arxiv.org/pdf/2012.12556.pdf)

- (arXiv 2020.9) **Efficient** Transformers: A Survey, [[Paper]](https://arxiv.org/pdf/2009.06732.pdf)

- (arXiv 2020.1) **Transformers in Vision**: A Survey, [[Paper]](https://arxiv.org/pdf/2101.01169.pdf)

## Recent Papers
<!-- ### 2022.02 -->
<!-- - (arXiv 2022.02) , [[Paper]]() -->

<!-- - (arXiv 2022.02) , [[Paper]]()

- (arXiv 2022.02) , [[Paper]]()

- (arXiv 2022.02) , [[Paper]]()

- (arXiv 2022.02) , [[Paper]]()

- (arXiv 2022.02) , [[Paper]]()

- (arXiv 2022.02) , [[Paper]]()

- (arXiv 2022.02) , [[Paper]]()

- (arXiv 2022.02) , [[Paper]]() -->

- (arXiv 2022.02) TransFollower: Long-Sequence Car-Following **Trajectory Prediction** through Transformer, [[Paper]](https://arxiv.org/pdf/2202.03183.pdf)

- (arXiv 2022.02) The devil is in the labels: **Semantic segmentation** from sentences, [[Paper]](https://arxiv.org/pdf/2202.02002.pdf)

- (arXiv 2022.02) Webly Supervised Concept Expansion for **General Purpose Vision Models**, [[Paper]](https://arxiv.org/pdf/2202.02317.pdf), [[Project]](https://prior.allenai.org/projects/gpv2)

- (arXiv 2022.02) VU-BERT: A UNIFIED FRAMEWORK FOR **VISUAL DIALOG**, [[Paper]](https://arxiv.org/pdf/2202.10787.pdf)

- (arXiv 2022.02) **UNIFYING** ARCHITECTURES, TASKS, AND MODALITIES THROUGH A SIMPLE SEQUENCE-TO-SEQUENCE LEARNING FRAMEWORK, [[Paper]](https://arxiv.org/pdf/2202.03052.pdf), [[Code]](https://github.com/OFA-Sys/OFA)

- (arXiv 2022.02) Transformers in Self-Supervised **Monocular Depth Estimation** with Unknown Camera Intrinsics, [[Paper]](https://arxiv.org/pdf/2202.03131.pdf)

- (arXiv 2022.02) TRANSDREAMER: **REINFORCEMENT LEARNING** WITH TRANSFORMER WORLD MODELS, [[Paper]](https://arxiv.org/pdf/2202.09481.pdf)

- (arXiv 2022.02) **Vision-Language** Pre-Training with Triple Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2202.10401.pdf), [[Code]](https://github.com/uta-smile/TCL)

- (arXiv 2022.02) Corrupted Image Modeling for **Self-Supervised** Visual **Pre-Training**, [[Paper]](https://arxiv.org/pdf/2202.03382.pdf)

- (arXiv 2022.02) BLIP: Bootstrapping Language-Image Pre-training for Unified **Vision-Language** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2201.12086.pdf), [[Code]](https://github.com/salesforce/BLIP)

- (arXiv 2022.02) DNNFuser: Generative Pre-Trained Transformer as a Generalized Mapper for Layer Fusion in **DNN Accelerators**, [[Paper]](https://arxiv.org/pdf/2201.11218.pdf)

- (arXiv 2022.02) Interactron: **Embodied** Adaptive **Object Detection**, [[Paper]](https://arxiv.org/pdf/2202.00660.pdf)

- (arXiv 2022.02) Local Feature Matching with Transformers for low-end devices **LoFTR** method adaptation approach, [[Paper]](https://arxiv.org/pdf/2202.00770.pdf), [[Code]](https://github.com/Kolkir/Coarse_LoFTR_TRT)

- (arXiv 2022.02) Pre-Trained Language Models for **Interactive Decision-Making**, [[Paper]](https://arxiv.org/pdf/2202.01771.pdf)

- (arXiv 2022.02) Can Transformers be Strong **Treatment Effect Estimators**?, [[Paper]](https://arxiv.org/pdf/2202.01336.pdf)

- (arXiv 2022.02) Improving **Sample Efficiency of Value** Based Models Using Attention and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2202.00710.pdf)

- (arXiv 2022.02) Detecting **Human-Object Interactions** with Object-Guided Cross-Modal Calibrated Semantics, [[Paper]](https://arxiv.org/pdf/2202.00259.pdf), [[Code]](https://github.com/JacobYuan7/OCN-HOI-Benchmark)

### 2022.01

- (arXiv 2022.01) O-ViT: Orthogonal Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12133.pdf)

- (arXiv 2022.01) DynaMixer: A Vision **MLP** Architecture with Dynamic Mixing, [[Paper]](https://arxiv.org/pdf/2201.12083.pdf)

- (arXiv 2022.01) VRT: A **Video Restoration** Transformer, [[Paper]](https://arxiv.org/pdf/2201.12288.pdf), [[Code]](https://github.com/JingyunLiang/VRT)

- (arXiv 2022.01) DAB-DETR: DYNAMIC **ANCHOR** BOXES ARE BETTER QUERIES FOR **DETR**, [[Paper]](https://arxiv.org/pdf/2201.12329.pdf), [[Code]](https://github.com/SlongLiu/DAB-DETR)

- (arXiv 2022.01) Plug-In Inversion: Model-Agnostic **Inversion** for Vision with Data Augmentations, [[Paper]](https://arxiv.org/pdf/2201.12961.pdf)

- (arXiv 2022.01) MVP: Multi-Stage **Vision-Language** Pre-Training via Multi-Level Semantic Alignment, [[Paper]](https://arxiv.org/pdf/2201.12596.pdf)

- (arXiv 2022.01) VC-GPT: Visual Conditioned GPT for End-to-End Generative **Vision-and-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2201.12723.pdf)

- (arXiv 2022.01) BOAT: Bilateral Local **Attention** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.13027.pdf)

- (arXiv 2022.01) GRAPH SELF-ATTENTION FOR LEARNING **GRAPH** REPRESENTATION WITH TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2201.12787.pdf)

- (arXiv 2022.01) Aggregating **Global** Features into **Local** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12903.pdf), [[Code]](https://github.com/krushi1992/MOA-transformer)

- (arXiv 2022.01) Transformer Module Networks for Systematic Generalization in **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2201.11316.pdf)

- (arXiv 2022.01) Generalised Image **Outpainting** with U-Transformer, [[Paper]](https://arxiv.org/pdf/2201.11403.pdf)

- (arXiv 2022.01) RelTR: Relation Transformer for **Scene Graph Generation**, [[Paper]](https://arxiv.org/pdf/2201.11460.pdf)

- (arXiv 2022.01) DocSegTr: An Instance-Level End-to-End **Document Image Segmentation** Transformer, [[Paper]](https://arxiv.org/pdf/2201.11438.pdf)

- (arXiv 2022.01) Pre-Trained **Language** Transformers are Universal **Image** Classifiers, [[Paper]](https://arxiv.org/pdf/2201.10182.pdf)

- (arXiv 2022.01) Explore and Match: End-to-End **Video Grounding** with Transformer, [[Paper]](https://arxiv.org/pdf/2201.10168.pdf)

- (arXiv 2022.01) TGFuse: An **Infrared** and **Visible Image Fusion** Approach Based on Transformer and Generative Adversarial Network, [[Paper]](https://arxiv.org/pdf/2201.10147.pdf)

- (arXiv 2022.01) ViT-HGR: Vision Transformer-based **Hand Gesture Recognition** from High Density Surface EMG Signals, [[Paper]](https://arxiv.org/pdf/2201.10060.pdf)

- (arXiv 2022.01) ShapeFormer: Transformer-based **Shape Completion** via Sparse Representation, [[Paper]](https://arxiv.org/pdf/2201.10326.pdf), [[Project]](https://shapeformer.github.io/)

- (arXiv 2022.01) **CONVOLUTIONAL** XFORMERS FOR VISION, [[Paper]](https://arxiv.org/pdf/2201.10271.pdf), [[Code]](https://github.com/pranavphoenix/CXV)

- (arXiv 2022.01) DocEnTr: An End-to-End **Document Image Enhancement** Transformer, [[Paper]](https://arxiv.org/pdf/2201.10252.pdf), [[Code]](https://github.com/dali92002/DocEnTR)

- (arXiv 2022.01) Zero-Shot **Sketch** Based **Image Retrieval** using Graph Transformer, [[Paper]](https://arxiv.org/pdf/2201.10185.pdf)

- (arXiv 2022.01) SA-**VQA**: Structured Alignment of Visual and Semantic Representations for Visual Question Answering, [[Paper]](https://arxiv.org/pdf/2201.10654.pdf)

- (arXiv 2022.01) DUAL-TASKS SIAMESE TRANSFORMER FRAMEWORK FOR **BUILDING DAMAGE ASSESSMENT**, [[Paper]](https://arxiv.org/pdf/2201.10953.pdf)

- (arXiv 2022.01) When **Shift Operation** Meets Vision Transformer: An Extremely Simple Alternative to **Attention** Mechanism, [[Paper]](https://arxiv.org/pdf/2201.10801.pdf), [[Code]](https://github.com/microsoft/SPACH)

- (arXiv 2022.01) Self-supervised 3D Semantic Representation Learning for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2201.10788.pdf)

- (arXiv 2022.01) **Training** Vision Transformers with Only 2040 Images, [[Paper]](https://arxiv.org/pdf/2201.10728.pdf)

- (arXiv 2022.01) Learning To Recognize **Procedural Activities** with Distant Supervision, [[Paper]](https://arxiv.org/pdf/2201.10990.pdf)

- (arXiv 2022.01) EVALUATING **LANGUAGE**-BIASED **IMAGE** CLASSIFICATION BASED ON SEMANTIC REPRESENTATIONS, [[Paper]](https://arxiv.org/pdf/2201.11014.pdf)

- (arXiv 2022.01) A Comprehensive Study of Vision Transformers on **Dense Prediction Tasks**, [[Paper]](https://arxiv.org/pdf/2201.08683.pdf)

- (arXiv 2022.01) UniFormer: Unifying **Convolution** and **Self-attention** for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2201.09450.pdf), [[Code]](https://github.com/Sense-X/UniFormer)

- (arXiv 2022.01) **Patches** Are All You Need? [[Paper]](https://arxiv.org/pdf/2201.09792.pdf), [[Code]](https://github.com/locuslab/convmixer)

- (arXiv 2022.01) Reading-strategy Inspired Visual Representation Learning for **Text-to-Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2201.09168.pdf)

- (arXiv 2022.01) LEARNING TO ACT WITH AFFORDANCE-AWARE **MULTIMODAL** NEURAL **SLAM**, [[Paper]](https://arxiv.org/pdf/2201.09862.pdf)

- (arXiv 2022.01) Visual Information Guided **Zero-Shot Paraphrase Generation**, [[Paper]](https://arxiv.org/pdf/2201.09107.pdf)

- (arXiv 2022.01) TerViT: An **Efficient** **Ternary** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.08050.pdf)

- (arXiv 2022.01) End-to-end Generative Pretraining for **Multimodal Video Captioning**, [[Paper]](https://arxiv.org/pdf/2201.08264.pdf)

- (arXiv 2022.01) OMNIVORE: A Single Model for **Many** Visual **Modalities**, [[Paper]](https://arxiv.org/pdf/2201.08377.pdf), [[Project]](https://facebookresearch.github.io/omnivore/)

- (arXiv 2022.01) MeMViT: Memory-Augmented Multiscale Vision Transformer for **Efficient Long-Term Video Recognition**, [[Paper]](https://arxiv.org/pdf/2201.08383.pdf)

- (arXiv 2022.01) The CLEAR Benchmark: **Continual LEArning** on Real-World Imagery, [[Paper]](https://arxiv.org/pdf/2201.06289.pdf), [[Project]](https://clear-benchmark.github.io/)

- (arXiv 2022.01) ProposalCLIP: **Unsupervised** Open-Category Object **Proposal** Generation via Exploiting **CLIP** Cues, [[Paper]](https://arxiv.org/pdf/2201.06696.pdf)

- (arXiv 2022.01) Cross-modal Contrastive Distillation for **Instructional Activity Anticipation**, [[Paper]](https://arxiv.org/pdf/2201.06734.pdf)

- (arXiv 2022.01) Transformers in Action: **Weakly Supervised Action Segmentation**, [[Paper]](https://arxiv.org/pdf/2201.05675.pdf)

- (arXiv 2022.01) VAQF: Fully Automatic **Software-hardware Co-design** Framework for **Low-bit** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.06618.pdf)

- (arXiv 2022.01) CLIP-TD: **CLIP** Targeted Distillation for **Vision-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2201.05729.pdf)

- (arXiv 2022.01) **Domain Adaptation** via Bidirectional Cross-Attention Transformer, [[Paper]](https://arxiv.org/pdf/2201.05887.pdf)

- (arXiv 2022.01) Continual Transformers: Redundancy-Free Attention for **Online Inference**, [[Paper]](https://arxiv.org/pdf/2201.06268.pdf)

- (arXiv 2022.01) **Motion Inbetweening** via Deep ∆-Interpolator, [[Paper]](https://arxiv.org/pdf/2201.06701.pdf)

- (arXiv 2022.01) RePre: Improving **Self-Supervised** Vision Transformer with Reconstructive Pre-training, [[Paper]](https://arxiv.org/pdf/2201.06857.pdf)

- (arXiv 2022.01) GTrans: Spatiotemporal Autoregressive Transformer with Graph Embeddings for **Nowcasting Extreme Events**, [[Paper]](https://arxiv.org/pdf/2201.06717.pdf)

- (arXiv 2022.01) TransFuse: A Unified Transformer-based **Image Fusion** Framework using Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2201.07451.pdf)

- (arXiv 2022.01) Q-ViT: Fully Differentiable **Quantization** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.07703.pdf)

- (arXiv 2022.01) Disentangled Latent Transformer for **Interpretable Monocular Height Estimation**, [[Paper]](https://arxiv.org/pdf/2201.06357.pdf), [[Project]](https://github.com/ShadowXZT/DLT-Height-Estimation.pytorch)

- (arXiv 2022.01) Poseur: Direct Human **Pose Regression** with Transformers*, [[Paper]](https://arxiv.org/pdf/2201.07412.pdf)

- (arXiv 2022.01) SWINUNET3D - A HIERARCHICAL ARCHITECTURE FOR DEEP **TRAFFIC PREDICTION** USING SHIFTED WINDOW TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2201.06390.pdf), [[Code]](https://github.com/bojesomo/Traffic4Cast2021-SwinUNet3D)

- (arXiv 2022.01) SWIN-POSE: SWIN TRANSFORMER BASED HUMAN **POSE ESTIMATION**, [[Paper]](https://arxiv.org/pdf/2201.07384.pdf)

- (arXiv 2022.01) Look Closer: Bridging Egocentric and Third-Person Views with Transformers for **Robotic Manipulation**, [[Paper]](https://arxiv.org/pdf/2201.07779.pdf), [[Project]](https://jangirrishabh.github.io/lookcloser/)

- (arXiv 2022.01) ViT2Hash: Unsupervised Information-Preserving **Hashing**, [[Paper]](https://arxiv.org/pdf/2201.05541.pdf)

- (arXiv 2022.01) LANGUAGE-DRIVEN **SEMANTIC SEGMENTATION**, [[Paper]](https://arxiv.org/pdf/2201.03546.pdf), [[Code]](https://github.com/isl-org/lang-seg)

- (arXiv 2022.01) **Pedestrian Detection**: Domain Generalization, CNNs, Transformers and Beyond, [[Paper]](https://arxiv.org/pdf/2201.03176.pdf), [[Code]](https://github.com/hasanirtiza/Pedestron)

- (arXiv 2022.01) ImageSubject: A Large-scale Dataset for **Subject Detection**, [[Paper]](https://arxiv.org/pdf/2201.03101.pdf)

- (arXiv 2022.01) **Detecting** Twenty-thousand Classes using Image-level Supervision, [[Paper]](https://arxiv.org/pdf/2201.02605.pdf), [[Code]](https://github.com/facebookresearch/Detic)

- (arXiv 2022.01) Generalized **Category Discovery**, [[Paper]](https://arxiv.org/pdf/2201.02609.pdf), [[Code]](https://github.com/sgvaze/generalized-category-discovery)

- (arXiv 2022.01) Video **Summarization** Based on **Video-text** Modelling, [[Paper]](https://arxiv.org/pdf/2201.02494.pdf)

- (arXiv 2022.01) Spatio-Temporal Tuples Transformer for **Skeleton-Based Action Recognition**, [[Paper]](https://arxiv.org/pdf/2201.02849.pdf), [[Code]](https://github.com/heleiqiu/STTFormer)

- (arXiv 2022.01) **QUADTREE ATTENTION** FOR VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2201.02767.pdf), [[Code]](https://github.com/Tangshitao/QuadtreeAttention)

- (arXiv 2022.01) A Comprehensive Empirical Study of **Vision-Language** Pre-trained Model for Supervised Cross-Modal Retrieval, [[Paper]](https://arxiv.org/pdf/2201.02772.pdf), [[Project]](https://github.com/zhixiongz/CLIP4CMR)

- (arXiv 2022.01) MERLOT Reserve: Neural Script Knowledge through **Vision and Language and Sound**, [[Paper]](https://arxiv.org/pdf/2201.02639.pdf), [[Project]](https://rowanzellers.com/merlotreserve)

- (arXiv 2022.01) On the Efficacy of Co-Attention Transformer Layers in **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2201.03965.pdf)

- (arXiv 2022.01) Pyramid Fusion Transformer for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2201.04019.pdf)

- (arXiv 2022.01) Multiview Transformers for **Video Recognition**, [[Paper]](https://arxiv.org/pdf/2201.04288.pdf)

- (arXiv 2022.01) HYPERTRANSFORMER: MODEL GENERATION FOR SUPERVISED AND SEMI-SUPERVISED **FEW-SHOT LEARNING**, [[Paper]](https://arxiv.org/pdf/2201.04182.pdf)

- (arXiv 2022.01) UNIFORMER: UNIFIED TRANSFORMER FOR **EFFICIENT SPATIOTEMPORAL** REPRESENTATION LEARNING, [[Paper]](https://arxiv.org/pdf/2201.04676.pdf), [[Code]](https://github.com/Sense-X/UniFormer)

- (arXiv 2022.01) BridgeFormer: Bridging **Video-text** Retrieval with Multiple Choice Questions, [[Paper]](https://arxiv.org/pdf/2201.04850.pdf), [[Project]](https://geyuying.github.io/MCQ.html)

- (arXiv 2022.01) TransVOD: End-to-end **Video Object Detection** with Spatial-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2201.05047.pdf)

- (arXiv 2022.01) **CLIP**-Event: Connecting Text and Images with **Event** Structures, [[Paper]](https://arxiv.org/pdf/2201.05078.pdf), [[Code]](https://github.com/limanling/clip-event)

- (arXiv 2022.01) Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular **Vision-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2201.04026.pdf)

- (arXiv 2022.01) Lawin Transformer: Improving **Semantic Segmentation** Transformer with Multi-Scale Representations via Large Window Attention, [[Paper]](https://arxiv.org/pdf/2201.01615.pdf), [[Code]](https://github.com/yan-hao-tian/lawin)

- (arXiv 2022.01) **Self-Training** **Vision Language** BERTs with a Unified Conditional Model, [[Paper]](https://arxiv.org/pdf/2201.02010.pdf)

- (arXiv 2022.01) TransVPR: Transformer-based TransVPR: Transformer-based place recognition with multi-level attention aggregation with multi-level attention aggregation, [[Paper]](https://arxiv.org/pdf/2201.02001.pdf)

- (arXiv 2022.01) Compact Bidirectional Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2201.01984.pdf), [[Code]](https://github.com/YuanEZhou/CBTrans)

- (arXiv 2022.01) Flow-Guided Sparse Transformer for **Video Deblurring**, [[Paper]](https://arxiv.org/pdf/2201.01893.pdf)

- (arXiv 2022.01) **Stochastic Layers** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.15111.pdf)

- (arXiv 2022.01) ERNIE-VILG: UNIFIED GENERATIVE PRE-TRAINING FOR **BIDIRECTIONAL VISION-LANGUAGE GENERATION**, [[Paper]](https://arxiv.org/pdf/2112.15283.pdf)

- (arXiv 2022.01) InverseMV: **Composing Piano Scores** with a Convolutional **Video-Music** Transformer, [[Paper]](https://arxiv.org/pdf/2112.15320.pdf), [[Code]](https://github.com/linchintung/VMT)

- (arXiv 2022.01) CSformer: Bridging Convolution and Transformer for **Compressive Sensing**, [[Paper]](https://arxiv.org/pdf/2112.15299.pdf)

- (arXiv 2022.01) Persformer: A Transformer Architecture for **Topological Machine Learning**, [[Paper]](https://arxiv.org/pdf/2112.15210.pdf)

- (arXiv 2022.01) Vision Transformer **Slimming**: Multi-Dimension Searching in Continuous Optimization Space, [[Paper]](https://arxiv.org/pdf/2201.00814.pdf)

- (arXiv 2022.01) Language as Queries for **Referring Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2201.00487.pdf), [[Code]](https://github.com/wjn922/ReferFormer)

- (arXiv 2022.01) PyramidTNT: Improved **Transformer-in-Transformer** Baselines with Pyramid Architecture, [[Paper]](https://arxiv.org/pdf/2201.00978.pdf), [[Code]](https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch)

- (arXiv 2022.01) A TRANSFORMER-BASED SIAMESE NETWORK FOR **CHANGE DETECTION**, [[Paper]](https://arxiv.org/pdf/2201.01293.pdf), [[Code]](https://github.com/wgcban/ChangeFormer)

- (arXiv 2022.01) Vision Transformer with **Deformable Attention**, [[Paper]](https://arxiv.org/pdf/2201.00520.pdf), [[Code]](https://github.com/LeapLabTHU/DAT)

- (arXiv 2022.01) Splicing ViT Features for **Semantic Appearance Transfer**, [[Paper]](https://arxiv.org/pdf/2201.00424.pdf), [[Project]](https://splice-vit.github.io/)

- (arXiv 2022.01) Detail-Preserving Transformer for **Light Field Image Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2201.00346.pdf), [[Code]](https://github.com/BITszwang/DPT)

### 2021.12

- (arXiv 2021.12) Multi-Dimensional **Model Compression** of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.00043.pdf)

- (arXiv 2021.12) Siamese Network with Interactive Transformer for **Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.13983.pdf), [[Code]](https://github.com/LANMNG/SITVOS)

- (arXiv 2021.12) Pale Transformer: A General Vision Transformer **Backbone** with Pale-Shaped **Atention**, [[Paper]](https://arxiv.org/pdf/2112.14000.pdf), [[Code]](https://github.com/BR-IDL/PaddleViT)

- (arXiv 2021.12) APRIL: Finding the Achilles’ Heel on **Privacy** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.14087.pdf)

- (arXiv 2021.12) Synchronized Audio-Visual Frames with Fractional Positional Encoding for Transformers in **Video-to-Text Translation**, [[Paper]](https://arxiv.org/pdf/2112.14088.pdf)

- (arXiv 2021.12) Does **CLIP** Benefit **Visual Question Answering** in the Medical Domain as Much as it Does in the General Domain?, [[Paper]](https://arxiv.org/pdf/2112.13906.pdf)

- (arXiv 2021.12) SPViT: Enabling **Faster** Vision Transformers via Soft Token Pruning, [[Paper]](https://arxiv.org/pdf/2112.13890.pdf)

- (arXiv 2021.12) A FISTFUL OF WORDS: LEARNING TRANSFERABLE VISUAL MODELS FROM BAG-OF-WORDS SUPERVISION, [[Paper]](https://arxiv.org/pdf/2112.13884.pdf)

- (arXiv 2021.12) StyleGAN-V: A Continuous **Video** Generator with the Price, Image Quality and Perks of **StyleGAN2**, [[Paper]](https://arxiv.org/pdf/2112.14683.pdf), [[Code]](https://universome.github.io/stylegan-v)

- (arXiv 2021.12) A Simple Baseline for **Zero-shot Semantic Segmentation** with Pre-trained **Vision-language** Model, [[Paper]](https://arxiv.org/pdf/2112.14757.pdf), [[Code]](https://github.com/MendelXu/zsseg.baseline)

- (arXiv 2021.12) Miti-DETR: Object **Detection** based on Transformers with Mitigatory Self-Attention Convergence, [[Paper]](https://arxiv.org/pdf/2112.13310.pdf)

- (arXiv 2021.12) SIMVIT: EXPLORING A SIMPLE VISION TRANSFORMER WITH **SLIDING WINDOWS**, [[Paper]](https://arxiv.org/pdf/2112.13085.pdf), [[Code]](https://github.com/ucasligang/SimViT)

- (arXiv 2021.12) SGTR: End-to-end **Scene Graph Generation** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.12970.pdf)

- (arXiv 2021.12) **Video** Joint Modelling Based on Hierarchical Transformer for **Co-summarization**, [[Paper]](https://arxiv.org/pdf/2112.13478.pdf)

- (arXiv 2021.12) Vision Transformer for **Small-Size Datasets**, [[Paper]](https://arxiv.org/pdf/2112.13492.pdf)

- (arXiv 2021.12) Learning **Generative** Vision Transformer with Energy-Based Latent Space for **Saliency Prediction**, [[Paper]](https://arxiv.org/pdf/2112.13528.pdf)

- (arXiv 2021.12) ViR: the Vision **Reservoir**, [[Paper]](https://arxiv.org/pdf/2112.13545.pdf)

- (arXiv 2021.12) SeMask: Semantically Masked Transformers for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.12782.pdf), [[Code]](https://github.com/Picsart-AI-Research/SeMask-Segmentation)

- (arXiv 2021.12) Open-Vocabulary Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.12143.pdf)

- (arXiv 2021.12) ELSA: Enhanced Local **Self-Attention** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.12786.pdf), [[Code]](https://github.com/damo-cv/ELSA)

- (arXiv 2021.12) LaTr: Layout-Aware Transformer for **Scene-Text** **VQA**, [[Paper]](https://arxiv.org/pdf/2112.12494.pdf)

- (arXiv 2021.12) **Multimodal Personality Recognition** using Cross-Attention Transformer and Behaviour Encoding, [[Paper]](https://arxiv.org/pdf/2112.12180.pdf)

- (arXiv 2021.12) Fine-grained **Multi-Modal Self-Supervised Learning**, [[Paper]](https://arxiv.org/pdf/2112.12182.pdf)

- (arXiv 2021.12) SLIP: Self-supervision meets **Language-Image** Pre-training, [[Paper]](https://arxiv.org/pdf/2112.12750.pdf), [[Code]](https://github.com/facebookresearch/SLIP)

- (arXiv 2021.12) CLEVR3D: Compositional Language and Elementary Visual Reasoning for **Question Answering** in **3D Real-World Scenes**, [[Paper]](https://arxiv.org/pdf/2112.11691.pdf)

- (arXiv 2021.12) MIA-Former: **Efficient** and **Robust** Vision Transformers via Multi-grained Input Adaptation, [[Paper]](https://arxiv.org/pdf/2112.11542.pdf)

- (arXiv 2021.12) iSegFormer: Interactive Image **Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.11325.pdf), [[Code]](https://github.com/qinliuliuqin/iSegFormer.git)

- (arXiv 2021.12) Contrastive Object **Detection** Using Knowledge Graph Embeddings, [[Paper]](https://arxiv.org/pdf/2112.11366.pdf)

- (arXiv 2021.12) RepMLPNet: Hierarchical Vision **MLP** with Re-parameterized **Locality**, [[Paper]](https://arxiv.org/pdf/2112.11081.pdf), [[Code]](https://github.com/DingXiaoH/RepMLP)

- (arXiv 2021.12) **Lite** Vision Transformer with Enhanced **Self-Attention**, [[Paper]](https://arxiv.org/pdf/2112.10809.pdf), [[Code]](https://github.com/Chenglin-Yang/LVT)

- (arXiv 2021.12) MPViT : Multi-Path Vision Transformer for **Dense Prediction**, [[Paper]](https://arxiv.org/pdf/2112.11010.pdf), [[Code]](https://git.io/MPViT)

- (arXiv 2021.12) SOIT: **Segmenting** Objects with Instance-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2112.11037.pdf), [[Code]](https://github.com/yuxiaodongHRI/SOIT)

- (arXiv 2021.12) Learned Queries for Efficient Local **Attention**, [[Paper]](https://arxiv.org/pdf/2112.11435.pdf), [[Code]](https://github.com/moabarar/qna)

- (arXiv 2021.12) On **Efficient** Transformer and Image Pre-training for **Low-level** Vision, [[Paper]](https://arxiv.org/pdf/2112.10175.pdf), [[Code]](https://github.com/fenglinglwb/EDT)

- (arXiv 2021.12) LOCFORMER: Enabling Transformers to Perform **Temporal Moment Localization** on Long Untrimmed Videos With a Feature Sampling Approach, [[Paper]](https://arxiv.org/pdf/2112.10066.pdf)

- (arXiv 2021.12) Tell me what you see: A zero-shot **action recognition** method based on natural language descriptions, [[Paper]](https://arxiv.org/pdf/2112.09976.pdf), [[Code]](https://github.com/valterlej/zsarcap)

- (arXiv 2021.12) Pre-Training Transformers for **Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2112.09965.pdf)

- (arXiv 2021.12) ScanQA: 3D Question Answering for Spatial Scene Understanding, [[Paper]](https://arxiv.org/pdf/2112.10482.pdf)

- (arXiv 2021.12) Are Large-scale Datasets Necessary for Self-Supervised Pre-training? [[Paper]](https://arxiv.org/pdf/2112.10740.pdf)

- (arXiv 2021.12) StyleSwin: Transformer-based GAN for High-resolution **Image Generation**, [[Paper]](https://arxiv.org/pdf/2112.10762.pdf), [[Code]](https://github.com/microsoft/StyleSwin)

- (arXiv 2021.12) Mask2Former for **Video Instance Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.10764.pdf), [[Code]](https://github.com/facebookresearch/Mask2Former)

- (arXiv 2021.12) GLIDE: Towards Photorealistic **Image Generation** and **Editing** with **Text**-Guided Diffusion Models, [[Paper]](https://arxiv.org/pdf/2112.10741.pdf), [[Code]](https://github.com/openai/glide-text2im)

- (arXiv 2021.12) **Efficient** Visual **Tracking** with Exemplar Transformers, [[Paper]](https://arxiv.org/pdf/2112.09686.pdf), [[Code]](https://github.com/visionml/pytracking)

- (arXiv 2021.12) **Neuromorphic Camera Denoising** using Graph Neural Network-driven Transformers, [[Paper]](https://arxiv.org/pdf/2112.09685.pdf)

- (arXiv 2021.12) Align and Prompt: **Video-and-Language** Pre-training with Entity Prompts, [[Paper]](https://arxiv.org/pdf/2112.09583.pdf), [[Code]](https://github.com/salesforce/ALPRO)

- (arXiv 2021.12) DATA **EFFICIENT** **LANGUAGE-SUPERVISED ZEROSHOT RECOGNITION** WITH OPTIMAL TRANSPORT DISTILLATION, [[Paper]](https://arxiv.org/pdf/2112.09445.pdf)

- (arXiv 2021.12) SiamTrans: Zero-Shot Multi-Frame **Image Restoration** with Pre-Trained Siamese Transformers, [[Paper]](https://arxiv.org/pdf/2112.09426.pdf)

- (arXiv 2021.12) Full Transformer Framework for Robust **Point Cloud Registration** with Deep Information Interaction, [[Paper]](https://arxiv.org/pdf/2112.09385.pdf), [[Code]](https://github.com/CGuangyan-BIT/DIT)

- (arXiv 2021.12) ZeroVL: A Strong Baseline for Aligning **Vision-Language** Representations with **Limited Resources**, [[Paper]](https://arxiv.org/pdf/2112.09331.pdf)

- (arXiv 2021.12) Towards End-to-End **Image Compression and Analysis** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.09300.pdf)

- (arXiv 2021.12) How to **augment** your ViTs? Consistency loss and StyleAug, a random style transfer augmentation, [[Paper]](https://arxiv.org/pdf/2112.09260.pdf)

- (arXiv 2021.12) Learning to Prompt for **Continual Learning**, [[Paper]](https://arxiv.org/pdf/2112.08654.pdf), [[Code]](https://github.com/google-research/l2p)

- (arXiv 2021.12) Distilled Dual-Encoder Model for **Vision-Language** Understanding, [[Paper]](https://arxiv.org/pdf/2112.08723.pdf), [[Code]](https://github.com/kugwzk/Distilled-DualEncoder)

- (arXiv 2021.12) Dense Video **Captioning** Using Unsupervised Semantic Information, [[Paper]](https://arxiv.org/pdf/2112.08455.pdf), [[Code]](https://github.com/valterlej/dvcusi)

- (arXiv 2021.12) Looking Outside the Box to **Ground Language** in **3D** Scenes, [[Paper]](https://arxiv.org/pdf/2112.08879.pdf), [[Code]](https://github.com/nickgkan/beauty_detr)

- (arXiv 2021.12) Region**CLIP**: Region-based **Language-Image** Pretraining, [[Paper]](https://arxiv.org/pdf/2112.09106.pdf), [[Code]](https://github.com/microsoft/RegionCLIP)

- (arXiv 2021.12) DProST: **6-DoF Object Pose Estimation** Using Space Carving and Dynamic Projective Spatial Transformer, [[Paper]](https://arxiv.org/pdf/2112.08775.pdf)

- (arXiv 2021.12) Masked Feature Prediction for **Self-Supervised** Visual Pre-Training, [[Paper]](https://arxiv.org/pdf/2112.09133.pdf)

- (arXiv 2021.12) SGEITL: Scene Graph Enhanced Image-Text Learning for **Visual Commonsense Reasoning**, [[Paper]](https://arxiv.org/pdf/2112.08587.pdf)

- (arXiv 2021.12) TransZero++: Cross Attribute-Guided Transformer for **Zero-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2112.08643.pdf), [[Code]](https://github.com/shiming-chen/TransZero_pp)

- (arXiv 2021.12) Vision Transformer Based **Video Hashing Retrieval** for Tracing the Source of Fake Videos, [[Paper]](https://arxiv.org/pdf/2112.08117.pdf), [[Code]](https://github.com/lajlksdf/vtl)

- (arXiv 2021.12) Co-training Transformer with Videos and Images Improves **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2112.07175.pdf)

- (arXiv 2021.12) QAHOI: Query-Based Anchors for **Human-Object Interaction** Detection, [[Paper]](https://arxiv.org/pdf/2112.08647.pdf), [[Code]](https://github.com/cjw2021/QAHOI)

- (arXiv 2021.12) AdaViT: Adaptive Tokens for **Efficient** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.07658.pdf)

- (arXiv 2021.12) **CLIP**-Lite: Information **Efficient** Visual Representation Learning from Textual Annotations, [[Paper]](https://arxiv.org/pdf/2112.07133.pdf)

- (arXiv 2021.12) Towards a Unified Foundation Model: Jointly Pre-Training Transformers on **Unpaired Images and Text**, [[Paper]](https://arxiv.org/pdf/2112.07074.pdf)

- (arXiv 2021.12) Deep ViT Features as Dense Visual **Descriptors**, [[Paper]](https://arxiv.org/pdf/2112.05814.pdf), [[Project]](https://dino-vit-features.github.io/)

- (arXiv 2021.12) Geometry-Contrastive Transformer for Generalized 3D Pose Transfer, [[Paper]](https://arxiv.org/pdf/2112.07374.pdf), [[Code]](https://github.com/mikecheninoulu/CGT)

- (arXiv 2021.12) Temporal Transformer Networks with Self-Supervision for **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2112.07338.pdf)

- (arXiv 2021.12) COMPOSER: Compositional Learning of **Group Activity** in Videos, [[Paper]](https://arxiv.org/pdf/2112.05892.pdf)

- (arXiv 2021.12) Short and Long Range Relation Based Spatio-Temporal Transformer for **Micro-Expression Recognition**, [[Paper]](https://arxiv.org/pdf/2112.05851.pdf)

- (arXiv 2021.12) Improving and Diagnosing Knowledge-Based **Visual Question Answering** via Entity Enhanced Knowledge Injection, [[Paper]](https://arxiv.org/pdf/2112.06888.pdf)

- (arXiv 2021.12) SVIP: **Sequence VerIfication** for Procedures in **Videos**, [[Paper]](https://arxiv.org/pdf/2112.06447.pdf)

- (arXiv 2021.12) Improving Vision Transformers for **Incremental Learning**, [[Paper]](https://arxiv.org/pdf/2112.06103.pdf)

- (arXiv 2021.12) VL-ADAPTER: Parameter-Efficient Transfer Learning for **Vision-and-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2112.06825.pdf), [[Code]](https://github.com/ylsung/VL_adapter)

- (arXiv 2021.12) Embracing Single Stride **3D Object Detector** with Sparse Transformer, [[Paper]](https://arxiv.org/pdf/2112.06375.pdf), [[Code]](https://github.com/TuSimple/SST)

- (arXiv 2021.12) PartGlot: Learning **Shape Part Segmentation** from Language Reference Games, [[Paper]](https://arxiv.org/pdf/2112.06390.pdf)

- (arXiv 2021.12) **Pedestrian Trajectory Prediction** via Spatial Interaction Transformer Network, [[Paper]](https://arxiv.org/pdf/2112.06624.pdf)

- (arXiv 2021.12) LEARNING SEMANTIC-ALIGNED FEATURE REPRESENTATION FOR **TEXT-BASED PERSON SEARCH**, [[Paper]](https://arxiv.org/pdf/2112.06714.pdf)

- (arXiv 2021.12) L-Verse: Bidirectional **Generation** Between **Image** and **Text**, [[Paper]](https://arxiv.org/pdf/2111.11133.pdf)

- (arXiv 2021.12) **SELF-ATTENTION** DOES NOT NEED O(n^2) MEMORY, [[Paper]](https://arxiv.org/pdf/2112.05682.pdf)

- (arXiv 2021.12) Are Vision Transformers **Robust** to Patch Perturbations? [[Paper]](https://arxiv.org/pdf/2111.10659.pdf)

- (arXiv 2021.12) Mesa: A **Memory-saving Training** Framework for Transformers, [[Paper]](https://arxiv.org/pdf/2111.11124.pdf), [[Code]](https://github.com/zhuang-group/Mesa)

- (arXiv 2021.12) Injecting Semantic Concepts into End-to-End Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2112.05230.pdf)

- (arXiv 2021.12) MAGMA – Multimodal **Augmentation** of **Generative** Models through Adapter-based Finetuning, [[Paper]](https://arxiv.org/pdf/2112.05253.pdf)

- (arXiv 2021.12) LCTR: On Awakening the Local Continuity of Transformer for **Weakly Supervised Object Localization**, [[Paper]](https://arxiv.org/pdf/2112.05291.pdf)

- (arXiv 2021.12) FaceFormer: **Speech-Driven 3D Facial Animation** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.05329.pdf)

- (arXiv 2021.12) Rethinking the Two-Stage Framework for **Grounded Situation Recognition**, [[Paper]](https://arxiv.org/pdf/2112.05375.pdf), [[Code]](https://github.com/kellyiss/SituFormer)

- (arXiv 2021.12) **CLIP**2Style**GAN**: Unsupervised Extraction of StyleGAN Edit Directions, [[Paper]](https://arxiv.org/pdf/2112.05219.pdf)

- (arXiv 2021.12) Couplformer: Rethinking Vision Transformer with Coupling **Attention** Map, [[Paper]](https://arxiv.org/pdf/2112.05425.pdf)

- (arXiv 2021.12) Unified Multimodal Pre-training and Prompt-based Tuning for **Vision-Language** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2112.05587.pdf)

- (arXiv 2021.12) Visual Transformers with Primal Object Queries for **Multi-Label Image Classification**, [[Paper]](https://arxiv.org/pdf/2112.05485.pdf)

- (arXiv 2021.12) Colossal-AI: A Unified Deep Learning System For **Large-Scale Parallel Training**, [[Paper]](https://arxiv.org/pdf/2110.14883.pdf), [[Code]](https://github.com/hpcaitech/ColossalAI)

- (arXiv 2021.12) MS-TCT: Multi-Scale Temporal ConvTransformer for **Action Detection**, [[Paper]](https://arxiv.org/pdf/2112.03902.pdf)

- (arXiv 2021.12) Grounded **Language-Image** Pre-training, [[Paper]](https://arxiv.org/pdf/2112.03857.pdf), [[Code]](https://github.com/microsoft/GLIP)

- (arXiv 2021.12) U^2-Former: A Nested U-shaped Transformer for **Image Restoration**, [[Paper]](https://arxiv.org/pdf/2112.02279.pdf)

- (arXiv 2021.12) ADAPTIVE CHANNEL ENCODING TRANSFORMER FOR **POINT CLOUD** ANALYSIS, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2112/2112.02507.pdf)

- (arXiv 2021.12) Pose-guided Feature Disentangling for Occluded Person **Re-identification** Based on Transformer, [[Paper]](https://arxiv.org/pdf/2112.02466.pdf), [[Code]](https://github.com/WangTaoAs/PFD_Net)

- (arXiv 2021.12) VT-CLIP: Enhancing **Vision-Language** Models with Visual-guided Texts, [[Paper]](https://arxiv.org/pdf/2112.02399.pdf)

- (arXiv 2021.12) PointCLIP: **Point Cloud** Understanding by **CLIP**, [[Paper]](https://arxiv.org/pdf/2112.02413.pdf), [[Code]](https://github.com/ZrrSkywalker/PointCLIP)

- (arXiv 2021.12) Learning **Tracking** Representations via Dual-Branch Fully Transformer Networks, [[Paper]](https://arxiv.org/pdf/2112.02571.pdf), [[Code]](https://github.com/phiphiphi31/DualTFR)

- (arXiv 2021.12) DYNAMIC TOKEN **NORMALIZATION** IMPROVES VISION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2112.02624.pdf), [[Code]](https://github.com/wqshao126/DTN)

- (arXiv 2021.12) PTTR: Relational 3D **Point Cloud Object Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.02857.pdf), [[Code]](https://github.com/Jasonkks/PTTR)

- (arXiv 2021.12) GETAM: Gradient-weighted Element-wise Transformer Attention Map for **Weakly-supervised Semantic segmentation**, [[Paper]](https://arxiv.org/pdf/2112.02841.pdf)

- (arXiv 2021.12) **Text2Mesh**: Text-Driven Neural Stylization for Meshes, [[Paper]](https://arxiv.org/pdf/2112.03221.pdf), [[Project]](https://threedle.github.io/text2mesh/)

- (arXiv 2021.12) LMR-CBT: Learning Modality-fused Representations with CB-Transformer for Multimodal **Emotion Recognition** from Unaligned Multimodal Sequences, [[Paper]](https://arxiv.org/pdf/2112.01697.pdf)

- (arXiv 2021.12) Make A Long Image Short: Adaptive **Token** Length for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.01686.pdf)

- (arXiv 2021.12) FuseDream: Training-Free **Text-to-Image Generation** with Improved **CLIP**+GAN Space Optimization, [[Paper]](https://arxiv.org/pdf/2112.01573.pdf), [[Code]](https://github.com/gnobitab/FuseDream)

- (arXiv 2021.12) TransZero: Attribute-guided Transformer for **Zero-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2112.01683.pdf), [[Code]](https://github.com/shiming-chen/TransZero)

- (arXiv 2021.12) Learning Generalizable **Vision-Tactile** Robotic **Grasping** Strategy for Deformable Objects via Transformer, [[Paper]](https://arxiv.org/pdf/2112.06374.pdf), [[Code]](https://github.com/GTLIDAR/DeformableObjectsGrasping.git)

- (arXiv 2021.12) Hformer: Hybrid CNN-Transformer for **Fringe Order Prediction** in Phase Unwrapping of Fringe Projection, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2112/2112.06759.pdf)

- (arXiv 2021.12) Pre-training and Fine-tuning Transformers for **fMRI Prediction** Tasks, [[Paper]](https://arxiv.org/pdf/2112.05761.pdf)

- (arXiv 2021.12) Transformer based **trajectory prediction**, [[Paper]](https://arxiv.org/pdf/2112.04350.pdf)

- (arXiv 2021.12) Evaluating Transformers for Lightweight **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2111.09641.pdf)

- (arXiv 2021.12) Contextualized Spatio-Temporal **Contrastive Learning** with Self-Supervision, [[Paper]](https://arxiv.org/pdf/2112.05181.pdf)

- (arXiv 2021.12) CMA-CLIP: Cross-Modality Attention **CLIP** for **Image-Text** Classification, [[Paper]](https://arxiv.org/pdf/2112.03562.pdf)

- (arXiv 2021.12) **Bootstrapping** ViTs: Towards Liberating Vision Transformers from Pre-training, [[Paper]](https://arxiv.org/pdf/2112.03552.pdf)

- (arXiv 2021.12) Decision-based Black-box **Attack** Against Vision Transformers via Patch-wise Adversarial Removal, [[Paper]](https://arxiv.org/pdf/2112.03492.pdf), [[Code]](https://github.com/shiyuchengTJU/PAR)

- (arXiv 2021.12) DoodleFormer: Creative **Sketch Drawing** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.03258.pdf)

- (arXiv 2021.12) Creating **Multimodal Interactive Agents** with Imitation and Self-Supervised Learning, [[Paper]](https://arxiv.org/pdf/2112.03763.pdf)

- (arXiv 2021.12) **AUDIO-VISUAL** SYNCHRONISATION IN THE WILD, [[Paper]](https://arxiv.org/pdf/2112.04432.pdf), [[Project]](https://www.robots.ox.ac.uk/~vgg/research/avs)

- (arXiv 2021.12) **Classification**-Then-**Grounding**: Reformulating **Video** Scene Graphs as Temporal Bipartite Graphs, [[Paper]](https://arxiv.org/pdf/2112.04222.pdf)

- (arXiv 2021.12) Garment4D: **Garment Reconstruction** from Point Cloud Sequences, [[Paper]](https://arxiv.org/pdf/2112.04159.pdf), [[Code]](https://github.com/hongfz16/Garment4D)

- (arXiv 2021.12) Locally Shifted **Attention****** With Early Global Integration, [[Paper]](https://arxiv.org/pdf/2112.05080.pdf), [[Code]](https://github.com/shellysheynin/Locally-SAG-Transformer)

- (arXiv 2021.12) BLT: Bidirectional Layout Transformer for Controllable **Layout Generation**, [[Paper]](https://arxiv.org/pdf/2112.05112.pdf)

- (arXiv 2021.12) PE-former: **Pose Estimation** Transformer, [[Paper]](https://arxiv.org/pdf/2112.04981.pdf), [[Project]](https://www.ics.forth.gr/hccv/)

- (arXiv 2021.12) Hair**CLIP**: **Design** Your Hair by Text and Reference Image, [[Paper]](https://arxiv.org/pdf/2112.05142.pdf), [[Project]](https://github.com/wty-ustc/HairCLIP)

- (arXiv 2021.12) **CLIP**-**NeRF**: Text-and-Image Driven Manipulation of Neural Radiance Fields, [[Paper]](https://arxiv.org/pdf/2112.05139.pdf), [[Code]](https://cassiepython.github.io/clipnerf/)

- (arXiv 2021.12) A Bilingual, Open World Video Text **Dataset** and End-to-end **Video Text Spotter** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.04888.pdf), [[Code]](https://github.com/weijiawu/TransVTSpotter), [[Dataset]](https://github.com/weijiawu/BOVText-Benchmark)

- (arXiv 2021.12) DualFormer: Local-Global Stratified Transformer for **Efficient Video Recognition**, [[Paper]](https://arxiv.org/pdf/2112.04674.pdf), [[Code]](https://github.com/sail-sg/dualformer)

- (arXiv 2021.12) Recurrent Glimpse-based Decoder for **Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.04632.pdf), [[Code]](https://github.com/zhechen/Deformable-DETR-REGO)

- (arXiv 2021.12) Fast **Point** Transformer, [[Paper]](https://arxiv.org/pdf/2112.04702.pdf)

- (arXiv 2021.12) Assistive Tele-op: Leveraging Transformers to **Collect Robotic Task Demonstrations**, [[Paper]](https://arxiv.org/pdf/2112.05129.pdf), [[Project]](https://sites.google.com/view/assistive-teleop)

- (arXiv 2021.12) Cross-Modality Fusion Transformer for **Multispectral Object Detection**, [[Paper]](https://arxiv.org/pdf/2111.00273.pdf)

- (arXiv 2021.12) PatchFormer: An **Efficient** **Point** Transformer with Patch Attention, [[Paper]](https://arxiv.org/pdf/2111.00207.pdf)

- (arXiv 2021.12) Transformer-Based Approach for Joint **Handwriting** and **Named Entity Recognition** in Historical documents, [[Paper]](https://arxiv.org/pdf/2112.04189.pdf)

- (arXiv 2021.12) **MLP** Architectures for **Vision-and-Language** Modeling: An Empirical Study, [[Paper]](https://arxiv.org/pdf/2112.04453.pdf), [[Code]](https://github.com/easonnie/mlp-vil)

- (arXiv 2021.12) Everything at Once – Multi-modal Fusion Transformer for **Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2112.04446.pdf)

- (arXiv 2021.12) Prompting **Visual-Language** Models for Efficient Video Understanding, [[Paper]](https://arxiv.org/pdf/2112.04478.pdf), [[Project]](https://ju-chen.github.io/efficient-prompt/)

- (arXiv 2021.12) FLAVA: A Foundational **Language And Vision** Alignment Model, [[Paper]](https://arxiv.org/pdf/2112.04482.pdf)

- (arXiv 2021.12) Embedding Arithmetic for **Text-driven Image Transformation**, [[Paper]](https://arxiv.org/pdf/2112.03162.pdf)

- (arXiv 2021.12) LAVT: Language-Aware Vision Transformer for **Referring Image Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.02244.pdf)

- (arXiv 2021.12) Look at What I’m Doing: Self-Supervised **Spatial Grounding** of Narrations in Instructional Videos, [[Paper]](https://arxiv.org/pdf/2110.10596.pdf), [[Project]](https://cs-people.bu.edu/rxtan/projects/grounding_narrations/)

- (arXiv 2021.12) Uni-Perceiver: Pre-training Unified Architecture for **Generic Perception** for **Zero-shot and Few-shot** Tasks, [[Paper]](https://arxiv.org/pdf/2112.01522.pdf)

- (arXiv 2021.12) Dense**CLIP**: Language-Guided **Dense** Prediction with Context-Aware Prompting, [[Paper]](https://arxiv.org/pdf/2112.01518.pdf), [[Code]](https://github.com/raoyongming/DenseCLIP)

- (arXiv 2021.12) Self-supervised **Video** Transformer, [[Paper]](https://arxiv.org/pdf/2112.01514.pdf), [[Code]](https://git.io/J1juJ)

- (arXiv 2021.12) OW-DETR: **Open-world Detection** Transformer, [[Paper]](https://arxiv.org/pdf/2112.01513.pdf)

- (arXiv 2021.12) Zero-Shot **Text-Guided Object Generation** with Dream Fields, [[Paper]](https://arxiv.org/pdf/2112.01455.pdf), [[Project]](https://ajayj.com/dreamfields)

- (arXiv 2021.12) **Video-Text** Pre-training with Learned Regions, [[Paper]](https://arxiv.org/pdf/2112.01194.pdf), [[Code]](https://github.com/ruiyan1995/Region_Learner)

- (arXiv 2021.12) MTFNet: Mutual-Transformer Fusion Network for **RGB-D Salient Object Detection**, [[Paper]](https://arxiv.org/pdf/2112.01177.pdf)

- (arXiv 2021.12) TCTN: A 3D-Temporal Convolutional Transformer Network for **Spatiotemporal** Predictive Learning, [[Paper]](https://arxiv.org/pdf/2112.01085.pdf)

- (arXiv 2021.12) DenseCLIP: Extract Free **Dense** Labels from **CLIP**, [[Paper]](https://arxiv.org/pdf/2112.01071.pdf)

- (arXiv 2021.12) TransMEF: A Transformer-Based **Multi-Exposure Image Fusion** Framework using Self-Supervised Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2112.01030.pdf)

- (arXiv 2021.12) SwinTrack: A Simple and Strong Baseline for Transformer **Tracking**, [[Paper]](https://arxiv.org/pdf/2112.00995.pdf), [[Code]](https://github.com/LitingLin/SwinTrack)

- (arXiv 2021.12) Object-Centric Unsupervised Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2112.00969.pdf)

- (arXiv 2021.12) Vision Pair Learning: An **Efficient** Training Framework for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2112.00965.pdf)

- (arXiv 2021.12) Visual-Semantic Transformer for **Scene Text Recognition**, [[Paper]](https://arxiv.org/pdf/2112.00948.pdf)

- (arXiv 2021.12) Differentiable **Spatial Planning** using Transformers, [[Paper]](https://arxiv.org/pdf/2112.01010.pdf), [[Project]](https://devendrachaplot.github.io/projects/spatial-planning-transformers)

- (arXiv 2021.12) Improved **Multiscale** Vision Transformers for **Classification** and **Detection**, [[Paper]](https://arxiv.org/pdf/2112.01526.pdf)

- (arXiv 2021.12) Masked-attention Mask Transformer for Universal Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.01527.pdf), [[Code]](https://bowenc0221.github.io/mask2former)

- (arXiv 2021.12) BEVT: BERT Pretraining of **Video** Transformers, [[Paper]](https://arxiv.org/pdf/2112.01529.pdf)

- (arXiv 2021.12) **Human-Object Interaction Detection** via Weak Supervision, [[Paper]](https://arxiv.org/pdf/2112.00492.pdf)

- (arXiv 2021.12) Learning Transformer Features for **Image Quality Assessment**, [[Paper]](https://arxiv.org/pdf/2112.00485.pdf)

- (arXiv 2021.12) **CLIP**styler: **Image Style Transfer** with a Single Text Condition, [[Paper]](https://arxiv.org/pdf/2112.00374.pdf)

- (arXiv 2021.12) **Multi-View Stereo** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.00336.pdf)

- (arXiv 2021.12) VoRTX: **Volumetric 3D Reconstruction** With Transformers for Voxelwise View Selection and Fusion, [[Paper]](https://arxiv.org/pdf/2112.00236.pdf), [[Code]](https://noahstier.github.io/vortx)

- (arXiv 2021.12) Object-aware **Video-language** Pre-training for Retrieval, [[Paper]](https://arxiv.org/pdf/2112.00656.pdf), [[Code]](https://github.com/FingerRec/OA-Transformer)

### 2021.11

- (arXiv 2021.11) Multi-modal Transformers Excel at **Class-agnostic** Object **Detection**, [[Paper]](https://arxiv.org/pdf/2111.11430.pdf), [[Code]](https://git.io/J1HPY)

- (arXiv 2021.11) Predict, Prevent, and Evaluate: Disentangled **Text-Driven Image Manipulation** Empowered by Pre-Trained Vision-Language Model, [[Paper]](https://arxiv.org/pdf/2111.13333.pdf)

- (arXiv 2021.11) NomMer: Nominate Synergistic Context in Vision Transformer for **Visual Recognition**, [[Paper]](https://arxiv.org/pdf/2111.12994.pdf), [[Code]](https://github.com/NomMer1125/NomMer)

- (arXiv 2021.11) PolyViT: **Co-training** Vision Transformers on **Images**, **Videos** and **Audio**, [[Paper]](https://arxiv.org/pdf/2111.12993.pdf)

- (arXiv 2021.11) SWAT: Spatial Structure Within and Among Tokens, [[Paper]](https://arxiv.org/pdf/2111.13677.pdf)

- (arXiv 2021.11) ADAPTIVE **FOURIER** NEURAL OPERATORS: **EFFICIENT** TOKEN MIXERS FOR TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2111.13587.pdf)

- (arXiv 2021.11) DyTox: Transformers for **Continual Learning** with DYnamic TOken eXpansion, [[Paper]](https://arxiv.org/pdf/2111.11326.pdf), [[Code]](https://github.com/arthurdouillard/dytox)

- (arXiv 2021.11) DABS: A Domain-Agnostic **Benchmark** for **Self-Supervised** Learning, [[Paper]](https://arxiv.org/pdf/2111.12062.pdf), [[Code]](https://github.com/alextamkin/dabs)

- (arXiv 2021.11) Ice hockey **player identification** via transformers, [[Paper]](https://arxiv.org/pdf/2111.11535.pdf)

- (arXiv 2021.11) DBIA: Data-free Backdoor Injection **Attack** against Transformer Networks, [[Paper]](https://arxiv.org/pdf/2111.11870.pdf), [[Code]](https://anonymous.4open.science/r/DBIA-825D)

- (arXiv 2021.11) Sparse Fusion for **Multimodal** Transformers, [[Paper]](https://arxiv.org/pdf/2111.11992.pdf)

- (arXiv 2021.11) PhysFormer: **Facial Video-based Physiological Measurement** with Temporal Difference Transformer, [[Paper]](https://arxiv.org/pdf/2111.12082.pdf), [[Code]](https://github.com/ZitongYu/PhysFormer)

- (arXiv 2021.11) Self-Supervised Pre-Training for Transformer-Based Person **Re-Identification**, [[Paper]](https://arxiv.org/pdf/2111.12084.pdf), [[Code]](https://github.com/michuanhaohao/TransReID-SSL)

- (arXiv 2021.11) DISCRETE REPRESENTATIONS STRENGTHEN VISION TRANSFORMER **ROBUSTNESS**, [[Paper]](https://arxiv.org/pdf/2111.10493.pdf)

- (arXiv 2021.11) TRAVLR: Now You See It, Now You Don’t! Evaluating Cross-Modal Transfer of **Visio-Linguistic Reasoning**, [[Paper]](https://arxiv.org/pdf/2111.10756.pdf)

- (arXiv 2021.11) Crossing the Format Boundary of Text and Boxes: Towards Unified **Vision-Language** Modeling, [[Paper]](https://arxiv.org/pdf/2111.12085.pdf)

- (arXiv 2021.11) **Semi-Supervised** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.11067.pdf)

- (arXiv 2021.11) CpT: Convolutional Point Transformer for 3D **Point Cloud** Processing, [[Paper]](https://arxiv.org/pdf/2111.10866.pdf)

- (arXiv 2021.11) ZERO-SHOT CERTIFIED **DEFENSE** AGAINST **ADVERSARIAL** PATCHES WITH VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2111.10481.pdf)

- (arXiv 2021.11) PointMixer: MLP-Mixer for **Point Cloud** Understanding, [[Paper]](https://arxiv.org/pdf/2111.11187.pdf)

- (arXiv 2021.11) **MetaFormer** is Actually What You Need for Vision, [[Paper]](https://arxiv.org/pdf/2111.11418.pdf), [[Code]](https://github.com/sail-sg/poolformer)

- (arXiv 2021.11) Florence: A New **Foundation Model** for Computer Vision, [[Paper]](https://arxiv.org/pdf/2111.11432.pdf)

- (arXiv 2021.11) Benchmarking **Detection Transfer Learning** with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.11429.pdf)

- (arXiv 2021.11) Learning to **Compose Visual Relations**, [[Paper]](https://arxiv.org/pdf/2111.09297.pdf), [[Project]](https://composevisualrelations.github.io/)

- (arXiv 2021.11) REFERENCE-BASED **MAGNETIC RESONANCE IMAGE RECONSTRUCTION** USING TEXTURE TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2111.09492.pdf)

- (arXiv 2021.11) Induce, Edit, Retrieve: Language Grounded Multimodal Schema for **Instructional Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2111.09276.pdf)

- (arXiv 2021.11) **Swin Transformer V2**: Scaling Up Capacity and Resolution, [[Paper]](https://arxiv.org/pdf/2111.09883.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer)

- (arXiv 2021.11) SimMIM: A Simple Framework for **Masked Image Modeling**, [[Paper]](https://arxiv.org/pdf/2111.09886.pdf), [[Code]](https://github.com/microsoft/SimMIM)

- (arXiv 2021.11) Restormer: Efficient Transformer for **High-Resolution Image Restoration**, [[Paper]](https://arxiv.org/pdf/2111.09881.pdf), [[Code]](https://github.com/swz30/Restormer)

- (arXiv 2021.11) Simple but Effective: **CLIP** Embeddings for **Embodied AI**, [[Paper]](https://arxiv.org/pdf/2111.09888.pdf)

- (arXiv 2021.11) ClipCap: CLIP Prefix for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2111.09734.pdf), [[Code]](https://github.com/rmokady/CLIP_prefix_caption)

- (arXiv 2021.11) TransMix: Attend to **Mix** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.09833.pdf), [[Code]](https://github.com/Beckschen/TransMix)

- (arXiv 2021.11) TRIG: Transformer-Based **Text Recognizer** with Initial Embedding Guidance, [[Paper]](https://arxiv.org/pdf/2111.08314.pdf)

- (arXiv 2021.11) Multi-Grained **Vision Language** Pre-Training: Aligning Texts with Visual Concepts, [[Paper]](https://arxiv.org/pdf/2111.08276.pdf), [[Code]](https://github.com/zengyan-97/X-VLM)

- (arXiv 2021.11) Explainable Semantic Space by **Grounding Language to Vision** with Cross-Modal Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2111.07180.pdf), [[Code]](https://github.com/yizhen-zhang/VG-Bert)

- (arXiv 2021.11) Semantically Grounded Object Matching for Robust **Robotic Scene Rearrangement**, [[Paper]](https://arxiv.org/pdf/2111.07975.pdf), [[Code]](https://github.com/applied-ai-lab/object_matching)

- (arXiv 2021.11) **Tracking** People with **3D** Representations, [[Paper]](https://arxiv.org/pdf/2111.07868.pdf), [[Code]](https://brjathu.github.io/T3DP)

- (arXiv 2021.11) LiT: Zero-Shot Transfer with Locked-**image** **Text** **Tuning**, [[Paper]](https://arxiv.org/pdf/2111.07991.pdf)

- (arXiv 2021.11) FILIP: FINE-GRAINED INTERACTIVE **LANGUAGE-IMAGE** PRE-TRAINING, [[Paper]](https://arxiv.org/pdf/2111.07783.pdf)

- (arXiv 2021.11) Graph Relation Transformer: Incorporating **pairwise object features** into the Transformer architecture, [[Paper]](https://arxiv.org/pdf/2111.06075.pdf), [[Code]](https://github.com/derikclive/transformers)

- (arXiv 2021.11) **Attention** Approximates Sparse Distributed Memory, [[Paper]](https://arxiv.org/pdf/2111.05498.pdf)

- (arXiv 2021.11) SLICED **RECURSIVE** TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2111.05297.pdf), [[Code]](https://github.com/szq0214/SReT)

- (arXiv 2021.11) HYBRID **BYOL-VIT**: EFFICIENT APPROACH TO DEAL WITH **SMALL DATASETS**, [[Paper]](https://arxiv.org/pdf/2111.04845.pdf)

- (arXiv 2021.11) Tip-Adapter: Training-free **CLIP**-Adapter for Better **Vision-Language** Modeling, [[Paper]](https://arxiv.org/pdf/2111.03930.pdf), [[Code]](https://github.com/gaopengcuhk/Tip-Adapter)

- (arXiv 2021.11) Improving Visual Quality of **Image Synthesis** by A Token-based Generator with Transformers, [[Paper]](https://arxiv.org/pdf/2111.03481.pdf)

- (arXiv 2021.11) Style**CLIP**Draw: Coupling Content and Style in **Text-to-Drawing Synthesis**, [[Paper]](https://arxiv.org/pdf/2111.03133.pdf), [[Code]](https://github.com/pschaldenbrand/StyleCLIPDraw)

- (arXiv 2021.11) Revisiting **spatio-temporal** layouts for **compositional action recognition**, [[Paper]](https://arxiv.org/pdf/2111.01936.pdf), [[Code]](https://github.com/gorjanradevski/revisiting-spatial-temporal-layouts)

- (arXiv 2021.11) PatchGame: Learning to Signal Mid-level Patches in **Referential Games**, [[Paper]](https://arxiv.org/pdf/2111.01785.pdf), [[Code]](https://kampta.github.io/patch-game)

- (arXiv 2021.11) CAN VISION TRANSFORMERS PERFORM **CONVOLUTION**? [[Paper]](https://arxiv.org/pdf/2111.01353.pdf)

- (arXiv 2021.11) Livestock Monitoring with Transformer, [[Paper]](https://arxiv.org/pdf/2111.00801.pdf)

- (arXiv 2021.11) With a Little Help from my Temporal Context: Multimodal **Egocentric Action Recognition**, [[Paper]](https://arxiv.org/pdf/2111.01024.pdf), [[Code]](https://github.com/ekazakos/MTCN)

- (arXiv 2021.11) IconQA: A New Benchmark for Abstract Diagram Understanding and **Visual Language Reasoning**, [[Paper]](https://arxiv.org/pdf/2110.13214.pdf), [[Project]](https://iconqa.github.io/)

- (arXiv 2021.11) BoxeR: **Box-Attention** for 2D and 3D Transformers, [[Paper]](https://arxiv.org/pdf/2111.13087.pdf)

- (arXiv 2021.11) VLDeformer: **Vision-Language** Decomposed Transformer for Fast **Cross-Modal Retrieval**, [[Paper]](https://arxiv.org/pdf/2110.11338.pdf)

- (arXiv 2021.11) Multi-Person **3D Motion Prediction** with Multi-Range Transformers, [[Paper]](https://arxiv.org/pdf/2111.12073.pdf), [[Code]](https://jiashunwang.github.io/MRT/)

- (arXiv 2021.11) Scene Representation Transformer: Geometry-Free **Novel View Synthesis** Through Set-Latent Scene Representations, [[Paper]](https://arxiv.org/pdf/2111.13152.pdf), [[Project]](https://srt-paper.github.io/)

- (arXiv 2021.11) **Global Interaction Modelling** in Vision Transformer via Super Tokens, [[Paper]](https://arxiv.org/pdf/2111.13156.pdf)

- (arXiv 2021.11) ML-Decoder: Scalable and Versatile **Classification Head**, [[Paper]](https://arxiv.org/pdf/2111.12933.pdf), [[Code]](https://github.com/Alibaba-MIIL/ML_Decoder)

- (arXiv 2021.11) Exploiting Both Domain-specific and Invariant Knowledge via a Win-win Transformer for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2111.12941.pdf)

- (arXiv 2021.11) SWINBERT: End-to-End Transformers with Sparse Attention for **Video Captioning**, [[Paper]](https://arxiv.org/pdf/2111.13196.pdf)

- (arXiv 2021.11) Amortized Prompt: Lightweight Fine-Tuning for **CLIP** in **Domain Generalization**, [[Paper]](https://arxiv.org/pdf/2111.12853.pdf)

- (arXiv 2021.11) Universal Captioner: Long-Tail **Vision-and-Language** Model Training through Content-Style Separation, [[Paper]](https://arxiv.org/pdf/2111.12727.pdf)

- (arXiv 2021.11) **Sparse** is Enough in Scaling Transformers, [[Paper]](https://arxiv.org/pdf/2111.12763.pdf)

- (arXiv 2021.11) An implementation of the “**Guess who**?” game using CLIP, [[Paper]](https://arxiv.org/pdf/2112.00599.pdf), [[Code]](https://github.com/ArnauDIMAI/CLIP-GuessWho)

- (arXiv 2021.11) HEAT: Holistic Edge Attention Transformer for **Structured Reconstruction**, [[Paper]](https://arxiv.org/pdf/2111.15143.pdf)

- (arXiv 2021.11) A Unified **Pruning** Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.15127.pdf)

- (arXiv 2021.11) Pyramid **Adversarial Training** Improves ViT Performance, [[Paper]](https://arxiv.org/pdf/2111.15121.pdf)

- (arXiv 2021.11) AssistSR: Affordance-centric Question-driven **Video Segment Retrieval**, [[Paper]](https://arxiv.org/pdf/2111.15050.pdf), [[Code & Data]](https://github.com/StanLei52/AQVSR)

- (arXiv 2021.11) DAFormer: Improving Network Architectures and Training Strategies for **Domain-Adaptive Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2111.14887.pdf), [[Code]](https://github.com/lhoyer/DAFormer)

- (arXiv 2021.11) , [[Paper]](https://arxiv.org/pdf/2111.14887.pdf)

- (arXiv 2021.11) AdaViT: Adaptive Vision Transformers for **Efficient** Image Recognition, [[Paper]](https://arxiv.org/pdf/2111.15668.pdf)

- (arXiv 2021.11) ATS: Adaptive Token Sampling For **Efficient** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.15667.pdf)

- (arXiv 2021.11) **CLIP** Meets Video Captioners: Attribute-Aware Representation Learning Promotes Accurate **Captioning**, [[Paper]](https://arxiv.org/pdf/2111.15162.pdf)

- (arXiv 2021.11) CRIS: **CLIP**-Driven Referring Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2111.15174.pdf)

- (arXiv 2021.11) Shunted **Self-Attention** via Multi-Scale Token Aggregation, [[Paper]](https://arxiv.org/pdf/2111.15193.pdf), [[Code]](https://github.com/OliverRensu/Shunted-Transformer)

- (arXiv 2021.11) MC-SSL0.0: Towards Multi-Concept **Self-Supervised** Learning, [[Paper]](https://arxiv.org/pdf/2111.15340.pdf)

- (arXiv 2021.11) TransWeather: Transformer-based **Restoration of Images** Degraded by Adverse Weather Conditions, [[Paper]](https://arxiv.org/pdf/2111.14813.pdf), [[Code]](https://github.com/jeya-maria-jose/TransWeather)

- (arXiv 2021.11) Searching the **Search Space** of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.14725.pdf), [[Code]](https://github.com/microsoft/Cream)

- (arXiv 2021.11) TransMVSNet: Global Context-aware **Multi-view Stereo** Network with Transformers, [[Paper]](https://arxiv.org/pdf/2111.14600.pdf), [[Code]](https://github.com/MegviiRobot/TransMVSNet)

- (arXiv 2021.11) **Recurrent** Vision Transformer for Solving Visual **Reasoning** Problems, [[Paper]]()

- (arXiv 2021.11) **Video Frame Interpolation** Transformer, [[Paper]](https://arxiv.org/pdf/2111.13817.pdf)

- (arXiv 2021.11) FQ-ViT: Fully **Quantized** Vision Transformer without Retraining, [[Paper]](https://arxiv.org/pdf/2111.13824.pdf), [[Code]](https://github.com/linyang-zhh/FQ-ViT)

- (arXiv 2021.11) LAFITE : Towards Language-Free Training for **Text-to-Image Generation**, [[Paper]](https://arxiv.org/pdf/2111.13792.pdf)

- (arXiv 2021.11) SPARSE DETR: **EFFICIENT** END-TO-END OBJECT **DETECTION** WITH LEARNABLE SPARSITY, [[Paper]](https://arxiv.org/pdf/2111.14330.pdf), [[Code]](https://github.com/kakaobrain/sparse-detr)

- (arXiv 2021.11) End-to-End **Referring Video Object Segmentation** with Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2111.14821.pdf), [[Code]](https://github.com/mttr2021/MTTR)

- (arXiv 2021.11) Point-BERT: Pre-training 3D **Point Cloud** Transformers with Masked Point Modeling, [[Paper]](https://arxiv.org/pdf/2111.14819.pdf), [[Code]](https://github.com/lulutang0608/Point-BERT)

- (arXiv 2021.11) Zero-Shot **Image-to-Text Generation** for Visual-Semantic Arithmetic, [[Paper]](https://arxiv.org/pdf/2111.14447.pdf), [[Code]](https://github.com/YoadTew/zero-shot-image-to-text)

- (arXiv 2021.11) Blended Diffusion for **Text-driven Editing** of **Natural Images**, [[Paper]](https://arxiv.org/pdf/2111.14818.pdf), [[Code]](https://github.com/omriav/blended-diffusion)

- (arXiv 2021.11) Mask Transfiner for High-Quality **Instance Segmentation**, [[Paper]](https://arxiv.org/pdf/2111.13673.pdf), [[Code]](http://vis.xyz/pub/transfiner)

- (arXiv 2021.11) MHFormer: Multi-Hypothesis Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2111.12707.pdf), [[Code]](https://github.com/Vegetebird/MHFormer)

- (arXiv 2021.11) PeCo: Perceptual Codebook for **BERT Pre-training** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12710.pdf), [[Code]](https://github.com/microsoft/PeCo)

- (arXiv 2021.11) Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast **High-Resolution Image Generation** from Vector-Quantized Codes, [[Paper]](https://arxiv.org/pdf/2111.12701.pdf), [[COde]](https://github.com/samb-t/unleashing-transformers)

- (arXiv 2021.11) Towards Tokenized **Human Dynamics** Representation, [[Paper]](https://arxiv.org/pdf/2111.11433.pdf), [[Code]](https://github.com/likenneth/acton)

- (arXiv 2021.11) **Self-slimmed** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.12624.pdf)

- (arXiv 2021.11) VIOLET: End-to-End **Video-Language** Transformers with Masked Visual-token Modeling, [[Paper]](https://arxiv.org/pdf/2111.12681.pdf), [[Code]](https://github.com/tsujuifu/pytorch_violet)

- (arXiv 2021.11) A Lightweight Graph Transformer Network for **Human Mesh Reconstruction** from 2D Human Pose, [[Paper]](https://arxiv.org/pdf/2111.12696.pdf)

- (arXiv 2021.11) MorphMLP: A Self-Attention Free, **MLP**-Like Backbone for Image and Video, [[Paper]](https://arxiv.org/pdf/2111.12527.pdf)

- (arXiv 2021.11) Octree Transformer: Autoregressive **3D Shape Generation** on Hierarchically Structured Sequences, [[Paper]](https://arxiv.org/pdf/2111.12480.pdf)

- (arXiv 2021.11) Hierarchical Modular Network for **Video Captioning**, [[Paper]](https://arxiv.org/pdf/2111.12476.pdf)

- (arXiv 2021.11) NU¨WA: **Visual Synthesis Pre-training** for Neural visUal World creAtion, [[Paper]](https://arxiv.org/pdf/2111.12417.pdf), [[Code]](https://github.com/microsoft/NUWA)

- (arXiv 2021.11) An Image Patch is a Wave: Phase-Aware Vision **MLP**, [[Paper]](https://arxiv.org/pdf/2111.12294.pdf)

- (arXiv 2021.11) PTQ4ViT: Post-Training **Quantization** Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12293.pdf)

- (arXiv 2021.11) PU-Transformer: **Point Cloud Upsampling** Transformer, [[Paper]](https://arxiv.org/pdf/2111.12242.pdf)

- (arXiv 2021.11) Scaling Up **Vision-Language Pre-training** for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2111.12233.pdf)

- (arXiv 2021.11) Cerberus Transformer: Joint **Semantic, Affordance and Attribute Parsing**, [[Paper]](https://arxiv.org/pdf/2111.12608.pdf), [[Code]](https://github.com/OPEN-AIR-SUN/Cerberus)

- (arXiv 2021.11) Efficient **Video** Transformers with Spatial-Temporal Token Selection, [[Paper]](https://arxiv.org/pdf/2111.11591.pdf)

- (arXiv 2021.11) RedCaps: Web-curated **image-text data** created by the people, for the people, [[Paper]](https://arxiv.org/pdf/2111.11431.pdf), [[Project]](https://redcaps.xyz/)

- (arXiv 2021.11) EMScore: Evaluating **Video Captioning** via Coarse-Grained and Fine-Grained Embedding Matching, [[Paper]](https://arxiv.org/pdf/2111.08919.pdf), [[Code]](https://github.com/ShiYaya/emscore)

- (arXiv 2021.11) Compositional Transformers for **Scene Generation**, [[Paper]](https://arxiv.org/pdf/2111.08960.pdf), [[Code]](https://github.com/dorarad/gansformer)

- (arXiv 2021.11) Vis-TOP: Visual Transformer **Overlay Processor**, [[Paper]](https://arxiv.org/pdf/2110.10957.pdf)

- (arXiv 2021.11) **Grounded Situation Recognition** with Transformers, [[Paper]](https://arxiv.org/pdf/2111.10135.pdf), [[Code]](https://github.com/jhcho99/gsrtr)

- (arXiv 2021.11) Rethinking **Query, Key, and Value** Embedding in Vision Transformer under **Tiny Model** Constraints, [[Paper]](https://arxiv.org/pdf/2111.10017.pdf)

- (arXiv 2021.11) UFO: A UniFied TransfOrmer for **Vision-Language** Representation Learning, [[Paper]](https://arxiv.org/pdf/2111.10023.pdf)

- (arXiv 2021.11) Advancing High-Resolution **Video-Language** Representation with Large-Scale Video Transcriptions, [[Paper]](https://arxiv.org/pdf/2111.10337.pdf)

- (arXiv 2021.11) Combined Scaling for **Zero-shot Transfer Learning**, [[Paper]](https://arxiv.org/pdf/2111.10050.pdf)

- (arXiv 2021.11) Simple but Effective: **CLIP** Embeddings for **Embodied AI**, [[Paper]](https://arxiv.org/pdf/2111.09888.pdf)

- (arXiv 2021.11) Improved **Robustness** of Vision Transformer via PreLayerNorm in Patch Embedding, [[Paper]](https://arxiv.org/pdf/2111.08413.pdf)

- (arXiv 2021.11) IBOT: **IMAGE BERT PRE-TRAINING** WITH ONLINE TOKENIZER, [[Paper]](https://arxiv.org/pdf/2111.07832.pdf), [[Code]](https://github.com/bytedance/ibot)

- (arXiv 2021.11) **Masked Autoencoders** Are Scalable Vision Learners, [[Paper]](https://arxiv.org/pdf/2111.06377.pdf)

- (arXiv 2021.11) Mask-guided Spectral-wise Transformer for Efficient **Hyperspectral Image Reconstruction**, [[Paper]](https://arxiv.org/pdf/2111.07910.pdf)

- (arXiv 2021.11) Are Transformers More **Robust** Than CNNs?, [[Paper]](https://arxiv.org/pdf/2111.05464.pdf), [[Code]](https://github.com/ytongbai/ViTs-vs-CNNs)

- (arXiv 2021.11) CLIP2TV: An Empirical Study on Transformer-based Methods for **Video-Text Retrieval**, [[Paper]](https://arxiv.org/pdf/2111.05610.pdf)

- (arXiv 2021.11) Multimodal Transformer with Variable-length Memory for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2111.05759.pdf)

- (arXiv 2021.11) Improving **Visual Quality** of **Image Synthesis** by A Token-based Generator with Transformers, [[Paper]](https://arxiv.org/abs/2111.03481)

- (arXiv 2021.11) VLMO: Unified **Vision-Language** Pre-Training with Mixture-of-Modality-Experts, [[Paper]](https://arxiv.org/pdf/2111.02358.pdf), [[Code]](https://aka.ms/vlmo)

- (arXiv 2021.11) LAION-400M: Open **Dataset** of **CLIP**-Filtered 400 Million **Image-Text** Pairs, [[Paper]](https://arxiv.org/pdf/2111.02114.pdf), [[Project]](https://laion.ai/laion-400-open-dataset/)

- (arXiv 2021.11) An Empirical Study of **Training** End-to-End **Vision-and-Language** Transformers, [[Paper]](https://arxiv.org/pdf/2111.02387.pdf), [[Code]](https://github.com/zdou0830/METER)

- (arXiv 2021.11) CAN VISION TRANSFORMERS PERFORM **CONVOLUTION**? [[Paper]](https://arxiv.org/pdf/2111.01353.pdf)

- (arXiv 2021.11) HRViT: **Multi-Scale High-Resolution** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.01236.pdf)

### 2021.10

- (arXiv 2021.10) **Visual Keyword Spotting** with Attention, [[Paper]](https://arxiv.org/pdf/2110.15957.pdf), [[Project]](Visual Keyword Spotting with Attention)

- (arXiv 2021.10) Learning **Co-segmentation** by Segment Swapping for Retrieval and Discovery, [[Paper]](https://arxiv.org/pdf/2110.15904.pdf), [[Data & Code]](http://imagine.enpc.fr/~shenx/SegSwap/)

- (arXiv 2021.10) Visual Spatio-Temporal Relation-Enhanced Network for Cross-Modal **Text-Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2110.15609.pdf), [[Code]](https://https//github.com/Lionel-Hing/VSR-Net)

- (arXiv 2021.10) Dispensed Transformer Network for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2110.14944.pdf)

- (arXiv 2021.10) Scatterbrain: Unifying **Sparse** and **Low-rank Attention** Approximation, [[Paper]](https://arxiv.org/pdf/2110.15343.pdf)

- (arXiv 2021.10) **3D Object Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2110.14921.pdf), [[Code]](https://github.com/3bobo/lttr)

- (arXiv 2021.10) Blending **Anti-Aliasing** into Vision Transformer, [[Paper]](https://arxiv.org/pdf/2110.15156.pdf), [[Code]](https://github.com/amazon-research/anti-aliasing-transformer)

- (arXiv 2021.10) UltraPose: **Synthesizing** Dense Pose with 1 Billion Points by **Human-body** Decoupling **3D** Model, [[Paper]](https://arxiv.org/pdf/2110.15267.pdf), [[Data & Code]](https://github.com/MomoAILab/ultrapose)

- (arXiv 2021.10) SOAT: A Scene- and Object-Aware Transformer for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2110.14143.pdf)

- (arXiv 2021.10) Bangla Image **Caption Generation** through CNN-Transformer based Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2110.12442.pdf)

- (arXiv 2021.10) History Aware Multimodal Transformer for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2110.13309.pdf), [[Project]](https://cshizhe.github.io/projects/vln_hamt.html)

- (arXiv 2021.10) TriBERT: Full-body Human-centric Audio-visual Representation Learning for **Visual Sound Separation**, [[Paper]](https://arxiv.org/pdf/2110.13412.pdf)

- (arXiv 2021.10) TNTC: TWO-STREAM NETWORK WITH TRANSFORMER-BASED COMPLEMENTARITY FOR GAIT-BASED **EMOTION RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2110.13708.pdf)

- (arXiv 2021.10) Contextual Similarity Aggregation with Self-attention for **Visual Re-ranking**, [[Paper]](https://arxiv.org/pdf/2110.13430.pdf), [[Code]](https://github.com/MCC-WH/CSA)

- (arXiv 2021.10) IIP-Transformer: Intra-Inter-Part Transformer for **Skeleton-Based Action Recognition**, [[Paper]](https://arxiv.org/pdf/2110.13385.pdf), [[Code]](https://github.com/qtwang0035/IIP-Transformer)

- (arXiv 2021.10) IMAGE-BASED **CLIP**-GUIDED ESSENCE TRANSFER, [[Paper]](https://arxiv.org/pdf/2110.12427.pdf), [[Code]](https://github.com/hila-chefer/TargetCLIP)

- (arXiv 2021.10) Sinkformers: Transformers with Doubly Stochastic **Attention**, [[Paper]](https://arxiv.org/pdf/2110.11773.pdf)

- (arXiv 2021.10) ILLITERATE **DALL·E** LEARNS TO COMPOSE, [[Paper]](https://arxiv.org/pdf/2110.11405.pdf), [[Project]](https://sites.google.com/view/slate-autoencoder), [[Code]](https://github.com/singhgautam/slate)

- (arXiv 2021.10) Learning Text-Image Joint Embedding for Efficient **Cross-Modal Retrieval** with Deep Feature Engineering, [[Paper]](https://arxiv.org/pdf/2110.11592.pdf)

- (arXiv 2021.10) SOFT: Softmax-free Transformer with **Linear Complexity**, [[Paper]](https://arxiv.org/pdf/2110.11945.pdf), [[Code]](https://fudan-zvg.github.io/SOFT)

- (arXiv 2021.10) Deep Two-Stream Video Inference for Human Body **Pose** and **Shape Estimation**, [[Paper]](https://arxiv.org/pdf/2110.11680.pdf)

- (arXiv 2021.10) TRANSFORMER **ACCELERATION** WITH DYNAMIC SPARSE ATTENTION, [[Paper]](https://arxiv.org/pdf/2110.11299.pdf)

- (arXiv 2021.10) CLOOB: MODERN **HOPFIELD** NETWORKS WITH INFOLOOB OUTPERFORM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11316.pdf), [[Code]](https://github.com/ml-jku/cloob)

- (arXiv 2021.10) Integrating Visuospatial, Linguistic and Commonsense Structure into **Story Visualization**, [[Paper]](https://arxiv.org/pdf/2110.10834.pdf)

- (arXiv 2021.10) StructFormer: Learning Spatial Structure for **Language-Guided** Semantic **Rearrangement** of Novel Objects, [[Paper]](https://arxiv.org/pdf/2110.10189.pdf), [[Project]](https://sites.google.com/view/structformer)

- (arXiv 2021.10) Gophormer: Ego-**Graph** Transformer for **Node Classification**, [[Paper]](https://arxiv.org/pdf/2110.13094.pdf)

- (arXiv 2021.10) STRANSGAN: AN EMPIRICAL STUDY ON TRANSFORMER IN **GANS**, [[Paper]](https://arxiv.org/pdf/2110.13107.pdf), [[Code]](https://nbei.github.io/stransgan.html)

- (arXiv 2021.10) MVT: Multi-view Vision Transformer for **3D Object Recognition**, [[Paper]](https://arxiv.org/pdf/2110.13083.pdf)

- (arXiv 2021.10) DocTr: **Document Image** Transformer for Geometric Unwarping and Illumination Correction, [[Paper]](https://arxiv.org/pdf/2110.12942.pdf), [[Code]](https://github.com/fh2019ustc/DocTr)

- (arXiv 2021.10) Bangla Image **Caption** Generation through CNN-Transformer based Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2110.12442.pdf)

- (arXiv 2021.10) WAV2CLIP: LEARNING ROBUST **AUDIO REPRESENTATIONS** FROM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11499.pdf), [[Code]](https://github.com/descriptinc/lyrebird-wav2clip)

- (arXiv 2021.10) AFTer-UNet: Axial Fusion Transformer UNet for **Medical Image Segmentation**, [[Paper]](https://arxiv.org/pdf/2110.10403.pdf)

- (arXiv 2021.10) CLOOB: MODERN HOPFIELD NETWORKS WITH INFOLOOB OUTPERFORM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11316.pdf), [[Code]](https://github.com/ml-jku/cloob)

- (arXiv 2021.10) AniFormer: Data-driven **3D Animation** with Transformer, [[Paper]](https://arxiv.org/pdf/2110.10533.pdf), [[Code]](https://github.com/mikecheninoulu/AniFormer)

- (arXiv 2021.10) **Few-Shot Temporal Action Localization** with Query Adaptive Transformer, [[Paper]](https://arxiv.org/pdf/2110.10552.pdf), [[Code]](https://github.com/sauradip/fewshotQAT)

- (arXiv 2021.10) 3D-ANAS v2: Grafting Transformer Module on Automatically Designed ConvNet for **Hyperspectral Image Classification**, [[Paper]](https://arxiv.org/pdf/2110.11084.pdf), [[Code]](https://github.com/xmm/3D-ANAS-V2)

- (arXiv 2021.10) CMTR: Cross-modality Transformer for Visible-infrared **Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2110.08994.pdf)

- (arXiv 2021.10) 3D-RETR: End-to-End **Single and Multi-View 3D Reconstruction** with Transformers, [[Paper]](https://arxiv.org/pdf/2110.08861.pdf), [[Code]](https://github.com/FomalhautB/3D-RETR)

- (arXiv 2021.10) HRFormer: **High-Resolution** Transformer for **Dense Prediction**, [[Paper]](https://arxiv.org/pdf/2110.09408.pdf), [[Code]](https://github.com/HRNet/HRFormer)

- (arXiv 2021.10) Leveraging MoCap Data for **Human Mesh Recovery**, [[Paper]](https://arxiv.org/pdf/2110.09243.pdf)

- (arXiv 2021.10) A Good **Prompt** Is Worth Millions of Parameters? Low-resource Prompt-based Learning for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2110.08484.pdf)

- (arXiv 2021.10) ASFormer: Transformer for **Action Segmentation**, [[Paper]](https://arxiv.org/pdf/2110.08568.pdf), [[Code]](https://github.com/ChinaYi/ASFormer)

- (arXiv 2021.10) Multimodal **Dialogue Response Generation**, [[Paper]](https://arxiv.org/pdf/2110.08515.pdf)

- (arXiv 2021.10) Understanding **Procedural Knowledge** by Sequencing Multimodal Instructional Manuals, [[Paper]](https://arxiv.org/pdf/2110.08486.pdf)

- (arXiv 2021.10) COMPOSITIONAL **ATTENTION**: DISENTANGLING SEARCH AND RETRIEVAL, [[Paper]](https://arxiv.org/pdf/2110.09419.pdf), [[Code]](https://github.com/sarthmit/Compositional-Attention)

- (arXiv 2021.10) Spatial-Temporal Transformer for 3D **Point Cloud Sequences**, [[Paper]](https://arxiv.org/pdf/2110.09783.pdf)

- (arXiv 2021.10) TransFusion: Cross-view Fusion with Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2110.09554.pdf), [[Code]](https://github.com/HowieMa/TransFusion-Pose)

- (arXiv 2021.10) Unifying Multimodal Transformer for **Bi-directional Image and Text Generation**, [[Paper]](https://arxiv.org/pdf/2110.09753.pdf)

- (arXiv 2021.10) Transformer with a Mixture of **Gaussian Keys**, [[Paper]](https://arxiv.org/pdf/2110.08678.pdf)

- (arXiv 2021.10) DIFFUSIONCLIP: **TEXT-GUIDED IMAGE MANIPULATION** USING DIFFUSION MODELS, [[Paper]](https://arxiv.org/pdf/2110.02711.pdf)

- (arXiv 2021.10) Adversarial **Robustness** Comparison of Vision Transformer and MLP-Mixer to CNNs, [[Paper]](https://arxiv.org/pdf/2110.02797.pdf), [[Code]](https://github.com/phibenz/robustness_comparison_vit_mlp-mixer_cnn)

- (arXiv 2021.10) RIPPLE ATTENTION FOR VISUAL PERCEPTION WITH **SUB-QUADRATIC COMPLEXITY**, [[Paper]](https://arxiv.org/pdf/2110.02453.pdf)

- (arXiv 2021.10) Certified Patch **Robustness** via Smoothed Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.07719.pdf), [[Code]](https://github.com/MadryLab/smoothed-vit)

- (arXiv 2021.10) CLIP-Forge: Towards Zero-Shot **Text-to-Shape** Generation, [[Paper]](https://arxiv.org/pdf/2110.02624.pdf)

- (arXiv 2021.10) Understanding and Improving **Robustness** of Vision Transformers through Patch-based Negative Augmentation, [[Paper]](https://arxiv.org/pdf/2110.07858.pdf)

- (arXiv 2021.10) SPARSE MOES MEET **EFFICIENT ENSEMBLES**, [[Paper]](https://arxiv.org/pdf/2110.03360.pdf)

- (arXiv 2021.10) Shared **Visual Representations** of Drawing for Communication: How do different **biases** affect human interpretability and intent? [[Paper]](https://arxiv.org/pdf/2110.08203.pdf)

- (arXiv 2021.10) SignBERT: Pre-Training of Hand-Model-Aware Representation for **Sign Language Recognition**, [[Paper]](https://arxiv.org/pdf/2110.05382.pdf)

- (arXiv 2021.10) Revitalizing CNN Attentions via Transformers in **Self-Supervised** Visual Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.05340.pdf)

- (arXiv 2021.10) Investigating **Transfer Learning Capabilities** of Vision Transformers and CNNs by Fine-Tuning a Single Trainable Block, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2110/2110.05270.pdf)

- (arXiv 2021.10) SUPERVISION EXISTS EVERYWHERE: A DATA EFFICIENT CONTRASTIVE **LANGUAGE-IMAGE** PRE-TRAINING PARADIGM, [[Paper]](https://arxiv.org/pdf/2110.05208.pdf), [[Code]](https://github.com/Sense-GVT/)

- (arXiv 2021.10) CLIP4Caption ++: Multi-CLIP for **Video Caption**, [[Paper]](https://arxiv.org/pdf/2110.05204.pdf)

- (arXiv 2021.10) Transformer-based Dual Relation Graph for **Multi-label Image Recognition**, [[Paper]](https://arxiv.org/pdf/2110.04722.pdf)

- (arXiv 2021.10) VECTOR-QUANTIZED **IMAGE MODELING** WITH IMPROVED VQGAN, [[Paper]](https://arxiv.org/pdf/2110.04627.pdf)

- (arXiv 2021.10) Adaptively Multi-view and Temporal Fusing Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2110.05092.pdf), [[Code]](https://github.com/lelexx/MTF-Transformer)

- (arXiv 2021.10) NVIT: VISION TRANSFORMER **COMPRESSION** AND **PARAMETER REDISTRIBUTION**, [[Paper]](https://arxiv.org/pdf/2110.04869.pdf)

- (arXiv 2021.10) 6D-ViT: Category-Level **6D Object Pose Estimation** via Transformer-based Instance Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.04792.pdf)

- (arXiv 2021.10) CLIP-Adapter: Better **Vision-Language** Models with Feature Adapters, [[Paper]](https://arxiv.org/pdf/2110.04544.pdf), [[Code]](https://github.com/gaopengcuhk/CLIP-Adapter)

- (arXiv 2021.10) ATISS: Autoregressive Transformers for **Indoor Scene Synthesis**, [[Paper]](https://arxiv.org/pdf/2110.03675.pdf), [[Code]](https://nv-tlabs.github.io/ATISS)
， 
- (arXiv 2021.10) MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND **MOBILE**-FRIENDLY VISION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2110.02178.pdf)

- (arXiv 2021.10) **TOKEN POOLING** IN VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.03860.pdf)

- (arXiv 2021.10) VIDT: AN EFFICIENT AND EFFECTIVE FULLY TRANSFORMER-BASED **OBJECT DETECTOR**, [[Paper]](https://arxiv.org/pdf/2110.03921.pdf), [[Code]](https://github.com/naver-ai/vidt)

- (arXiv 2021.10) CLIP4Caption: CLIP for **Video Caption**, [[Paper]](https://arxiv.org/pdf/2110.06615.pdf)

- (arXiv 2021.10) **OBJECT**-REGION **VIDEO** TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.06915.pdf), [[Code]](https://roeiherz.github.io/ORViT/)

- (arXiv 2021.10) LEVERAGING **REDUNDANCY** IN ATTENTION WITH REUSE TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.06821.pdf)

- (arXiv 2021.10) **Dynamic Inference** with Neural Interpreters, [[Paper]](https://arxiv.org/pdf/2110.06399.pdf)

- (arXiv 2021.10) A CLIP-Enhanced Method for **Video-Language** Understanding, [[Paper]](https://arxiv.org/pdf/2110.07137.pdf)

- (arXiv 2021.10) **Visual Relationship Detection** Using Part-and-Sum Transformers with Composite Queries, [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Dong_Visual_Relationship_Detection_Using_Part-and-Sum_Transformers_With_Composite_Queries_ICCV_2021_paper.pdf)

- (arXiv 2021.10) Discovering Human **Interactions** with Large-Vocabulary Objects via Query and Multi-Scale Detection, [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Discovering_Human_Interactions_With_Large-Vocabulary_Objects_via_Query_and_Multi-Scale_ICCV_2021_paper.pdf)

- (arXiv 2021.10) Learning Structural Representations for **Recipe Generation** and **Food Retrieval**, [[Paper]](https://arxiv.org/pdf/2110.01209.pdf)

- (arXiv 2021.10) A FREE LUNCH FROM VIT: ADAPTIVE ATTENTION MULTI-SCALE FUSION TRANSFORMER FOR **FINE-GRAINED VISUAL RECOGNITION**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2110/2110.01240.pdf)

### 2021.09
- (arXiv 2021.09) Joint Multimedia **Event Extraction** from Video and Article, [[Paper]](https://arxiv.org/pdf/2109.12776.pdf)

- (arXiv 2021.09) Long-Range Transformers for **Dynamic Spatiotemporal Forecasting**, [[Paper]](https://arxiv.org/pdf/2109.12218.pdf)

- (arXiv 2021.09) **Visually Grounded Concept** Composition, [[Paper]](https://arxiv.org/pdf/2109.14115.pdf)

- (arXiv 2021.09) CoSeg: Cognitively Inspired Unsupervised Generic **Event Segmentation**, [[Paper]](https://arxiv.org/pdf/2109.15170.pdf)

- (arXiv 2021.09) CCTrans: Simplifying and Improving **Crowd Counting** with Transformer, [[Paper]](https://arxiv.org/pdf/2109.14483.pdf)

- (arXiv 2021.09) UFO-ViT: High Performance **Linear** Vision Transformer **without Softmax**, [[Paper]](https://arxiv.org/pdf/2109.14382.pdf)

- (arXiv 2021.09) **Infrared Small-Dim Target Detection** with Transformer under Complex Backgrounds, [[Paper]](https://arxiv.org/pdf/2109.14379.pdf)

- (arXiv 2021.09) **Localizing Objects** with Self-Supervised Transformers and no Labels, [[Paper]](https://arxiv.org/pdf/2109.14279.pdf), [[Code]](https://github.com/valeoai/LOST)

- (arXiv 2021.09) Geometry-Entangled Visual Semantic Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2109.14137.pdf)

- (arXiv 2021.09) VideoCLIP: Contrastive Pre-training for **Zero-shot Video-Text Understanding**, [[Paper]](https://arxiv.org/pdf/2109.14084.pdf), [[Code]](https://github.com/pytorch/fairseq/examples/MMPT)

- (arXiv 2021.09) Fine-tuning Vision Transformers for the Prediction of **State Variables in Ising Models**, [[Paper]](https://arxiv.org/pdf/2109.13925.pdf)

- (arXiv 2021.09) CLIP-It! Language-Guided **Video Summarization**, [[Paper]](https://arxiv.org/pdf/2107.00650.pdf), [[Project]](https://medhini.github.io/clip_it)

- (arXiv 2021.09) MFEVIT: A ROBUST LIGHTWEIGHT TRANSFORMER-BASED NETWORK FOR MULTIMODAL 2D+3D **FACIAL EXPRESSION RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2109.13086.pdf)

- (arXiv 2021.09) Sparse Spatial Transformers for **Few-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2109.12932.pdf), [[Code]](https://github.com/chenhaoxing/SSFormers)

- (arXiv 2021.09) Vision Transformer Hashing for **Image Retrieval**, [[Paper]](https://arxiv.org/pdf/2109.12564.pdf)

- (arXiv 2021.09) PETA: **Photo Albums Event Recognition** using Transformers Attention, [[Paper]](https://arxiv.org/pdf/2109.12499.pdf)

- (arXiv 2021.09) MLIM: **VISION-AND-LANGUAGE** MODEL PRE-TRAINING WITH MASKED LANGUAGE AND IMAGE MODELING, [[Paper]](https://arxiv.org/pdf/2109.12178.pdf)

- (arXiv 2021.09) Dense Contrastive **Visual-Linguistic** Pretraining, [[Paper]](https://arxiv.org/pdf/2109.11778.pdf)

- (arXiv 2021.09) CPT: COLORFUL **PROMPT TUNING** FOR PRE-TRAINED VISION-LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2109.11797.pdf)

- (arXiv 2021.09) Localizing ∞-shaped fishes: **Sketch-guided object localization** in the wild, [[Paper]](https://arxiv.org/pdf/2109.11874.pdf), [[Code]](https://github.com/priba/sgol_wild)

- (arXiv 2021.09) CLIPORT: What and Where Pathways for **Robotic Manipulation**, [[Paper]](https://arxiv.org/pdf/2109.12098.pdf), [[Project]](https://cliport.github.io/), [[Code]](https://github.com/cliport/cliport)

- (arXiv 2021.09) GraFormer: Graph Convolution Transformer for **3D Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2109.08364.pdf), [[Code]](https://github.com/Graformer/GraFormer)

- (arXiv 2021.09) Multimodal Incremental Transformer with Visual Grounding for **Visual Dialogue Generation**, [[Paper]](https://arxiv.org/pdf/2109.08478.pdf)

- (arXiv 2021.09) Expression Snippet Transformer for Robust Video-based **Facial Expression Recognition**, [[Paper]](https://arxiv.org/pdf/2109.08409.pdf), [[Code]](https://anonymous.4open.science/r/ATSE-C58B)

- (arXiv 2021.09) LOTR: **Face Landmark Localization** Using Localization Transformer, [[Paper]](https://arxiv.org/pdf/2109.10057.pdf)

- (arXiv 2021.09) Dyadformer: A **Multi-modal** Transformer for Long-Range Modeling of Dyadic Interactions, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2109/2109.09487.pdf)

- (arXiv 2021.09) SDTP: Semantic-aware Decoupled Transformer Pyramid for **Dense Image Prediction**, [[Paper]](https://arxiv.org/pdf/2109.08963.pdf)

- (arXiv 2021.09) KD-VLP: Improving End-to-End **Vision-and-Language Pretraining** with Object Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2109.10504.pdf)

- (arXiv 2021.09) T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression, [[Paper]](https://arxiv.org/pdf/2109.10948.pdf)

- (arXiv 2021.09) OH-Former: Omni-Relational High-Order Transformer for **Person Re-Identification**, [[Paper]](https://arxiv.org/pdf/2109.11159.pdf)

- (arXiv 2021.09) PIX2SEQ: A LANGUAGE MODELING FRAMEWORK FOR **OBJECT DETECTION**, [[Paper]](https://arxiv.org/pdf/2109.10852.pdf)

- (arXiv 2021.09) ActionCLIP: A New Paradigm for **Video Action Recognition**, [[Paper]](https://arxiv.org/pdf/2109.08472.pdf)

- (arXiv 2021.09) BGT-Net: Bidirectional GRU Transformer Network for **Scene Graph Generation**, [[Paper]](https://arxiv.org/pdf/2109.05346.pdf)

- (arXiv 2021.09) Neural Human Performer: Learning Generalizable Radiance Fields for **Human Performance Rendering**, [[Paper]](https://arxiv.org/pdf/2109.07448.pdf), [[Code]](https://youngjoongunc.github.io/nhp/)

- (arXiv 2021.09) **Anchor DETR**: Query Design for Transformer-Based Detector, [[Paper]](https://arxiv.org/pdf/2109.07107.pdf), [[Code]](https://github.com/megvii-model/AnchorDETR)

- (arXiv 2021.09) An End-to-End Transformer Model for **3D Object Detection**, [[Paper]](https://arxiv.org/pdf/2109.08141.pdf), [[Code]](https://facebookresearch.github.io/3detr)

- (arXiv 2021.09) Hybrid Local-Global Transformer for **Image Dehazing**, [[Paper]](https://arxiv.org/pdf/2109.07100.pdf)

- (arXiv 2021.09) Semi-Supervised Wide-Angle **Portraits Correction** by Multi-Scale Transformer, [[Paper]](https://arxiv.org/pdf/2109.08024.pdf)

- (arXiv 2021.09) Label-Attention Transformer with Geometrically Coherent Objects for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2109.07799.pdf)

- (arXiv 2021.09) Pose Transformers (POTR): **Human Motion Prediction** with Non-Autoregressive Transformers, [[Paper]](https://arxiv.org/pdf/2109.07531.pdf), [[Code]](https://github.com/idiap/potr)

- (arXiv 2021.09) PnP-DETR: Towards **Efficient** Visual Analysis with Transformers, [[Paper]](https://arxiv.org/pdf/2109.07036.pdf), [[Code]](https://github.com/twangnh/pnp-detr)

- (arXiv 2021.09) Learning to **Ground** Visual Objects for Visual Dialog, [[Paper]](https://arxiv.org/pdf/2109.06013.pdf)

- (arXiv 2021.09) On Pursuit of Designing Multi-modal Transformer for **Video Grounding**, [[Paper]](https://arxiv.org/pdf/2109.06085.pdf), [[Code]](https://sites.google.com/view/mengcao/publication/gtr)

- (arXiv 2021.09) CDTrans: Cross-domain Transformer for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2109.06165.pdf)

- (arXiv 2021.09) IS ATTENTION BETTER THAN **MATRIX DECOMPOSITION**? [[Paper]](https://arxiv.org/pdf/2109.04553.pdf), [[Code]](https://github.com/Gsunshine/Enjoy-Hamburger)

- (arXiv 2021.09) Temporal Pyramid Transformer with Multimodal Interaction for **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2109.04735.pdf)

- (arXiv 2021.09) Line as a Visual Sentence: Context-aware **Line Descriptor** for Visual Localization, [[Paper]](https://arxiv.org/pdf/2109.04753.pdf)

- (arXiv 2021.09) Negative Sample Matters: A Renaissance of Metric Learning for **Temporal Grounding**, [[Paper]](https://arxiv.org/pdf/2109.04872.pdf)

- (arXiv 2021.09) LAViTeR: Learning Aligned **Visual and Textual** Representations Assisted by Image and Caption Generation, [[Paper]](https://arxiv.org/pdf/2109.04993.pdf), [[Code]](https://github.com/mshaikh2/LaViTeR)

- (arXiv 2021.09) Panoptic Narrative **Grounding**, [[Paper]](https://arxiv.org/pdf/2109.04988.pdf)

- (arXiv 2021.09) An Empirical Study of GPT-3 for Few-Shot Knowledge-Based **VQA**, [[Paper]](https://arxiv.org/pdf/2109.05014.pdf)

- (arXiv 2021.09) PlaTe: **Visually-Grounded Planning** with Transformers in Procedural Tasks, [[Paper]](https://arxiv.org/pdf/2109.04869.pdf), [[Project]](https://www.pair.toronto.edu/plate-planner/)

- (arXiv 2021.09) **EfficientCLIP**: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling, [[Paper]](https://arxiv.org/pdf/2109.04699.pdf)

- (arXiv 2021.09) **Scaled ReLU** Matters for **Training** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.03810.pdf)

- (arXiv 2021.09) FuseFormer: Fusing Fine-Grained Information in Transformers for **Video Inpainting**, [[Paper]](https://arxiv.org/pdf/2109.02974.pdf), [[Code]](https://github.com/ruiliu-ai/FuseFormer)

- (arXiv 2021.09) GCsT: Graph Convolutional Skeleton Transformer for **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2109.02860.pdf)

- (arXiv 2021.09) WHYACT: Identifying **Action Reasons** in Lifestyle **Vlogs**, [[Paper]](https://arxiv.org/pdf/2109.02747.pdf)

- (arXiv 2021.09) Zero-Shot **Open Set Detection** by Extending **CLIP**, [[Paper]](https://arxiv.org/pdf/2109.02748.pdf)

- (arXiv 2021.09) Towards Transferable **Adversarial Attacks** on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.04176.pdf)

- (arXiv 2021.09) Learning to **Prompt** for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2109.01134), [[Code]](https://github.com/KaiyangZhou/CoOp)

- (arXiv 2021.09) Improving **Video-Text Retrieval** by Multi-Stream Corpus Alignment and Dual Softmax Loss, [[Paper]](https://arxiv.org/pdf/2109.04290.pdf), [[Code]](https://github.com/starmemda/CAMoW/)

- (arXiv 2021.09) UCTransNet: Rethinking the **Skip Connections in U-Net** from a Channel-wise Perspective with Transformer, [[Paper]](https://arxiv.org/pdf/2109.04335.pdf), [[Code]](https://github.com/McGregorWwww/UCTransNet)

- (arXiv 2021.09) ConvMLP: Hierarchical Convolutional **MLPs** for Vision, [[Paper]](https://arxiv.org/pdf/2109.04454.pdf), [[Code]](https://github.com/SHI-Labs/Convolutional-MLPs)

- (arXiv 2021.09) TxT: **Crossmodal** End-to-End Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2109.04422.pdf)

- (arXiv 2021.09) Vision-and-Language or Vision-for-Language? On **Cross-Modal Influence** in Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2109.04448.pdf)

- (arXiv 2021.09) **Sparse**-MLP: A Fully-**MLP** Architecture with Conditional Computation, [[Paper]](https://arxiv.org/pdf/2109.02008.pdf)

- (arXiv 2021.09) SORNet: Spatial Object-Centric Representations for Sequential **Manipulation**, [[Paper]](https://arxiv.org/pdf/2109.03891.pdf), [[Project]](https://wentaoyuan.github.io/sornet)

- (arXiv 2021.09) Audio-Visual Transformer Based **Crowd Counting**, [[Paper]](https://arxiv.org/pdf/2109.01926.pdf)

- (arXiv 2021.09) Weakly Supervised Relative Spatial Reasoning for **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2109.01934.pdf), [[Code]](https://github.com/pratyay-banerjee/weak_sup_vqa)

- (arXiv 2021.09) FUSFORMER: A TRANSFORMER-BASED FUSION APPROACH FOR HYPERSPECTRAL IMAGE **SUPER-RESOLUTION**, [[Paper]](https://arxiv.org/pdf/2109.02079.pdf)

- (arXiv 2021.09) CTRL-C: **Camera calibration** TRansformer with Line-Classification, [[Paper]](https://arxiv.org/pdf/2109.02259.pdf), [[Code]](https://github.com/jwlee-vcl/CTRL-C)

- (arXiv 2021.09) Learning to Generate **Scene Graph** from Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2109.02227.pdf), [[Code]](https://github.com/YiwuZhong/SGG_from_NLS)

- (arXiv 2021.09) The Animation Transformer: Visual **Correspondence** via Segment Matching, [[Paper]](https://arxiv.org/pdf/2109.02614.pdf)

- (arXiv 2021.09) Voxel Transformer for **3D Object Detection**, [[Paper]](https://arxiv.org/pdf/2109.02497.pdf)

- (ICCV 2021.09) **3D Human Texture Estimation** from a Single Image with Transformers, [[Paper]](http://personal.ie.cuhk.edu.hk/~ccloy/files/iccv_2021_texformer.pdf), [[Code]](https://github.com/xuxy09/Texformer)

- (arXiv 2021.09) Encoder-decoder with Multi-level Attention for **3D Human Shape and Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2109.02303.pdf), [[Code]](https://github.com/ziniuwan/maed)

- (arXiv 2021.09) Joint Graph Learning and Matching for **Semantic Feature Correspondence**, [[Paper]](https://arxiv.org/pdf/2109.00240.pdf)

- (arXiv 2021.09) Searching for **Efficient** Multi-Stage Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.00642.pdf), [[Code]](https://github.com/yilunliao/vit-search)

### 2021.08
- (arXiv 2021.08) SIGN: Spatial-information Incorporated Generative Network for **Generalized Zero-shot Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2108.12517.pdf)

- (arXiv 2021.08) GroupFormer: **Group Activity Recognition** with Clustered Spatial-Temporal Transformer, [[Paper]](https://arxiv.org/pdf/2108.12630.pdf), [[Code]](https://github.com/xueyee/GroupFormer)

- (arXiv 2021.08) **A Battle of Network Structures**: An Empirical Study of CNN, Transformer, and MLP, [[Paper]](https://arxiv.org/pdf/2108.13002.pdf)

- (arXiv 2021.08) Exploring and Improving **Mobile** Level Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.13015.pdf)

- (arXiv 2021.08) Cross-category **Video Highlight Detection** via Set-based Learning, [[Paper]](https://arxiv.org/pdf/2108.11770.pdf), [[Code]](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)

- (arXiv 2021.08) Shifted Chunk Transformer for **Spatio-Temporal** Representational Learning, [[Paper]](https://arxiv.org/pdf/2108.11575.pdf)

- (arXiv 2021.08) SASRA: Semantically-aware Spatio-temporal Reasoning Agent for **Vision-and-Language Navigation** in Continuous Environments, [[Paper]](https://arxiv.org/pdf/2108.11945.pdf)

- (arXiv 2021.08) LocTex: Learning **Data-Efficient** Visual **Representations** from Localized Textual Supervision, [[Paper]](https://arxiv.org/pdf/2108.11950.pdf), [[Project]](https://loctex.mit.edu/)

- (arXiv 2021.08) Guiding Query Position and Performing Similar Attention for Transformer-Based **Detection** Heads, [[Paper]](https://arxiv.org/pdf/2108.09691.pdf)

- (arXiv 2021.08) SIMVLM: SIMPLE **VISUAL LANGUAGE** MODEL PRETRAINING WITH WEAK SUPERVISION, [[Paper]](https://arxiv.org/pdf/2108.10904.pdf)

- (arXiv 2021.08) TransFER: Learning Relation-aware **Facial Expression Representations** with Transformers, [[Paper]](https://arxiv.org/pdf/2108.11116.pdf)

- (arXiv 2021.08) Efficient Transformer for Single Image **Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2108.11084.pdf)

- (arXiv 2021.08) Discovering Spatial Relationships by Transformers for **Domain Generalization**, [[Paper]](https://arxiv.org/pdf/2108.10046.pdf)

- (arXiv 2021.08) TACo: Token-aware Cascade Contrastive Learning for **Video-Text Alignment**, [[Paper]](https://arxiv.org/pdf/2108.09980.pdf)

- (arXiv 2021.08) MM-ViT: Multi-Modal Video Transformer for **Compressed Video Action Recognition**, [[Paper]](https://arxiv.org/pdf/2108.09322.pdf)

- (arXiv 2021.08) SwinIR: **Image Restoration** Using Swin Transformer, [[Paper]](https://arxiv.org/pdf/2108.10257.pdf), [[Code]](https://github.com/JingyunLiang/SwinIR)

- (arXiv 2021.08) Grid-VLP: Revisiting Grid Features for **Vision-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2108.09479.pdf)

- (arXiv 2021.08) Improving **3D Object Detection** with Channel-wise Transformer, [[Paper]](https://arxiv.org/pdf/2108.10723.pdf)

- (arXiv 2021.08) No-Reference **Image Quality Assessment** via Transformers, Relative Ranking, and Self-Consistency, [[Paper]](https://arxiv.org/pdf/2108.06858.pdf), [[Code]](https://github.com/isalirezag/TReS)

- (arXiv 2021.08) SOTR: **Segmenting** Objects with Transformers, [[Paper]](https://arxiv.org/pdf/2108.06747.pdf), [[Code]](https://github.com/easton-cau/SOTR)

- (arXiv 2021.08) ROSITA: Enhancing **Vision-and-Language** Semantic Alignments via Cross- and Intra-modal Knowledge Integration, [[Paper]](https://arxiv.org/pdf/2108.07073.pdf), [[Code]](https://github.com/MILVLG/rosita)

- (arXiv 2021.08) Escaping the **Gradient Vanishing**: Periodic Alternatives of Softmax in Attention Mechanism, [[Paper]](https://arxiv.org/pdf/2108.07153.pdf), [[Code]](https://github.com/slwang9353/Period-alternatives-of-Softmax)

- (arXiv 2021.08) End-to-End Dense **Video Captioning** with Parallel Decoding, [[Paper]](https://arxiv.org/pdf/2108.07781.pdf), [[Code]](https://github.com/ttengwang/PDVC)

- (arXiv 2021.08) Trans4Trans: **Efficient** Transformer for Transparent Object and Semantic Scene Segmentation in Real-World **Navigation** Assistance, [[Paper]](https://arxiv.org/pdf/2108.09174.pdf)

- (arXiv 2021.08) **Video Relation Detection** via Tracklet based Visual Transformer, [[Paper]](https://arxiv.org/pdf/2108.08669.pdf), [[Code]](https://github.com/Dawn-LX/VidVRD-tracklets)

- (arXiv 2021.08) PoinTr: Diverse **Point Cloud Completion** with Geometry-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2108.08839.pdf), [[Code]](https://github.com/yuxumin/PoinTr)

- (arXiv 2021.08) ImageBART: Bidirectional Context with Multinomial Diffusion for **Autoregressive Image Synthesis**, [[Paper]](https://arxiv.org/pdf/2108.08827.pdf), [[Project]](https://compvis.github.io/imagebart/)

- (arXiv 2021.08) Do Vision Transformers **See Like Convolutional Neural Networks?** [[Paper]](https://arxiv.org/pdf/2108.08810.pdf)

- (arXiv 2021.08) TVT: Transferable Vision Transformer for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2108.05988.pdf)

- (arXiv 2021.08) MUSIQ: Multi-scale **Image Quality** Transformer, [[Paper]](https://arxiv.org/pdf/2108.05997.pdf)

- (arXiv 2021.08) **Point-Voxel** Transformer: An Efficient Approach To 3D Deep Learning, [[Paper]](https://arxiv.org/pdf/2108.06076.pdf), [[Code]](https://github.com/2020zhangcheng/PVT)

- (arXiv 2021.08) Conditional **DETR** for **Fast** Training Convergence, [[Paper]](https://arxiv.org/pdf/2108.06152.pdf), [[Code]](https://git.io/ConditionalDETR)

- (arXiv 2021.08) Vision-Language Transformer and Query Generation for **Referring Segmentation**, [[Paper]](https://arxiv.org/pdf/2108.05565.pdf), [[Code]](https://github.com/henghuiding/Vision-Language-Transformer)

- (arXiv 2021.08) Mobile-Former: Bridging **MobileNet** and Transformer, [[Paper]](https://arxiv.org/pdf/2108.05895.pdf)

- (arXiv 2021.08) **Multiview Detection** with Shadow Transformer (and View-Coherent Data Augmentation), [[Paper]](https://arxiv.org/pdf/2108.05888.pdf), [[Code]](https://github.com/hou-yz/MVDeTr)

- (arXiv 2021.08) **Billion-Scale Pretraining** with Vision Transformers for Multi-Task Visual Representations, [[Paper]](https://arxiv.org/pdf/2108.05887.pdf)

- (arXiv 2021.08) Embodied BERT: A Transformer Model for **Embodied**, Language-guided Visual Task Completion, [[Paper]](https://arxiv.org/pdf/2108.04927.pdf), [[Code]](https://github.com/amazon-research/embert)

- (arXiv 2021.08) Video Transformer for **Deepfake Detection** with Incremental Learning, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2108/2108.05307.pdf)

- (arXiv 2021.08) **ConvNets vs. Transformers**: Whose Visual Representations are More **Transferable**? [[Paper]](https://arxiv.org/pdf/2108.05305.pdf)

- (arXiv 2021.08) A Transformer-based Math Language Model for **Handwritten Math Expression Recognition**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2108/2108.05002.pdf)

- (arXiv 2021.08) Optimizing Latency for **Online Video Captioning** Using Audio-Visual Transformers, [[Paper]](https://arxiv.org/pdf/2108.02147.pdf)

- (arXiv 2021.08) TransRefer3D: Entity-and-Relation Aware Transformer for **Fine-Grained 3D Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2108.02388.pdf)

- (arXiv 2021.08) Fast **Convergence** of **DETR** with Spatially Modulated Co-Attention. [[Paper]](https://arxiv.org/pdf/2108.02404.pdf), [[Code]](https://github.com/gaopengcuhk/SMCA-DETR)

- (arXiv 2021.08) Token Shift Transformer for **Video Classification**, [[Paper]](https://arxiv.org/pdf/2108.02432.pdf), [[Code]](https://github.com/VideoNetworks/TokShift-Transformer)

- (arXiv 2021.08) Simpler is Better: **Few-shot Semantic Segmentation** with Classifier Weight Transformer, [[Paper]](https://arxiv.org/pdf/2108.03032.pdf), [[Code]](https://github.com/zhiheLu/CWT-for-FSS)

- (arXiv 2021.08) Joint Inductive and Transductive Learning for **Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2108.03679.pdf), [[Code]](https://github.com/maoyunyao/JOINT)

- (arXiv 2021.08) OVIS: **Open-Vocabulary Visual Instance Search** via Visual-Semantic Aligned Representation Learning, [[Paper]](https://arxiv.org/pdf/2108.03704.pdf)

- (arXiv 2021.08) Paint Transformer: Feed Forward Neural Painting with Stroke Prediction, [[Paper]](https://arxiv.org/pdf/2108.03798.pdf), [[Code-1]](https://github.com/PaddlePaddle/PaddleGAN), [[Code-2]](https://github.com/Huage001/PaintTransformer)

- (arXiv 2021.08) TransForensics: **Image Forgery Localization** with Dense Self-Attention, [[Paper]](https://arxiv.org/pdf/2108.03871.pdf)

- (arXiv 2021.08) TriTransNet: **RGB-D Salient Object Detection** with a Triplet Transformer Embedding Network, [[Paper]](https://arxiv.org/pdf/2108.03990.pdf)

- (arXiv 2021.08) **Image Retrieval** on Real-life Images with Pre-trained Vision-and-Language Models, [[Paper]](https://arxiv.org/pdf/2108.04024.pdf), [[Code]](https://cuberick-orion.github.io/CIRR/)

- (arXiv 2021.08) The Right to Talk: An **Audio-Visual** Transformer Approach, [[Paper]](https://arxiv.org/pdf/2108.03256.pdf)

- (arXiv 2021.08) PSViT: Better Vision Transformer via **Token Pooling** and **Attention Sharing**, [[Paper]](https://arxiv.org/pdf/2108.03428.pdf)

- (arXiv 2021.08) Unifying Global-Local Representations in **Salient Object Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2108.02759.pdf), [[Code]](https://github.com/OliverRensu/GLSTR)

- (arXiv 2021.08) Boosting **Few-shot Semantic Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2108.02266.pdf), [[Code]](https://github.com/GuoleiSun/TRFS)

- (arXiv 2021.08) Vision Transformer with **Progressive Sampling**, [[Paper]](https://arxiv.org/pdf/2108.01684.pdf), [[Code]](https://github.com/yuexy/PS-ViT)

- (arXiv 2021.08) Armour: **Generalizable Compact Self-Attention** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.01778.pdf)

- (arXiv 2021.08) Evo-ViT: Slow-Fast Token Evolution for **Dynamic** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2108.01390.pdf)

- (arXiv 2021.08) S^2-MLPV2: IMPROVED SPATIAL-SHIFT **MLP** ARCHITECTURE FOR VISION, [[Paper]](https://arxiv.org/pdf/2108.01072.pdf)

- (arXiv 2021.08) Congested **Crowd Instance Localization** with Dilated Convolutional Swin Transformer, [[Paper]](https://arxiv.org/pdf/2108.00584.pdf)

- (arXiv 2021.08) Multi-Head Self-Attention via Vision Transformer for **Zero-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2108.00045.pdf)

- (arXiv 2021.08) CROSSFORMER: A VERSATILE VISION TRANSFORMER BASED ON **CROSS-SCALE ATTENTION**, [[Paper]](https://arxiv.org/pdf/2108.00154.pdf), [[Code]](https://github.com/cheerss/CrossFormer)

- (arXiv 2021.08) Word2Pix: Word to Pixel Cross Attention Transformer in **Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2108.00205.pdf)

- (arXiv 2021.08) Transformer-based deep **imitation learning** for dual-arm robot manipulation, [[Paper]](https://arxiv.org/pdf/2108.00385.pdf)

- (arXiv 2021.08) GTNet:Guided Transformer Network for Detecting **Human-Object Interactions**, [[Paper]](https://arxiv.org/pdf/2108.00596.pdf), [[Code]](https://github.com/UCSB-VRL/GTNet)

### 2021.07
- (arXiv 2021.07) Perceiver IO: A **General Architecture** for Structured Inputs & Outputs, [[Paper]](https://arxiv.org/pdf/2107.14795.pdf), [[Code]](https://dpmd.ai/perceiver-code)

- (arXiv 2021.07) DPT: **Deformable Patch**-based Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2107.14467.pdf), [[Code]](https://github.com/CASIA-IVA-Lab/DPT)

- (arXiv 2021.07) Product1M: Towards Weakly Supervised Instance-Level **Product Retrieval** via **Cross-modal Pretraining**, [[Paper]](https://arxiv.org/pdf/2107.14572.pdf)

- (arXiv 2021.07) Exceeding the Limits of **Visual-Linguistic Multi-Task Learning**, [[Paper]](https://arxiv.org/pdf/2107.13054.pdf)

- (arXiv 2021.07) UIBert: Learning Generic Multimodal Representations for **UI Understanding**, [[Paper]](https://arxiv.org/pdf/2107.13731.pdf)

- (arXiv 2021.07) Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for **Video Anomaly Detection**, [[Paper]](https://arxiv.org/pdf/2107.13720.pdf)

- (arXiv 2021.07) A Unified Efficient Pyramid Transformer for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2107.14209.pdf)

- (arXiv 2021.07) PPT Fusion: Pyramid Patch Transformer for a Case Study in **Image Fusion**, [[Paper]](https://arxiv.org/pdf/2107.13967.pdf)

- (arXiv 2021.07) ReFormer: The Relational Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2107.14178.pdf)

- (arXiv 2021.07) Rethinking and Improving **Relative Position Encoding** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2107.14222.pdf), [[Code]](https://github.com/microsoft/Cream/tree/main/iRPE)

- (arXiv 2021.07) **Statistically** Meaningful Approximation: a Case Study on **Approximating Turing Machines** with Transformers, [[Paper]](https://arxiv.org/pdf/2107.13163.pdf)

- (arXiv 2021.07) PlaneTR: Structure-Guided Transformers for **3D Plane Recovery**, [[Paper]](https://arxiv.org/pdf/2107.13108.pdf), [[Code]](https://git.io/PlaneTR)

- (arXiv 2021.07) Is Object Detection Necessary for **Human-Object Interaction** Recognition? [[Paper]](https://arxiv.org/pdf/2107.13083.pdf)

- (arXiv 2021.07) Exceeding the Limits of **Visual-Linguistic** Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2107.13054.pdf)

- (arXiv 2021.07) Don’t Sweep your Learning Rate under the Rug: A Closer Look at **Cross-modal Transfer** of Pretrained Transformers, [[Paper]](https://arxiv.org/pdf/2107.12460.pdf)

- (arXiv 2021.07) Exploring Sequence Feature Alignment for **Domain Adaptive Detection** Transformers, [[Paper]](https://arxiv.org/pdf/2107.12636.pdf), [[Code]](https://github.com/encounter1997/SFA)

- (arXiv 2021.07) Go **Wider** Instead of Deeper, [[Paper]](https://arxiv.org/pdf/2107.11817.pdf)

- (arXiv 2021.07) **Contextual** Transformer Networks for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2107.12292.pdf), [[Code]](https://github.com/JDAI-CV/CoTNet)

- (arXiv 2021.07) Mixed SIGNals: **Sign Language Production** via a Mixture of Motion Primitives, [[Paper]](https://arxiv.org/pdf/2107.11317.pdf)

- (arXiv 2021.07) Query2Label: A Simple Transformer Way to **Multi-Label Classification**, [[Paper]](https://arxiv.org/pdf/2107.10834.pdf), [[Code]](https://github.com/SlongLiu/query2labels)

- (arXiv 2021.07) EAN: Event Adaptive Network for Enhanced **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2107.10771.pdf), [[Code]](https://github.com/tianyuan168326/EAN-Pytorch)

- (arXiv 2021.07) CycleMLP: A **MLP**-like Architecture for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2107.10224.pdf), [[Code]](https://github.com/ShoufaChen/CycleMLP)

- (arXiv 2021.07) Generative **Video** Transformer: Can Objects be the Words? [[Paper]](https://arxiv.org/pdf/2107.09240.pdf)

- (arXiv 2021.07) QVHIGHLIGHTS: Detecting Moments and Highlights in **Videos** via **Natural Language Queries**, [[Paper]](https://arxiv.org/pdf/2107.09609.pdf), [[Code]](https://github.com/jayleicn/moment_detr)

- (arXiv 2021.07) PICASO: Permutation-Invariant Cascaded Attentional **Set Operator**, [[Paper]](https://arxiv.org/pdf/2107.08305.pdf), [[Code]](https://github.com/samzare/PICASO)

- (arXiv 2021.07) RAMS-Trans: Recurrent Attention Multi-scale Transformer for **Fine-grained Image Recognition**, [[Paper]](https://arxiv.org/pdf/2107.08192.pdf)

- (arXiv 2021.07) OODformer: **Out-Of-Distribution Detection** Transformer, [[Paper]](https://arxiv.org/pdf/2107.08976.pdf), [[Code]](https://github.com/rajatkoner08/oodformer)

- (arXiv 2021.07) **Image Fusion** Transformer, [[Paper]](https://arxiv.org/pdf/2107.09011.pdf), [[Code]](https://github.com/Vibashan/Image-Fusion-Transformer)

- (arXiv 2021.07) ResT: An Efficient Transformer for **Visual Recognition**, [[Paper]](https://arxiv.org/pdf/2105.13677.pdf), [[Code]](https://github.com/wofmanaf/ResT)

- (arXiv 2021.07) STAR: Sparse Transformer-based **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2107.07089.pdf), [[Code]](https://github.com/imj2185/STAR)

- (arXiv 2021.07) Transformer with Peak Suppression and Knowledge Guidance for **Fine-grained Image Recognition**, [[Paper]](https://arxiv.org/pdf/2107.06538.pdf)

- (arXiv 2021.07) How Much Can **CLIP** Benefit Vision-and-Language Tasks? [[Paper]](https://arxiv.org/pdf/2107.06383.pdf)

- (arXiv 2021.07) Locally **Enhanced Self-Attention**: Rethinking Self-Attention as Local and Context Terms, [[Paper]](https://arxiv.org/pdf/2107.05637.pdf), [[Code]](https://github.com/Chenglin-Yang/LESA)

- (arXiv 2021.07) Visual Parser: Representing **Part-whole Hierarchies** with Transformers, [[Paper]](https://arxiv.org/pdf/2107.05790.pdf), [[Code]](https://github.com/kevin-ssy/ViP)

- (arXiv 2021.07) Combiner: Full Attention Transformer with **Sparse** Computation Cost, [[Paper]](https://arxiv.org/pdf/2107.05768.pdf)

- (arXiv 2021.07) Per-Pixel Classification is Not All You Need for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2107.06278.pdf), [[Project]](https://bowenc0221.github.io/maskformer)

- (arXiv 2021.07) Learning Multi-Scene **Absolute Pose Regression** with Transformers, [[Paper]](https://arxiv.org/pdf/2103.11468.pdf)

- (arXiv 2021.07) CMT: **Convolutional** Neural Networks Meet Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.06263.pdf)

- (arXiv 2021.07) HAT: Hierarchical Aggregation Transformers for **Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2107.05946.pdf), [[Code]](https://github.com/AI-Zhpp/HAT)

- (arXiv 2021.07) THE **BROWNIAN MOTION** IN THE TRANSFORMER MODEL, [[Paper]](https://arxiv.org/pdf/2107.05264.pdf)

- (arXiv 2021.07) Local-to-Global **Self-Attention** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.04735.pdf), [[Code]](https://github.com/ljpadam/LG-Transformer)

- (arXiv 2021.07) Scenes and Surroundings: **Scene Graph Generation** using Relation Transformer, [[Paper]](https://arxiv.org/pdf/2107.05448.pdf)

- (arXiv 2021.07) ViTGAN: Training **GANs** with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.04589.pdf)

- (arXiv 2021.07) Long-Short Temporal **Contrastive Learning** of **Video** Transformers, [[Paper]](https://arxiv.org/pdf/2106.09212.pdf)

- (arXiv 2021.07) PVTv2: Improved Baselines with **Pyramid** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.13797.pdf), [[Code]](https://github.com/whai362/PVT)

- (arXiv 2021.07) Learning **Vision-Guided Quadrupedal Locomotion** End-to-End with Cross-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2107.03996.pdf), [[Code]](https://rchalyang.github.io/LocoTransformer)

- (arXiv 2021.07) LanguageRefer: Spatial-Language Model for **3D Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2107.03438.pdf)

- (arXiv 2021.07) EEG-CONVTRANSFORMER FOR SINGLE-TRIAL **EEG** BASED VISUAL STIMULI CLASSIFICATION, [[Paper]](https://arxiv.org/pdf/2107.03983.pdf)

- (arXiv 2021.07) Feature Fusion Vision Transformer for **Fine-Grained Visual Categorization**, [[Paper]](https://arxiv.org/pdf/2107.02341.pdf)

- (arXiv 2021.07) Long-Short Transformer: **Efficient** Transformers for Language and Vision, [[Paper]](https://arxiv.org/pdf/2107.02192.pdf)

- (arXiv 2021.07) TransformerFusion: Monocular RGB **Scene Reconstruction** using Transformers, [[Paper]](https://arxiv.org/pdf/2107.02191.pdf)

- (arXiv 2021.07) VIDLANKD: Improving **Language Understanding** via Video-Distilled Knowledge Transfer, [[Paper]](https://arxiv.org/pdf/2107.02681.pdf), [[Code]](https://github.com/zinengtang/VidLanKD)

- (arXiv 2021.07) GLiT: Neural **Architecture Search** for Global and Local Image Transformer, [[Paper]](https://arxiv.org/pdf/2107.02960.pdf)

- (arXiv 2021.07) LEARNING VISION TRANSFORMER WITH SQUEEZE AND EXCITATION FOR FACIAL **EXPRESSION** RECOGNITION, [[Paper]](https://arxiv.org/pdf/2107.03107.pdf)

- (arXiv 2021.07) Trans4Trans: Efficient Transformer for Transparent Object **Segmentation** to Help Visually Impaired People Navigate in the Real World, [[Paper]](https://arxiv.org/pdf/2107.03172.pdf)

- (arXiv 2021.07) Long Short-Term Transformer for Online **Action Detection**, [[Paper]](https://arxiv.org/pdf/2107.03377.pdf)

- (arXiv 2021.07) VISION XFORMERS: **EFFICIENT** ATTENTION FOR IMAGE CLASSIFICATION, [[Paper]](https://arxiv.org/pdf/2107.02239.pdf)

- (arXiv 2021.07) Test-Time Personalization with a Transformer for Human **Pose** Estimation, [[Paper]](https://arxiv.org/pdf/2107.02133.pdf), [[Code]](https://liyz15.github.io/TTP/)

- (arXiv 2021.07) What Makes for **Hierarchical** Vision Transformer? [[Paper]](https://arxiv.org/pdf/2107.02174.pdf)

- (arXiv 2021.07) **Efficient** Vision Transformers via Fine-Grained Manifold Distillation, [[Paper]](https://arxiv.org/pdf/2107.01378.pdf)

- (arXiv 2021.07) **Visual Relationship Forecasting** in Videos, [[Paper]](https://arxiv.org/pdf/2107.01181.pdf)

- (arXiv 2021.07) Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service **Robots**, [[Paper]](https://arxiv.org/pdf/2107.00811.pdf)

- (arXiv 2021.07) Case Relation Transformer: A Crossmodal Language Generation Model for **Fetching Instructions**, [[Paper]](https://arxiv.org/pdf/2107.00789.pdf)

- (arXiv 2021.07) CSWin Transformer: A General Vision Transformer **Backbone** with Cross-Shaped Windows, [[Paper]](https://arxiv.org/pdf/2107.00652.pdf), [[Code]](https://github.com/microsoft/CSWin-Transformer.)

- (arXiv 2021.07) CLIP-It! Language-Guided **Video Summarization**, [[Paper]](https://arxiv.org/pdf/2107.00650.pdf), [[Code]](https://medhini.github.io/clip_it)

- (arXiv 2021.07) AutoFormer: Searching Transformers for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2107.00651.pdf), [[Code]](https://github.com/microsoft/AutoML)

- (arXiv 2021.07) Focal Self-**attention** for Local-Global Interactions in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.00641.pdf)

- (arXiv 2021.07) Global Filter Networks for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2107.00645.pdf), [[Code]](https://github.com/raoyongming/GFNet)

- (arXiv 2021.07) VideoLightFormer: Lightweight **Action Recognition** using Transformers, [[Paper]](https://arxiv.org/pdf/2107.00451.pdf)

- (arXiv 2021.07) OPT: Omni-Perception Pre-Trainer for **Cross-Modal** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2107.00249.pdf)

- (arXiv 2021.07) TransSC: Transformer-based Shape Completion for **Grasp Evaluation**, [[Paper]](https://arxiv.org/pdf/2107.00511.pdf)

- (arXiv 2021.07) Action Transformer: A Self-Attention Model for Short-Time Human **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2107.00606.pdf)

### 2021.06
- (arXiv 2021.06) Associating Objects with Transformers for **Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2106.02638.pdf), [[Code]](https://github.com/z-x-yang/AOT)

- (arXiv 2021.06) Video **Super-Resolution** Transformer, [[Paper]](https://arxiv.org/pdf/2106.06847.pdf), [[Code]](https://github.com/caojiezhang/VSR-Transformer)

- (arXiv 2021.06) **Thinking** Like Transformers, [[Paper]](https://arxiv.org/pdf/2106.06981.pdf)

- (arXiv 2021.06) **Kernel Identification** Through Transformers, [[Paper]](https://arxiv.org/pdf/2106.08185.pdf)

- (arXiv 2021.06) XCiT: **Cross-Covariance** Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.09681.pdf)

- (arXiv 2021.06) THUNDR: Transformer-based **3D HUmaN Reconstruction** with Markers, [[Paper]](https://arxiv.org/pdf/2106.09336.pdf)

- (arXiv 2021.06) Probing **Image–Language** Transformers for Verb Understanding, [[Paper]](https://arxiv.org/pdf/2106.09141.pdf)

- (arXiv 2021.06) How to **train** your ViT? Data, Augmentation, and Regularization in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.10270.pdf), [[Code]](https://github.com/google-research/vision_transformer), [[Model]](https://github.com/rwightman/pytorch-image-models)

- (arXiv 2021.06) End-to-end Temporal **Action Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2106.10271.pdf), [[Code]](https://github.com/xlliu7/TadTR)

- (arXiv 2021.06) **Efficient** Self-supervised Vision Transformers for Representation Learning, [[Paper]](https://arxiv.org/pdf/2106.09785.pdf)

- (arXiv 2021.06) CLIP2Video: Mastering Video-Text **Retrieval** via Image CLIP, [[Paper]](https://arxiv.org/pdf/2106.11097.pdf), [[Code]](https://github.com/CryhanFang/CLIP2Video)

- (arXiv 2021.06) Keeping Your Eye on the Ball: **Trajectory Attention** in Video Transformers, [[Paper]](https://arxiv.org/pdf/2106.05392.pdf), [[Code]](https://github.com/facebookresearch/Motionformer)

- (arXiv 2021.06) Transformed **ROIs** for Capturing Visual Transformations in Videos, [[Paper]](https://arxiv.org/pdf/2106.03162.pdf)

- (arXiv 2021.06) Transformer in **Convolutional** Neural Networks, [[Paper]](https://arxiv.org/pdf/2106.03180.pdf), [[Code]](https://github.com/yun-liu/TransCNN)

- (arXiv 2021.06) **Video Instance Segmentation** using Inter-Frame Communication Transformers, [[Paper]](https://arxiv.org/pdf/2106.03299.pdf)

- (arXiv 2021.06) Patch Slimming for **Efficient** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02852.pdf)

- (arXiv 2021.06) CAPE: Encoding Relative **Positions** with Continuous Augmented Positional Embeddings, [[Paper]](https://arxiv.org/pdf/2106.03143.pdf)

- (arXiv 2021.06) RegionViT: **Regional-to-Local Attention** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02689.pdf)

- (arXiv 2021.06) **Motion Planning** Transformers: One Model to Plan Them All, [[Paper]](https://arxiv.org/pdf/2106.02791.pdf)

- (arXiv 2021.06) Oriented Object **Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2106.03146.pdf)

- (arXiv 2021.06) Referring Transformer: A One-step Approach to Multi-task **Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2106.03089.pdf)

- (arXiv 2021.06) Grounding **inductive biases** in natural images: invariance stems from variations in data, [[Paper]](https://arxiv.org/pdf/2106.05121.pdf)

- (arXiv 2021.06) CoAtNet: Marrying **Convolution and Attention** for All Data Sizes, [[Paper]](https://arxiv.org/pdf/2106.04803.pdf)

- (arXiv 2021.06) **Scaling** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04560.pdf)

- (arXiv 2021.06) Uformer: A General U-Shaped Transformer for **Image Restoration**, [[Paper]](https://arxiv.org/pdf/2106.03106.pdf), [[Code]](https://github.com/ZhendongWang6/Uformer)

- (arXiv 2021.06) Visual Transformer for Task-aware **Active Learning**, [[Paper]](https://arxiv.org/pdf/2106.03801.pdf), [[Code]](https://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning)

- (arXiv 2021.06) Chasing **Sparsity** in Vision Transformers: An End-to-End Exploration, [[Paper]](https://arxiv.org/pdf/2106.04533.pdf), [[Code]](https://github.com/VITA-Group/SViTE)

- (arXiv 2021.06) DETReg: Unsupervised Pretraining with Region Priors for Object **Detection**, [[Paper]](https://arxiv.org/pdf/2106.04550.pdf), [[Code]](https://amirbar.net/detreg)

- (arXiv 2021.06) MVT: MASK VISION TRANSFORMER FOR FACIAL **EXPRESSION** RECOGNITION IN THE WILD, [[Paper]](https://arxiv.org/pdf/2106.04520.pdf)

- (arXiv 2021.06) **Demystifying** Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight, [[Paper]](https://arxiv.org/pdf/2106.04263.pdf)

- (arXiv 2021.06) Diverse Part Discovery: Occluded Person **Re-identification** with Part-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2106.04095.pdf)

- (arXiv 2021.06) MlTr: **Multi-label Classification** with Transformer, [[Paper]](https://arxiv.org/pdf/2106.06195.pdf), [[Code]](https://github.com/starmemda/MlTr/)

- (arXiv 2021.06) Going Beyond **Linear** Transformers with Recurrent Fast Weight Programmers, [[Paper]](https://arxiv.org/pdf/2106.06295.pdf), [[Code]](https://github.com/IDSIA/recurrent-fwp)

- (arXiv 2021.06) On Improving **Adversarial Transferability** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04169.pdf), [[Code]](https://git.io/JZmG3)

- (arXiv 2021.06) Fully Transformer Networks for Semantic Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2106.04108.pdf)

- (arXiv 2021.06) MST: Masked Self-Supervised Transformer for **Visual Representation**, [[Paper]](https://arxiv.org/pdf/2106.05656.pdf)

- (arXiv 2021.06) Space-time Mixing **Attention** for Video Transformer, [[Paper]](https://arxiv.org/pdf/2106.05968.pdf)

- (arXiv 2021.06) VIT-INCEPTION-GAN FOR **IMAGE COLOURISING**, [[Paper]](https://arxiv.org/pdf/2106.06321.pdf)

- (arXiv 2021.06) HYBRID **GENERATIVE-CONTRASTIVE REPRESENTATION** LEARNING, [[Paper]](https://arxiv.org/pdf/2106.06162.pdf), [[Code]](https://github.com/kakaobrain/gcrl)

- (arXiv 2021.06) OadTR: Online **Action Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.11149.pdf), [[Code]](https://github.com/wangxiang1230/OadTR)

- (arXiv 2021.06) VIMPAC: **Video** Pre-Training via Masked Token Prediction and Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2106.11250.pdf), [[Code]](https://github.com/airsplay/vimpac)

- (arXiv 2021.06) Delving Deep into the **Generalization** of Vision Transformers under **Distribution Shifts**, [[Paper]](https://arxiv.org/pdf/2106.07617.pdf), [[Code]](https://github.com/Phoenix1153/ViT_OOD_generalization)

- (arXiv 2021.06) Improved Transformer for **High-Resolution GANs**, [[Paper]](https://arxiv.org/pdf/2106.07631.pdf)

- (arXiv 2021.06) Towards Long-Form **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2106.11310.pdf), [[Code]](https://github.com/chaoyuaw/lvu)

- (arXiv 2021.06) TokenLearner: What Can 8 Learned **Tokens** Do for Images and Videos? [[Paper]](https://arxiv.org/pdf/2106.11297.pdf)

- (arXiv 2021.06) More than Encoder: Introducing Transformer Decoder to **Upsample**, [[Paper]](https://arxiv.org/pdf/2106.10637.pdf)

- (arXiv 2021.06) A Picture May Be Worth a Hundred Words for **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2106.13445.pdf)

- (arXiv 2021.06) Probing Inter-modality: Visual Parsing with Self-Attention for **Vision-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2106.13488.pdf)

- (arXiv 2021.06) **Shape registration** in the time of transformers, [[Paper]](https://arxiv.org/pdf/2106.13679.pdf)

- (arXiv 2021.06) Vision Transformer **Architecture Search**, [[Paper]](https://arxiv.org/pdf/2106.13700.pdf), [[Code]](https://github.com/xiusu/ViTAS)

- (arXiv 2021.06) Unified Questioner Transformer for **Descriptive Question Generation** in Goal-Oriented Visual Dialogue, [[Paper]](https://arxiv.org/pdf/2106.15550.pdf)

- (arXiv 2021.06) **Multi-Exit** Vision Transformer for Dynamic Inference, [[Paper]](https://arxiv.org/pdf/2106.15183.pdf)

- (arXiv 2021.06) Early **Convolutions** Help Transformers See Better, [[Paper]](https://arxiv.org/pdf/2106.14881.pdf)

- (arXiv 2021.06) Rethinking Token-Mixing **MLP** for MLP-based Vision Backbone, [[Paper]](https://arxiv.org/pdf/2106.14882.pdf)

- (arXiv 2021.06) Augmented **Shortcuts** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.15941.pdf)

- (arXiv 2021.06) CAT: **Cross Attention** in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.05786.pdf), [[Code]](https://github.com/linhezheng19/CAT)

- (arXiv 2021.06) Post-Training **Quantization** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.14156.pdf)

- (arXiv 2021.06) Attention Bottlenecks for **Multimodal** Fusion, [[Paper]](https://arxiv.org/pdf/2107.00135.pdf)

- (arXiv 2021.06) Improving the **Efficiency** of Transformers for Resource-Constrained Devices, [[Paper]](https://arxiv.org/pdf/2106.16006.pdf)

- (arXiv 2021.06) **Multimodal** Few-Shot Learning with Frozen Language Models, [[Paper]](https://arxiv.org/pdf/2106.13884.pdf)

- (arXiv 2021.06) Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object **Detection** and **Segmentation**, [[Paper]](https://arxiv.org/pdf/2106.11401.pdf)

- (arXiv 2021.06) Exploring Vision Transformers for Fine-grained **Classification**, [[Paper]](https://arxiv.org/pdf/2106.10587.pdf), [[Code]](https://github.com/mv-lab/ViT-FGVC8)

- (arXiv 2021.06) S^2-MLP: Spatial-Shift **MLP** Architecture for Vision, [[Paper]](https://arxiv.org/pdf/2106.07477.pdf)

- (arXiv 2021.06) Styleformer: Transformer based **Generative Adversarial Networks** with Style Vector, [[Paper]](https://arxiv.org/pdf/2106.07023.pdf), [[Code]](https://github.com/Jeeseung-Park/Styleformer)

- (arXiv 2021.06) ViTAE: Vision Transformer Advanced by Exploring **Intrinsic Inductive Bias**, [[Paper]](https://arxiv.org/pdf/2106.03348.pdf), [[Code]](https://github.com/Annbless/ViTAE)

- (arXiv 2021.06) Shuffle Transformer: Rethinking **Spatial Shuffle** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.03650.pdf)

- (arXiv 2021.06) Refiner: **Refining Self-attention** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.03714.pdf), [[Code]](https://github.com/zhoudaquan/Refiner_ViT)

- (arXiv 2021.06) Person **Re-Identification** with a Locally Aware Transformer, [[Paper]](https://arxiv.org/pdf/2106.03720.pdf)

- (arXiv 2021.06) **Efficient** Training of Visual Transformers with **Small-Size Datasets**, [[Paper]](https://arxiv.org/pdf/2106.03746.pdf)

- (arXiv 2021.06) **Glance-and-Gaze** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.02277.pdf), [[Code]](https://github.com/yucornetto/GG-Transformer)

- (arXiv 2021.06) **Few-Shot Segmentation** via Cycle-Consistent Transformer, [[Paper]](https://arxiv.org/pdf/2106.02320.pdf)

- (arXiv 2021.06) Semantic **Correspondence** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.02520.pdf), [[Code]](https://github.com/SunghwanHong/CATs)

- (arXiv 2021.06) THE **IMAGE LOCAL AUTOREGRESSIVE** TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2106.02514.pdf)

- (arXiv 2021.06) MERLOT: Multimodal **Neural Script Knowledge** Models, [[Paper]](https://arxiv.org/pdf/2106.02636.pdf), [[Project]](https://rowanzellers.com/merlot)

- (arXiv 2021.06) SOLQ: **Segmenting** Objects by Learning Queries, [[Paper]](https://arxiv.org/pdf/2106.02351.pdf), [[Code]](https://github.com/megvii-research/SOLQ)

- (arXiv 2021.06) Personalizing **Pre-trained Models**, [[Paper]](https://arxiv.org/pdf/2106.01499.pdf), [[Code]](https://github.com/PAL-ML/CLIPPER)

- (arXiv 2021.06) E2E-VLP: End-to-End **Vision-Language Pre-training** Enhanced by Visual Learning, [[Paper]](https://arxiv.org/pdf/2106.01804.pdf)

- (arXiv 2021.06) VOLO: Vision Outlooker for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2106.13112.pdf), [[Code]](https://github.com/sail-sg/volo)

- (arXiv 2021.06) Container: **Context Aggregation** Network, [[Paper]](https://arxiv.org/pdf/2106.01401.pdf)

- (arXiv 2021.06) Exploring Corruption **Robustness**: Inductive Biases in Vision Transformers and MLP-Mixers, [[Paper]](https://arxiv.org/pdf/2106.13122.pdf)

- (arXiv 2021.06) Video **Swin** Transformer, [[Paper]](https://arxiv.org/pdf/2106.13230.pdf), [[Code]](https://github.com/SwinTransformer/Video-Swin-Transformer)

- (arXiv 2021.06) IA-RED^2: Interpretability-Aware **Redundancy Reduction** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.12620.pdf), [[Code]](http://people.csail.mit.edu/bpan/ia-red/)

- (arXiv 2021.06) AudioCLIP: Extending **CLIP** to Image, Text and Audio, [[Paper]](https://arxiv.org/pdf/2106.13043.pdf)

- (arXiv 2021.06) VISION PERMUTATOR: A PERMUTABLE MLP-LIKE ARCHITECTURE FOR VISUAL **RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2106.12368.pdf), [[Code]](https://github.com/Andrew-Qibin/VisionPermutator)

- (arXiv 2021.06) Co-advise: Cross Inductive Bias **Distillation**, [[Paper]](https://arxiv.org/pdf/2106.12378.pdf)

- (arXiv 2021.06) Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2106.12023.pdf)

- (arXiv 2021.06) P2T: Pyramid Pooling Transformer for **Scene Understanding**, [[Paper]](https://arxiv.org/pdf/2106.12011.pdf), [[Code]](https://github.com/yuhuan-wu/P2T)

- (arXiv 2021.06) LegoFormer: Transformers for Block-by-Block **Multi-view 3D Reconstruction**, [[Paper]](https://arxiv.org/pdf/2106.12102.pdf)

- (arXiv 2021.06) Stable, Fast and Accurate: Kernelized Attention with Relative **Positional Encoding**, [[Paper]](https://arxiv.org/pdf/2106.12566.pdf)

- (arXiv 2021.06) MODETR: Moving Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.11422.pdf)

- (arXiv 2021.06) ResMLP: **Feedforward networks** for image classification with data-efficient training, [[Paper]](https://arxiv.org/pdf/2105.03404.pdf)

- (arXiv 2021.06) Multi-**head** or Single-**head**? An Empirical Comparison for Transformer Training, [[Paper]](https://arxiv.org/pdf/2106.09650.pdf)

- (arXiv 2021.06) Dynamic Head: Unifying Object **Detection** Heads with Attentions, [[Paper]](https://arxiv.org/pdf/2106.08322v1.pdf), [[Code]](https://github.com/microsoft/DynamicHead)

- (arXiv 2021.06) MLP-Mixer: An all-**MLP** Architecture for Vision, [[Paper]](https://arxiv.org/pdf/2105.01601.pdf), [[Code]](https://github.com/google-research/vision_transformer)

- (arXiv 2021.06) BEIT: BERT **Pre-Training** of Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.08254.pdf), [[Code]](https://aka.ms/beit)

- (arXiv 2021.06) Scaling Vision with **Sparse** Mixture of Experts, [[Paper]](https://arxiv.org/pdf/2106.05974.pdf)

- (arXiv 2021.06) Towards Training Stronger Video Vision Transformers for EPIC-KITCHENS-100 **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2106.05058.pdf)

- (arXiv 2021.06) Semi-Supervised 3D **Hand-Object Poses Estimation** with Interactions in Time, [[Paper]](https://arxiv.org/pdf/2106.05266.pdf), [[Code]](https://stevenlsw.github.io/Semi-Hand-Object)

- (arXiv 2021.06) DynamicViT: **Efficient** Vision Transformers with Dynamic Token Sparsification, [[Paper]](https://arxiv.org/pdf/2106.02034.pdf), [[Code]](https://github.com/raoyongming/DynamicViT)

- (arXiv 2021.06) SCTN: Sparse Convolution-Transformer Network for **Scene Flow Estimation**, [[Paper]](https://arxiv.org/pdf/2105.04447.pdf)

- (arXiv 2021.06) **Anticipative Video** Transformer, [[Paper]](https://arxiv.org/pdf/2106.02036.pdf), [[Project]](https://facebookresearch.github.io/AVT/)

- (arXiv 2021.06) Pay Attention to **MLPs**, [[Paper]](https://arxiv.org/pdf/2105.08050.pdf)

- (arXiv 2021.06) When Vision Transformers Outperform **ResNets** without Pretraining or Strong Data Augmentations, [[Paper]](https://arxiv.org/pdf/2106.01548.pdf)

- (arXiv 2021.06) StyTr^2: Unbiased Image **Style Transfer** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.14576.pdf)

- (arXiv 2021.06) THG:Transformer with **Hyperbolic Geometry**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2106/2106.07350.pdf)

- (arXiv 2021.06) You Only Look at One Sequence: Rethinking Transformer in Vision through Object **Detection**, [[Paper]](https://arxiv.org/pdf/2106.00666.pdf), [[Code]](https://github.com/hustvl/YOLOS)

- (arXiv 2021.06) TransVOS: **Video Object Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.00588.pdf)

- (2021.06) **Reinforcement Learning** as One Big Sequence Modeling Problem, [[Paper]](https://people.eecs.berkeley.edu/~janner/trajectory-transformer/files/trajectory-transformer.pdf), [[Project]](https://trajectory-transformer.github.io/)

- (arXiv 2021.06) Less is More: Pay Less **Attention** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.14217.pdf), [[Code]](https://github.com/MonashAI/LIT)

- (arXiv 2021.06) SegFormer: Simple and Efficient Design for **Semantic Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.15203.pdf), [[Code]](https://github.com/NVlabs/SegFormer)

### 2021.05
- (arXiv 2021.05) KVT: k-NN **Attention** for Boosting Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.00515.pdf)

- (arXiv 2021.05) Memory-Efficient Differentiable Transformer **Architecture Search**, [[Paper]](https://arxiv.org/pdf/2105.14669.pdf)

- (arXiv 2021.05) An **Attention Free** Transformer, [[Paper]](https://arxiv.org/pdf/2105.14103.pdf)

- (arXiv 2021.05) On the Bias Against **Inductive Biases**, [[Paper]](https://arxiv.org/pdf/2105.14077.pdf)

- (arXiv 2021.05) MixerGAN: An MLP-Based Architecture for **Unpaired Image-to-Image Translation**, [[Paper]](https://arxiv.org/pdf/2105.14110.pdf)

- (arXiv 2021.05) Transformer-Based Source-Free **Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2105.14138.pdf), [[Code]](https://github.com/ygjwd12345/TransDA)

- (arXiv 2021.05) FoveaTer: Foveated Transformer for **Image Classification**, [[Paper]](https://arxiv.org/pdf/2105.14173.pdf)

- (arXiv 2021.05) UFC-BERT: Unifying Multi-Modal Controls for Conditional **Image Synthesis**, [[Paper]](https://arxiv.org/pdf/2105.14211.pdf)

- (arXiv 2021.05) **Gaze Estimation** using Transformer, [[Paper]](https://arxiv.org/pdf/2105.14424.pdf), [[Code]](https://github.com/yihuacheng/GazeTR)

- (arXiv 2021.05) Transformer-Based Deep Image Matching for Generalizable Person Re-identification, [[Paper]](https://arxiv.org/pdf/2105.14432.pdf), [[Project]](https://liaosc.wordpress.com/)

- (arXiv 2021.05) Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with **Adaptive Sequence Length**, [[Paper]](https://arxiv.org/pdf/2105.15075.pdf)

- (arXiv 2021.05) Analogous to **Evolutionary Algorithm**: Designing a Unified Sequence Model, [[Paper]](https://arxiv.org/pdf/2105.15089.pdf)

- (arXiv 2021.05) MSG-Transformer: Exchanging **Local Spatial Information** by Manipulating Messenger Tokens, [[Paper]](https://arxiv.org/pdf/2105.15168.pdf), [[Code]](https://github.com/hustvl/MSG-Transformer)

- (arXiv 2021.05) Sequence Parallelism: Making 4D **Parallelism** Possible, [[Paper]](https://arxiv.org/pdf/2105.13120.pdf)

- (arXiv 2021.05) CogView: Mastering **Text-to-Image Generation** via Transformers, [[Paper]](https://arxiv.org/pdf/2105.13290.pdf), [[Code]](https://github.com/THUDM/CogView)

- (arXiv 2021.05) TrTr: Visual **Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2105.03817.pdf), [[Code]](https://github.com/tongtybj/TrTr)

- (arXiv 2021.05) Conformer: Local Features Coupling Global Representations for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2105.03889.pdf), [[Code]](https://github.com/pengzhiliang/Conformer)

- (arXiv 2021.05) Visual **Grounding** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.04281.pdf)

- (arXiv 2021.05) **Self-Supervised** Learning with Swin Transformers, [[Paper]](https://arxiv.org/pdf/2105.04553.pdf), [[Code]](https://github.com/SwinTransformer/Transformer-SSL)

- (arXiv 2021.05) Are Pre-trained **Convolutions** Better than Pre-trained Transformers? [[Paper]](https://arxiv.org/pdf/2105.03322.pdf)

- (arXiv 2021.05) MOTR: End-to-End Multiple-Object **Tracking** with TRansformer, [[Paper]](https://arxiv.org/pdf/2105.03247.pdf), [[Code]](https://github.com/megvii-model/MOTR)

- (arXiv 2021.05) Attention for **Image Registration** (AiR): an unsupervised Transformer approach, [[Paper]](https://arxiv.org/pdf/2105.02282.pdf), [[Code]](https://gitlab.inria.fr/zihwang/transformer-for-image-registration)

- (arXiv 2021.05) EXPLORING EXPLICIT AND IMPLICIT VISUAL RELATIONSHIPS FOR IMAGE **CAPTIONING**, [[Paper]](https://arxiv.org/pdf/2105.02391.pdf)

- (arXiv 2021.05) **Computer-Aided Design** as Language, [[Paper]](https://arxiv.org/pdf/2105.02769.pdf)

- (arXiv 2021.05) FLEX: Parameter-free Multi-view 3D Human **Motion Reconstruction**, [[Paper]](https://arxiv.org/pdf/2105.01937.pdf), [[Project]](https://briang13.github.io/FLEX/)

- (arXiv 2021.05) TransHash: Transformer-based Hamming Hashing for Efficient Image **Retrieval**, [[Paper]](https://arxiv.org/pdf/2105.01823.pdf)

- (arXiv 2021.05) High-Resolution Complex **Scene Synthesis** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.06458.pdf)

- (arXiv 2021.05) Episodic Transformer for Vision-and-Language **Navigation**, [[Paper]](https://arxiv.org/pdf/2105.06453.pdf)

- (arXiv 2021.05) Towards **Robust** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2105.07926.pdf), [[Code]](https://git.io/Jswdk)

- (arXiv 2021.05) Vision Transformers are **Robust** Learners, [[Paper]](https://arxiv.org/pdf/2105.07581.pdf), [[Code]](https://git.io/J3VO0)

- (arXiv 2021.05) ISTR: End-to-End Instance **Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.00637.pdf), [[Code]](https://github.com/hujiecpp/ISTR)

- (arXiv 2021.05) SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale **Place Recognition**, [[Paper]](https://arxiv.org/pdf/2105.00149.pdf)

- (arXiv 2021.05) Rethinking **Skip** Connection with Layer Normalization in Transformers and ResNets, [[Paper]](https://arxiv.org/pdf/2105.07205.pdf)

- (arXiv 2021.05) IntFormer: **Predicting pedestrian intention** with the aid of the Transformer architecture, [[Paper]](https://arxiv.org/pdf/2105.08647.pdf)

- (arXiv 2021.05) Parallel Attention Network with Sequence Matching for Video **Grounding**, [[Paper]](https://arxiv.org/pdf/2105.08481.pdf), [[Code]](https://github.com/IsaacChanghau/SeqPAN)

- (arXiv 2021.05) Relative **Positional Encoding** for Transformers with Linear Complexity, [[Paper]](https://arxiv.org/pdf/2105.08399.pdf)

- (arXiv 2021.05) VTNET: VISUAL TRANSFORMER NETWORK FOR OBJECT GOAL **NAVIGATION**, [[Paper]](https://arxiv.org/pdf/2105.09447.pdf)

- (arXiv 2021.05) DeepCAD: A Deep Generative Network for **Computer-Aided Design** Models, [[Paper]](https://arxiv.org/pdf/2105.09492.pdf)

- (arXiv 2021.05) Single-Layer Vision Transformers for More Accurate **Early Exits** with Less Overhead, [[Paper]](https://arxiv.org/pdf/2105.09121.pdf)

- (arXiv 2021.05) An **Attention Free** Transformer, [[Paper]](https://arxiv.org/pdf/2105.14103.pdf)

- (arXiv 2021.05) Beyond Self-attention: **External Attention** using Two Linear Layers for Visual Tasks, [[Paper]](https://arxiv.org/pdf/2105.02358.pdf), [[Code]](https://github.com/MenghaoGuo/-EANet)

- (arXiv 2021.05) Combining Transformer Generators with Convolutional **Discriminators**, [[Paper]](https://arxiv.org/pdf/2105.10189.pdf)

- (arXiv 2021.05) VLM: Task-agnostic **Video-Language** Model Pre-training for Video Understanding, [[Paper]](https://arxiv.org/pdf/2105.09996.pdf)

- (arXiv 2021.05) Improving Generation and Evaluation of **Visual Stories** via Semantic Consistency, [[Paper]](https://arxiv.org/pdf/2105.10026.pdf), [[Code]](https://github.com/adymaharana/StoryViz)

- (arXiv 2021.05) BELT: Blockwise **Missing Embedding Learning** Transfomer, [[Paper]](https://arxiv.org/pdf/2105.10360.pdf)

- (arXiv 2021.05) End-to-End Video Object **Detection** with Spatial-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2105.10920.pdf), [[Code]](https://github.com/SJTU-LuHe/TransVOD)

- (arXiv 2021.05) SAT: 2D Semantics Assisted Training for 3D Visual **Grounding**, [[Paper]](https://arxiv.org/pdf/2105.11450.pdf)

- (arXiv 2021.05) Aggregating **Nested** Transformers, [[Paper]](https://arxiv.org/pdf/2105.12723.pdf)

- (arXiv 2021.05) Intriguing **Properties** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.10497.pdf), [[Code]](https://git.io/Js15X)

- (arXiv 2021.05) Temporal **Action Proposal Generation** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.12043.pdf)

- (arXiv 2021.05) Learning Better **Visual Dialog Agents** with Pretrained Visual-Linguistic Representation, [[Paper]](https://arxiv.org/pdf/2105.11541.pdf), [[Code]](https://github.com/amazon-research/read-up)

- (arXiv 2021.05) Perceptual **Image Quality Assessment** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.14730.pdf)

- (arXiv 2021.05) Do You Even Need Attention? A Stack of **Feed-Forward Layers** Does Surprisingly Well on ImageNet, [[Paper]](https://arxiv.org/pdf/2105.02723v1.pdf), [[Code]](https://github.com/lukemelas/do-you-even-need-attention)

- (arXiv 2021.05) Pay Attention to **MLPs**, [[Paper]](https://arxiv.org/pdf/2105.08050.pdf)

- (arXiv 2021.05) ResMLP: Feedforward networks for image **classification** with data-efficient training, [[Paper]](https://arxiv.org/pdf/2105.03404v1.pdf)

- (arXiv 2021.05) RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image **Recognition**, [[Paper]](https://arxiv.org/pdf/2105.01883v1.pdf), [[Code]](https://github.com/DingXiaoH/RepMLP)

- (arXiv 2021.05) Are Convolutional Neural Networks or Transformers more like **human vision**? [[Paper]](https://arxiv.org/pdf/2105.07197.pdf)

- (arXiv 2021.05) FNet: Mixing Tokens with **Fourier** Transforms, [[Paper]](https://arxiv.org/pdf/2105.03824.pdf)

- (arXiv 2021.05) Segmenter: Transformer for Semantic **Segmentation**, [[Paper]](https://arxiv.org/pdf/2105.05633.pdf), [[Code]](https://github.com/rstrudel/segmenter)

- (arXiv 2021.05) TransHash: Transformer-based Hamming Hashing for **Efficient** Image Retrieval, [[Paper]](https://arxiv.org/pdf/2105.01823.pdf)

- (arXiv 2021.05) Visual **Composite Set Detection** Using Part-and-Sum Transformers, [[Paper]](https://arxiv.org/pdf/2105.02170.pdf)

### 2021.04
- (arXiv 2021.04) HandsFormer: Keypoint Transformer for Monocular 3D **Pose** Estimation of Hands and Object in Interaction, [[Paper]](https://arxiv.org/pdf/2104.14639.pdf)

- (arXiv 2021.04) Chop Chop BERT: **Visual Question Answering** by Chopping VisualBERT’s Heads, [[Paper]](https://arxiv.org/pdf/2104.14741.pdf)

- (arXiv 2021.04) CoSformer: **Detecting** Co-Salient Object with Transformers, [[Paper]](https://arxiv.org/pdf/2104.14729.pdf)

- (arXiv 2021.04) CAT: Cross-Attention Transformer for One-Shot Object **Detection**, [[Paper]](https://arxiv.org/pdf/2104.14984.pdf)

- (arXiv 2021.04) Dual Transformer for **Point Cloud** Analysis, [[Paper]](https://arxiv.org/pdf/2104.13044.pdf)

- (arXiv 2021.04) Playing **Lottery Tickets** with Vision and Language, [[Paper]](https://arxiv.org/pdf/2104.11832.pdf)

- (arXiv 2021.04) M3DETR: Multi-representation, Multi-scale, Mutual-relation 3D Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.11896.pdf)

- (arXiv 2021.04) RelTransformer: Balancing the **Visual Relationship** Detection from Local Context, Scene and Memory, [[Paper]](https://arxiv.org/pdf/2104.11934.pdf), [[Code]](https://github.com/Vision-CAIR/RelTransformer)

- (arXiv 2021.04) MDETR-Modulated Detection for End-to-End **Multi-Modal** Understanding, [[Paper]](https://arxiv.org/pdf/2104.12763.pdf), [[Code]](https://github.com/ashkamath/mdetr)

- (arXiv 2021.04) Rich Semantics Improve **Few-shot** Learning, [[Paper]](https://arxiv.org/pdf/2104.12709.pdf), [[Code]](https://github.com/MohamedAfham/RS_FSL)

- (arXiv 2021.04) Effect of Vision-and-Language Extensions on Natural Language Understanding in **Vision-and-Language** Models, [[Paper]](https://arxiv.org/pdf/2104.08066.pdf)

- (arXiv 2021.04) Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with **56M Parameters** on ImageNet, [[Paper]](https://arxiv.org/pdf/2104.10858.pdf), [[Code]](https://github.com/zihangJiang/TokenLabeling)

- (arXiv 2021.04) So-ViT: Mind Visual **Tokens** for Vision Transforme, [[Paper]](https://arxiv.org/pdf/2104.10935.pdf)

- (arXiv 2021.04) **Multiscale** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.11227.pdf), [[Code]](https://github.com/facebookresearch/SlowFast)

- (arXiv 2021.04) M2TR: Multi-modal Multi-scale Transformers for **Deepfake** Detection, [[Paper]](https://arxiv.org/pdf/2104.09770.pdf)

- (arXiv 2021.04) Transformer Transforms Salient Object Detection and Camouflaged Object **Detection**, [[Paper]](https://arxiv.org/pdf/2104.10127.pdf)

- (arXiv 2021.04) T2VLAD: Global-Local Sequence Alignment for Text-Video **Retrieval**, [[Paper]](https://arxiv.org/pdf/2104.10054.pdf)

- (arXiv 2021.04) VT-ADL: A Vision Transformer Network for Image **Anomaly** Detection and Localization, [[Paper]](https://arxiv.org/pdf/2104.10036.pdf)

- (arXiv 2021.04) Multi-Modal Fusion Transformer for End-to-End **Autonomous Driving**, [[Paper]](https://arxiv.org/pdf/2104.09224.pdf), [[Code]](https://github.com/autonomousvision/transfuser)

- (arXiv 2021.04) TransVG: End-to-End Visual **Grounding** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.08541.pdf)

- (arXiv 2021.04) Visual Transformer **Pruning**, [[Paper]](https://arxiv.org/pdf/2104.08500.pdf)

- (arXiv 2021.04) Higher Order Recurrent **Space-Time** Transformer, [[Paper]](https://arxiv.org/pdf/2104.08665.pdf), [[Code]](https://github.com/CorcovadoMing/HORST)

- (arXiv 2021.04) CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip **Retrieval**, [[Paper]](https://arxiv.org/pdf/2104.08860.pdf), [[Code]](https://github.com/ArrowLuo/CLIP4Clip)

- (arXiv 2021.04) Lessons on **Parameter Sharing** across Layers in Transformers, [[Paper]](https://arxiv.org/pdf/2104.06022.pdf)

- (arXiv 2021.04) Disentangled Motif-aware **Graph** Learning for Phrase Grounding, [[Paper]](https://arxiv.org/pdf/2104.06008.pdf)

- (arXiv 2021.04) Co-**Scale** Conv-**Attentional** Image Transformers, [[Paper]](https://arxiv.org/pdf/2104.06399.pdf), [[Code]](https://github.com/mlpc-ucsd/CoaT)

- (arXiv 2021.04) Cloth Interactive Transformer for **Virtual Try-On**, [[Paper]](https://arxiv.org/pdf/2104.05519.pdf), [[Code]](https://github.com/Amazingren/CIT)

- (arXiv 2021.04) LocalViT: Bringing **Locality** to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.05707.pdf), [[Code]](https://github.com/ofsoundof/LocalViT)

- (arXiv 2021.04) Seeing Out of tHe bOx: End-to-End Pre-training for **Vision-Language** Representation Learning, [[Paper]](https://arxiv.org/pdf/2104.03135.pdf)

- (arXiv 2021.04) Facial Attribute Transformers for Precise and Robust **Makeup Transfer**, [[Paper]](https://arxiv.org/pdf/2104.02894.pdf)

- (arXiv 2021.04) Emerging Properties in **Self-Supervised** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.14294.pdf), [[Code]](https://github.com/facebookresearch/dino?fbclid=IwAR3tq8GzNrR4BaZjjSqGyWe6qsFK1XenBZNIZCCgjJmMi0ghettvIiDRAxM)

- (arXiv 2021.04) ConTNet: Why not use **convolution** and transformer at the same time? [[Paper]](https://arxiv.org/pdf/2104.13497.pdf), [[Code]](https://github.com/yan-hao-tian/ConTNet)

- (arXiv 2021.04) **Point Cloud** Learning with Transformer, [[Paper]](https://arxiv.org/pdf/2104.13636.pdf)

- (arXiv 2021.04) Twins: Revisiting the Design of **Spatial Attention** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.13840.pdf), [[Code]](https://github.com/Meituan-AutoML/Twins)

- (arXiv 2021.04) Inpainting Transformer for **Anomaly** Detection, [[Paper]](https://arxiv.org/pdf/2104.13897.pdf)

- (arXiv 2021.04) Shot Contrastive Self-Supervised Learning for **Scene Boundary Detection**, [[Paper]](https://arxiv.org/pdf/2104.13537.pdf)

- (arXiv 2021.04) HOTR: End-to-End **Human-Object Interaction** Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2104.13682.pdf)

- (arXiv 2021.04) Visual **Saliency** Transformer, [[Paper]](https://arxiv.org/pdf/2104.12099.pdf)

- (arXiv 2021.04) Improve Vision Transformers Training by Suppressing **Over-smoothing**, [[Paper]](https://arxiv.org/pdf/2104.12753.pdf), [[Code]](https://github.com/ChengyueGongR/PatchVisionTransformer)

- (arXiv 2021.04) Visformer: The Vision-**friendly** Transformer, [[Paper]](https://arxiv.org/pdf/2104.12533.pdf), [[Code]](https://github.com/danczs/Visformer)

- (arXiv 2021.04) TransMOT: Spatial-Temporal Graph Transformer for Multiple Object **Tracking**, [[Paper]](https://arxiv.org/pdf/2104.00194.pdf)

- (arXiv 2021.04) **Mesh** Graphormer, [[Paper]](https://arxiv.org/pdf/2104.00272.pdf), [[Code]](https://github.com/microsoft/meshgraphormer)

- (arXiv 2021.04) TRAJEVAE - Controllable Human **Motion Generation** from Trajectories, [[Paper]](https://arxiv.org/pdf/2104.00351.pdf)

- (arXiv 2021.04) UC^2: Universal Cross-lingual Cross-modal **Vision-and-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2104.00332.pdf)

- (arXiv 2021.04) Learning to Cluster **Faces** via Transformer, [[Paper]](https://arxiv.org/pdf/2104.11502.pdf)

- (arXiv 2021.04) Skeletor: Skeletal Transformers for Robust Body-**Pose** Estimation, [[Paper]](https://arxiv.org/pdf/2104.11712.pdf)

- (arXiv 2021.04) VidTr: Video Transformer **Without Convolutions**, [[Paper]](https://arxiv.org/pdf/2104.11746.pdf)

- (arXiv 2021.04) VATT: Transformers for Multimodal Self-Supervised Learning from Raw **Video, Audio and Text**, [[Paper]](https://arxiv.org/pdf/2104.11178.pdf)

- (arXiv 2021.04) Going **deeper** with Image Transformers, [[Paper]](https://arxiv.org/pdf/2103.17239.pdf)

- (arXiv 2021.04) **EFFICIENT** PRE-TRAINING OBJECTIVES FOR TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2104.09694.pdf), [[Code]](https://github.com/iKernels/efficient-pre-training-objectives-for-transformers)

- (arXiv 2021.04) ROFORMER: ENHANCED TRANSFORMER WITH ROTARY **POSITION EMBEDDING**, [[Paper]](https://arxiv.org/pdf/2104.09864.pdf)

- (arXiv 2021.04) VideoGPT: **Video Generation** using VQ-VAE and Transformers, [[Paper]](https://arxiv.org/pdf/2104.10157.pdf), [[Code]](https://wilson1yan.github.io/videogpt/index.html)

- (arXiv 2021.04) DODRIO: Exploring Transformer Models with **Interactive Visualization**, [[Paper]](https://arxiv.org/pdf/2103.14625.pdf), [[Code]](https://poloclub.github.io/dodrio/)

- (arXiv 2021.04) Lifting Transformer for 3D Human **Pose** Estimation in Video, [[Paper]](https://arxiv.org/pdf/2103.14304.pdf)

- (arXiv 2021.04) Demystifying the Better Performance of **Position Encoding** Variants for Transformer, [[Paper]](https://arxiv.org/pdf/2104.08698.pdf)

- (arXiv 2021.04) Consistent Accelerated Inference via **Confident Adaptive** Transformers, [[Paper]](https://arxiv.org/pdf/2104.08803.pdf), [[Code]](https://github.com/TalSchuster/CATs)

- (arXiv 2021.04) Temporal Query Networks for Fine-grained **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2104.09496.pdf), [[Code]](http://www.robots.ox.ac.uk/~vgg/research/tqn/)

- (arXiv 2021.04) Face Transformer for **Recognition**, [[Paper]](https://arxiv.org/pdf/2103.14803.pdf), [[Code]](https://github.com/zhongyy/Face-Transformer)

- (arXiv 2021.04) VGNMN: Video-grounded Neural Module Network to **Video-Grounded Language** Tasks, [[Paper]](https://arxiv.org/pdf/2104.07921.pdf)

- (arXiv 2021.04) Self-supervised Video **Retrieval** Transformer Network, [[Paper]](https://arxiv.org/pdf/2104.07993.pdf)

- (arXiv 2021.04) Cross-Modal Retrieval Augmentation for Multi-Modal **Classification**, [[Paper]](https://arxiv.org/pdf/2104.08108.pdf)

- (arXiv 2021.04) Point-Based Modeling of **Human Clothing**, [[Paper]](https://arxiv.org/pdf/2104.08230.pdf)

- (arXiv 2021.04) Points as Queries: Weakly Semi-supervised Object **Detection** by Points, [[Paper]](https://arxiv.org/pdf/2104.07434.pdf)

- (arXiv 2021.04) Geometry-Free **View Synthesis**: Transformers and no 3D Priors, [[Paper]](https://arxiv.org/pdf/2104.07652.pdf), [[Code]](https://git.io/JOnwn)

- (arXiv 2021.04) Self-supervised Video Object **Segmentation** by Motion Grouping, [[Paper]](https://arxiv.org/pdf/2104.07658.pdf), [[Project]](https://charigyang.github.io/motiongroup/)

- (arXiv 2021.04) Decoupled Spatial-Temporal Transformer for Video **Inpainting**, [[Paper]](https://arxiv.org/pdf/2104.06637.pdf), [[Code]](https://github.com/ruiliu-ai/DSTT)

- (arXiv 2021.04) **Pose** Recognition with Cascade Transformers, [[Paper]](https://arxiv.org/pdf/2104.06976.pdf), [[Code]](https://github.com/mlpc-ucsd/PRTR)

- (arXiv 2021.04) Action-Conditioned **3D Human Motion Synthesis** with Transformer VAE, [[Paper]](https://arxiv.org/pdf/2104.05670.pdf), [[Project]](https://imagine.enpc.fr/~petrovim/actor)

- (arXiv 2021.04) Escaping the Big Data Paradigm with **Compact** Transformers, [[Paper]](https://arxiv.org/pdf/2104.05704.pdf), [[Code]](https://github.com/SHI-Labs/Compact-Transformers)

- (arXiv 2021.04) Know What and Know Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language **Navigation**, [[Paper]](https://arxiv.org/pdf/2104.04167.pdf)

- (arXiv 2021.04) **Handwriting** Transformers, [[Paper]](https://arxiv.org/pdf/2104.03964.pdf)

- (arXiv 2021.04) SiT: **Self-supervised** vIsion Transformer, [[Paper]](https://arxiv.org/pdf/2104.03602.pdf)

- (arXiv 2021.04) EFFICIENT TRANSFORMERS IN **REINFORCEMENT LEARNING** USING ACTOR-LEARNER DISTILLATION, [[Paper]](https://arxiv.org/pdf/2104.01655.pdf)

- (arXiv 2021.04) **Compressing** Visual-linguistic Model via Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2104.02096.pdf)

- (arXiv 2021.04) When Pigs Fly: Contextual **Reasoning** in Synthetic and Natural Scenes, [[Paper]](https://arxiv.org/pdf/2104.02215.pdf)

- (arXiv 2021.04) Variational Transformer Networks for **Layout Generation**, [[Paper]](https://arxiv.org/pdf/2104.02416.pdf)

- (arXiv 2021.04) Few-Shot Transformation of Common **Actions** into Time and Space, [[Paper]](https://arxiv.org/pdf/2104.02439.pdf)

- (arXiv 2021.04) **Fourier** Image Transformer, [[Paper]](https://arxiv.org/pdf/2104.02555.pdf)

- (arXiv 2021.04) **Efficient** DETR: Improving End-to-End Object Detector with Dense Prior, [[Paper]](https://arxiv.org/pdf/2104.01318.pdf)

- (arXiv 2021.04) A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person **Re-identification**, [[Paper]](https://arxiv.org/pdf/2104.01745.pdf)

- (arXiv 2021.04) An Empirical Study of **Training** Self-Supervised Visual Transformers, [[Paper]](https://arxiv.org/pdf/2104.02057.pdf)

- (arXiv 2021.04) Multitarget **Tracking** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00734.pdf)

- (arXiv 2021.04) TFill: **Image Completion** via a Transformer-Based Architecture, [[Paper]](https://arxiv.org/pdf/2104.00845.pdf), [[Code]](https://github.com/lyndonzheng/TFill)

- (arXiv 2021.04) AAformer: Auto-Aligned Transformer for Person **Re-Identification**, [[Paper]](https://arxiv.org/pdf/2104.00921.pdf)

- (arXiv 2021.04) VisQA: X-raying Vision and Language **Reasoning** in Transformers, [[Paper]](https://arxiv.org/pdf/2104.00926.pdf)

- (arXiv 2021.04) TubeR: Tube-Transformer for **Action Detection**, [[Paper]](https://arxiv.org/pdf/2104.00969.pdf)

- (arXiv 2021.04) Language-based **Video Editing** via Multi-Modal Multi-Level Transformer, [[Paper]](https://arxiv.org/pdf/2104.01122.pdf)

- (arXiv 2021.04) LeViT: a Vision Transformer in ConvNet’s Clothing for **Faster** Inference, [[Paper]](https://arxiv.org/pdf/2104.01136.pdf)

- (arXiv 2021.04) LoFTR: Detector-Free **Local Feature Matching** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00680.pdf), [[Code]](https://zju3dv.github.io/loftr/)

- (arXiv 2021.04) Putting **NeRF** on a Diet: Semantically Consistent Few-Shot View Synthesis, [[Paper]](https://arxiv.org/pdf/2104.00677.pdf), [[Project]](https://www.ajayj.com/dietnerf)

- (arXiv 2021.04) Group-Free 3D Object **Detection** via Transformers, [[Paper]](https://arxiv.org/pdf/2104.00678.pdf), [[Code]](https://github.com/zeliu98/Group-Free-3D)

- (arXiv 2021.04) Frozen in Time: A Joint Video and Image Encoder for End-to-End **Retrieval**, [[Paper]](https://arxiv.org/pdf/2104.00650.pdf)

- (arXiv 2021.04) Composable Augmentation Encoding for Video **Representation** Learning, [[Paper]](https://arxiv.org/pdf/2104.00616.pdf)

### 2021.03
- (arXiv 2021.03) TransCenter: Transformers with Dense Queries for Multiple-Object **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.15145.pdf)

- (arXiv 2021.03) PixelTransformer: Sample Conditioned **Signal Generation**, [[Paper]](https://arxiv.org/pdf/2103.15813.pdf), [[Code]](https://shubhtuls.github.io/PixelTransformer/)

- (arXiv 2021.03) Augmented Transformer with Adaptive Graph for **Temporal Action Proposal Generation**, [[Paper]](https://arxiv.org/pdf/2103.16024.pdf)

- (arXiv 2021.03) DA-DETR: Domain Adaptive **Detection** Transformer by Hybrid Attention, [[Paper]](https://arxiv.org/pdf/2103.17084.pdf)

- (arXiv 2021.03) Learning Spatio-Temporal Transformer for Visual **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.17154.pdf), [[Code]](https://github.com/researchmm/Stark)

- (arXiv 2021.03) StyleCLIP: Text-Driven Manipulation of **StyleGAN** Imagery, [[Paper]](https://arxiv.org/pdf/2103.17249.pdf), [[Code]](https://github.com/orpatashnik/StyleCLIP)

- (arXiv 2021.03) Multimodal **Motion Prediction** with Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)

- (arXiv 2021.03) Robust Facial **Expression** Recognition with Convolutional Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.16854.pdf)

- (arXiv 2021.03) **Describing and Localizing** Multiple Changes with Transformers, [[Paper]](https://arxiv.org/pdf/2103.14146.pdf), [[Project]](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers)

- (arXiv 2021.03) COTR: **Correspondence** Transformer for Matching Across Images, [[Paper]](https://arxiv.org/pdf/2103.14167.pdf)

- (arXiv 2021.03) nderstanding Robustness of Transformers for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2103.14586.pdf)

- (arXiv 2021.03) CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2103.14899.pdf)

- (arXiv 2021.03) Looking Beyond Two Frames: End-to-End Multi-Object **Tracking** Using Spatial and Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2103.14829.pdf)

- (arXiv 2021.03) HiT: Hierarchical Transformer with Momentum Contrast for Video-Text **Retrieval**, [[Paper]](https://arxiv.org/pdf/2103.15049.pdf)

- (arXiv 2021.03) TFPose: Direct Human **Pose** Estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2103.15320.pdf), [[Code]](https://git.io/AdelaiDet)

- (arXiv 2021.03) Multi-Scale Vision Longformer: A New Vision Transformer for **High-Resolution Image** Encoding, [[Paper]](https://arxiv.org/pdf/2103.15358.pdf)

- (arXiv 2021.03) Transformer **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.15436.pdf), [[Code]](https://github.com/chenxin-dlut/TransT)

- (arXiv 2021.03) ViViT: A **Video** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.15691.pdf)

- (arXiv 2021.03) CvT: Introducing **Convolutions** to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2103.15808.pdf), [[Code]](https://github.com/leoxiaobin/CvT)

- (arXiv 2021.03) Generic Attention-model **Explainability** for Interpreting Bi-Modal and Encoder-Decoder Transformers, [[Paper]](https://arxiv.org/pdf/2103.15679.pdf), [[Code]](https://github.com/hila-chefer/Transformer-MM-Explainability)

- (arXiv 2021.03) On the Adversarial **Robustness** of Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.15670.pdf)

- (arXiv 2021.03) Rethinking **Spatial Dimensions** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2103.16302.pdf), [[Code]](https://github.com/naver-ai/pit)

- (arXiv 2021.03) Spatiotemporal Transformer for Video-based **Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2103.16469.pdf)

- (arXiv 2021.03) Read and Attend: **Temporal Localisation** in Sign Language Videos, [[Paper]](https://arxiv.org/pdf/2103.16481.pdf), [[Benchmark]](https://github.com/visipedia/newt)

- (arXiv 2021.03) Thinking Fast and Slow: **Efficient** Text-to-Visual Retrieval with Transformers, [[Paper]](https://arxiv.org/pdf/2103.16553.pdf)

- (arXiv 2021.03) An Image is Worth 16x16 Words, What is a **Video** Worth? [[Paper]](https://arxiv.org/pdf/2103.13915.pdf)

- (arXiv 2021.03) High-Fidelity Pluralistic **Image Completion** with Transformers, [[Paper]](https://arxiv.org/pdf/2103.14031.pdf), [[Code]](http://raywzy.com/ICT)

- (arXiv 2021.03) Swin Transformer: **Hierarchical** Vision Transformer using Shifted Windows, [[Paper]](https://arxiv.org/pdf/2103.14030.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer)

- (arXiv 2021.03) Revamping Cross-Modal Recipe **Retrieval** with Hierarchical Transformers and Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2103.13061.pdf), [[Code]](https://github.com/amzn/image-to-recipe-transformers)

- (arXiv 2021.03) Multi-view 3D **Reconstruction** with Transformer, [[Paper]](https://arxiv.org/pdf/2103.12957.pdf)

- (arXiv 2021.03) Scene-Intuitive Agent for Remote Embodied Visual **Grounding**, [[Paper]](https://arxiv.org/pdf/2103.12944.pdf)

- (arXiv 2021.03) Can Vision Transformers Learn **without Natural Images**? [[Paper]](https://arxiv.org/pdf/2103.13023.pdf)

- (arXiv 2021.03) On the **Robustness** of Vision Transformers to Adversarial Examples, [[Paper]](https://arxiv.org/pdf/2104.02610.pdf)

- (arXiv 2021.03) Kaleido-BERT: **Vision-Language** Pre-training on Fashion Domain, [[Paper]](https://arxiv.org/pdf/2103.16110.pdf), [[Code]](http://dpfan.net/Kaleido-BERT)

- (arXiv 2021.03) End-to-End Trainable Multi-Instance **Pose Estimation** with Transformers, [[Paper]](https://arxiv.org/pdf/2103.12115.pdf)

- (arXiv 2021.03) Transformers Solve the Limited Receptive Field for **Monocular Depth Prediction**, [[Paper]](https://arxiv.org/pdf/2103.12091.pdf), [[Code]](https://github.com/ygjwd12345/TransDepth)

- (arXiv 2021.03) Meta-DETR: Few-Shot Object **Detection** via Unified Image-Level Meta-Learning, [[Paper]](https://arxiv.org/pdf/2103.11731.pdf)

- (arXiv 2021.03) Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.11681.pdf), [[Code]](https://github.com/594422814/TransformerTrack)

- (arXiv 2021.03) DeepViT: Towards **Deeper** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.11886.pdf), [[Code]](https://github.com/zhoudaquan/dvit_repo)

- (arXiv 2021.03) Incorporating **Convolution** Designs into Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.11816.pdf)

- (arXiv 2021.03) Multimodal **Motion Prediction** with Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)

- (arXiv 2021.03) MaAST: Map Attention with Semantic Transformers for Efficient Visual **Navigation**, [[Paper]](https://arxiv.org/pdf/2103.11374.pdf)

- (arXiv 2021.03) Paying Attention to Multiscale Feature Maps in Multimodal **Image Matching**, [[Paper]](https://arxiv.org/pdf/2103.11247.pdf)

- (arXiv 2021.03) HOPPER: MULTI-HOP TRANSFORMER FOR **SPATIOTEMPORAL REASONING**, [[Paper]](https://arxiv.org/pdf/2103.10574.pdf), [[Code]](https://github.com/necla-ml/cater-h)

- (arXiv 2021.03) Scalable Visual Transformers with **Hierarchical Pooling**, [[Paper]](https://arxiv.org/pdf/2103.10619.pdf)

- (arXiv 2021.03) AgentFormer: Agent-Aware Transformers for Socio-Temporal **Multi-Agent Forecasting**, [[Paper]](https://arxiv.org/pdf/2103.14023.pdf), [[Code]](https://github.com/Khrylx/AgentFormer)

- (arXiv 2021.03) Vision Transformers for **Dense Prediction**, [[Paper]](https://arxiv.org/pdf/2103.13413.pdf), [[Code]](https://github.com/intel-isl/DPT)

- (arXiv 2021.03) **3D Human Pose** Estimation with Spatial and Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2103.10455.pdf), [[Code]](https://github.com/zczcwh/PoseFormer)

- (arXiv 2021.03) ConViT: Improving Vision Transformers ith **Soft Convolutional Inductive Biases**, [[Paper]](https://arxiv.org/pdf/2103.10697.pdf), [[Code]](https://github.com/facebookresearch/convit)

- (arXiv 2021.03) MDMMT: Multidomain Multimodal Transformer for Video **Retrieval**, [[Paper]](https://arxiv.org/pdf/2103.10699.pdf)

- (arXiv 2021.03) On the **Sentence Embeddings** from Pre-trained Language Models, [[Paper]](https://arxiv.org/pdf/2011.05864.pdf)

- (arXiv 2021.03) Enhancing Transformer for **Video** Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training, [[Paper]](https://arxiv.org/pdf/2103.10043.pdf)

- (arXiv 2021.03) DanceNet3D: Music Based **Dance Generation** with Parametric Motion Transformer, [[Paper]](https://arxiv.org/pdf/2103.10206.pdf)

- (arXiv 2021.03) Decoupled Spatial Temporal Graphs for Generic **Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2103.10191.pdf)

- (arXiv 2021.03) Space-Time Crop & Attend: Improving Cross-modal **Video Representation** Learning, [[Paper]](https://arxiv.org/pdf/2103.10211.pdf)

- (arXiv 2021.03) Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2103.08849.pdf), [[Code]](http://github.com/berniebear/Mutli-HT100M)

- (arXiv 2021.03) TransFG: A Transformer Architecture for Fine-grained **Recognition**, [[Paper]](https://arxiv.org/pdf/2103.07976.pdf)

- (arXiv 2021.03) Causal Attention for **Vision-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2103.03493.pdf), [[Code]](https://github.com/yangxuntu/catt)

- (arXiv 2021.03) Continuous 3D Multi-Channel **Sign Language Production** via Progressive Transformers and Mixture Density Networks, [[Paper]](https://arxiv.org/pdf/2103.06982.pdf)

- (arXiv 2021.03) WenLan: Bridging Vision and Language by Large-Scale Multi-Modal **Pre-Training**, [[Paper]](https://arxiv.org/pdf/2103.06561.pdf)

- (arXiv 2021.03) **Attention** is not all you need: pure attention loses rank doubly exponentially with depth, [[Paper]](https://arxiv.org/pdf/2103.03404v1.pdf)

- (arXiv 2021.03) QPIC: Query-Based Pairwise **Human-Object Interaction** Detection with Image-Wide Contextual Information, [[Paper]](https://arxiv.org/pdf/2103.05399), [[Code]](https://github.com/hitachi-rd-cv/qpic)

- (arXiv 2021.03) Reformulating **HOI** Detection as Adaptive Set Prediction, [[Paper]](https://arxiv.org/pdf/2103.05983), [[Code]](https://github.com/yoyomimi/AS-Net)

- (arXiv 2021.03) End-to-End **Human Object Interaction** Detection with HOI Transformer, [[Paper]](https://arxiv.org/pdf/2103.04503), [[Code]](https://github.com/bbepoch/HoiTransformer)

- (arXiv 2021.03) Perceiver: General Perception with Iterative **Attention**, [[Paper]](https://arxiv.org/pdf/2103.03206.pdf)

- (arXiv 2021.03) Transformer **in** Transformer, [[Paper]](https://arxiv.org/pdf/2103.00112.pdf), [[Code]](https://github.com/huawei-noah/noah-research/tree/master/TNT)

- (arXiv 2021.03) **Generative Adversarial** Transformers, [[Paper]](https://arxiv.org/pdf/2103.01209.pdf), [[Code]](https://github.com/dorarad/gansformer)

- (arXiv 2021.03) OmniNet: Omnidirectional **Representations** from Transformers, [[Paper]](https://arxiv.org/pdf/2103.01075.pdf)

- (arXiv 2021.03) Single-Shot **Motion Completion** with Transformer, [[Paper]](https://arxiv.org/pdf/2103.00776.pdf), [[Code]](https://github.com/FuxiCV/SSMCT)

### 2021.02
- (arXiv 2021.02) Evolving Attention with **Residual** Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12895.pdf)

- (arXiv 2021.02) GEM: Glare or Gloom, I Can Still See You – End-to-End Multimodal Object **Detector**, [[Paper]](https://arxiv.org/pdf/2102.12319.pdf)

- (arXiv 2021.02) SparseBERT: Rethinking the Importance Analysis in **Self-attention**, [[Paper]](https://arxiv.org/pdf/2102.12871.pdf)

- (arXiv 2021.02) Investigating the **Limitations** of Transformers with Simple Arithmetic Tasks, [[Paper]](https://arxiv.org/pdf/2102.13019.pdf), [[Code]](https://github.com/castorini/transformers-arithmetic)

- (arXiv 2021.02) Do Transformer Modifications **Transfer** Across Implementations and Applications? [[Paper]](https://arxiv.org/pdf/2102.11972.pdf)

- (arXiv.2021.02) Do We Really Need Explicit **Position Encodings** for Vision Transformers? [[Paper]](https://arxiv.org/pdf/2102.10882.pdf), [[Code]](https://github.com/Meituan-AutoML/CPVT)

- (arXiv.2021.02) A Straightforward Framework For Video **Retrieval** Using CLIP, [[Paper]](https://arxiv.org/pdf/2102.12443.pdf), [[Code]](https://github.com/Deferf/CLIP_Video_Representation)

- (arXiv.2021.02) Pyramid Vision Transformer: A Versatile Backbone for **Dense Prediction** without Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12122.pdf), [[Code]](https://github.com/whai362/PVT)

- (arXiv.2021.02) VisualGPT: Data-efficient Image **Captioning** by Balancing Visual Input and Linguistic Knowledge from Pretraining, [[Paper]](https://arxiv.org/pdf/2102.10407.pdf), [[Code]](https://github.com/Vision-CAIR/VisualGPT)

- (arXiv.2021.02) Towards Accurate and **Compact** Architectures via Neural Architecture Transformer, [[Paper]](https://arxiv.org/pdf/2102.10301.pdf)

- (arXiv.2021.02) Centroid Transformer: Learning to **Abstract** with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) **Linear** Transformers Are Secretly Fast Weight Memory Systems, [[Paper]](https://arxiv.org/pdf/2102.11174.pdf)

- (arXiv.2021.02) **POSITION** INFORMATION IN TRANSFORMERS: AN OVERVIEW, [[Paper]](https://arxiv.org/pdf/2102.11090.pdf)

- (arXiv 2021.02) Transformer is All You Need: **Multimodal** Multitask Learning with a Unified Transformer, [[Paper]](https://arxiv.org/pdf/2102.10772.pdf), [[Project]](https://mmf.sh/), [[Code]](https://github.com/facebookresearch/mmf)

- (arXiv 2021.02) Centroid Transformer: Learning to **Abstract** with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) Conceptual 12M: Pushing Web-Scale **Image-Text** Pre-Training To Recognize Long-Tail Visual Concepts, [[Paper]](https://arxiv.org/pdf/2102.08981.pdf)

- (arXiv 2021.02) TransGAN: Two Transformers Can Make One Strong **GAN**, [[Paper]](https://arxiv.org/pdf/2102.07074.pdf), [[Code]](https://github.com/VITA-Group/TransGAN)

- (arXiv 2021.02) END-TO-END **AUDIO-VISUAL SPEECH RECOGNITION** WITH CONFORMERS, [[Paper]](https://arxiv.org/pdf/2102.06657.pdf)

- (arXiv 2021.02) Is Space-Time Attention All You Need for **Video** Understanding? [[Paper]](https://arxiv.org/pdf/2102.05095.pdf), [[Code]](https://github.com/lucidrains/TimeSformer-pytorch)

- (arXiv 2021.02) Less is More: CLIPBERT for **Video-and-Language** Learning via Sparse Sampling, [[Paper]](https://arxiv.org/pdf/2102.06183.pdf), [[Code]](https://github.com/jayleicn/ClipBERT)

- (arXiv 2021.02) **Video** Transformer Network, [[Paper]](https://arxiv.org/pdf/2102.00719.pdf)

- (arXiv 2021.02) Training Vision Transformers for Image **Retrieval**, [[Paper]](https://arxiv.org/pdf/2102.05644.pdf)

- (arXiv 2021.02) Relaxed Transformer Decoders for Direct **Action Proposal Generation**, [[Paper]](https://arxiv.org/pdf/2102.01894.pdf), [[Code]](https://github.com/MCG-NJU/RTD-Action)

- (arXiv 2021.02) TransReID: Transformer-based Object **Re-Identification**, [[Paper]](https://arxiv.org/pdf/2102.04378.pdf)

- (arXiv 2021.02) Improving Visual **Reasoning** by Exploiting The Knowledge in Texts, [[Paper]](https://arxiv.org/pdf/2102.04760.pdf)

### 2021.01
- (arXiv 2021.01) **Fast** Convergence of DETR with Spatially Modulated Co-Attention, [[Paper]](https://arxiv.org/pdf/2101.07448.pdf)

- (arXiv 2021.01) Dual-Level Collaborative Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2101.06462.pdf)

- (arXiv 2021.01) SSTVOS: Sparse Spatiotemporal Transformers for Video Object **Segmentation** (arXiv 2021.1), [[Paper]](https://arxiv.org/pdf/2101.08833.pdf)

- (arXiv 2021.01) CPTR: FULL TRANSFORMER NETWORK FOR IMAGE **CAPTIONING**, [[Paper]](https://arxiv.org/pdf/2101.10804.pdf)

- (arXiv 2021.01) Trans2Seg: Transparent Object **Segmentation** with Transformer, [[Paper]](https://arxiv.org/pdf/2101.08461), [[Code]](https://github.com/xieenze/Trans2Seg)

- (arXiv 2021.01) Scheduled Sampling in **Vision-Language** Pretraining with Decoupled Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2101.11562.pdf), [[Code]](https://github.com/YehLi/TDEN)

- (arXiv 2021.01) Trear: Transformer-based RGB-D Egocentric **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2101.03904.pdf)

- (arXiv 2021.01) Learn to Dance with AIST++: Music Conditioned **3D Dance Generation**, [[Paper]](https://arxiv.org/pdf/2101.08779), [[Page]](https://google.github.io/aichoreographer/;)

- (arXiv 2021.01) Spherical Transformer: Adapting **Spherical** Signal to CNNs, [[Paper]](https://arxiv.org/pdf/2101.03848.pdf)

- (arXiv 2021.01) Are We There Yet? Learning to **Localize** in Embodied Instruction Following, [[Paper]](https://arxiv.org/pdf/2101.03431.pdf)

- (arXiv 2021.01) VinVL: Making Visual Representations Matter in **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2101.00529.pdf)

- (arXiv 2021.01) Bottleneck Transformers for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2101.11605.pdf)

- (arXiv 2021.01) Investigating the Vision Transformer Model for Image **Retrieval** Tasks, [[Paper]](https://arxiv.org/pdf/2101.03771)

- (arXiv 2021.01) ADDRESSING SOME LIMITATIONS OF TRANSFORMERS WITH **FEEDBACK MEMORY**, [[Paper]](https://arxiv.org/pdf/2002.09402.pdf)

- (arXiv 2021.01) Tokens-to-Token ViT: **Training** Vision Transformers from Scratch on ImageNet, [[Paper]](https://arxiv.org/pdf/2101.11986.pdf), [[Code]](https://github.com/yitu-opensource/T2T-ViT)

- (arXiv 2021.01) TrackFormer: Multi-Object **Tracking** with Transformers, [[Paper]](https://arxiv.org/pdf/2101.02702)

- (arXiv 2021.01) VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale **Text-to-Image Search**, [[Paper]](https://arxiv.org/pdf/2101.00265)

- (arXiv 2021.01) **Line Segment Detection** Using Transformers without Edges, [[Paper]](https://arxiv.org/pdf/2101.01909)

- (arXiv 2021.01) Decoupling the Role of Data, Attention, and Losses in **Multimodal** Transformers, [[Paper]](https://arxiv.org/pdf/2102.00529.pdf)

### 2020.12
- (arXiv 2020.12) **Cloud** Transformers, [[Paper]](https://arxiv.org/pdf/2007.11679.pdf)

- (arXiv 2020.12) Accurate Word **Representations** with Universal Visual Guidance, [[Paper]](https://arxiv.org/pdf/2012.15086.pdf)

- (arXiv 2020.12) DETR for **Pedestrian Detection**, [[Paper]](https://arxiv.org/pdf/2012.06785)

- (arXiv 2020.12) Transformer **Interpretability** Beyond Attention Visualization, [[Paper]](https://arxiv.org/pdf/2012.09838), [[Code]](https://github.com/hila-chefer/Transformer-Explainability)

- (arXiv 2020.12) PCT: **Point Cloud** Transformer, [[Paper]](https://arxiv.org/pdf/2012.09688)

- (arXiv 2020.12) TransPose: Towards Explainable Human **Pose** Estimation by Transformer, [[Paper]](https://arxiv.org/pdf/2012.14214)

- (arXiv 2020.12) Rethinking Semantic **Segmentation** from a Sequence-to-Sequence Perspective with Transformers, [[Paper]](https://arxiv.org/pdf/2012.15840), [[Code]](https://github.com/fudan-zvg/SETR)

- (arXiv 2020.12) Transformer Guided Geometry Model for Flow-Based Unsupervised **Visual Odometry**, [[Paper]](https://arxiv.org/pdf/2101.02143)

- (arXiv 2020.12) Transformer for **Image Quality Assessment**, [[Paper]](https://arxiv.org/pdf/2101.01097), [[Code]](https://github.com/junyongyou/triq)

- (arXiv 2020.12) TransTrack: Multiple-Object **Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2012.15460), [[Code]](https://github.com/PeizeSun/TransTrack)

- (arXiv 2020.12) 3D Object **Detection** with Pointformer, [[Paper]](https://arxiv.org/pdf/2012.11409)

- (arXiv 2020.12) Training data-**efficient** image transformers & distillation through attention, [[Paper]](https://arxiv.org/pdf/2012.12877)

- (arXiv 2020.12) Toward Transformer-Based Object **Detection**, [[Paper]](https://arxiv.org/pdf/2012.09958)

- (arXiv 2020.12) SceneFormer: **Indoor Scene Generation** with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09793)

- (arXiv 2020.12) **Point** Transformer, [[Paper]](https://arxiv.org/pdf/2012.09164)

- (arXiv 2020.12) End-to-End **Human Pose and Mesh Reconstruction** with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09760)

- (arXiv 2020.12) Informer: Beyond Efficient Transformer for Long Sequence Time-Series **Forecasting**, [[Paper]](https://arxiv.org/pdf/2012.07436.pdf)

- (arXiv 2020.12) Pre-Trained **Image Processing** Transformer, [[Paper]](https://arxiv.org/pdf/2012.00364)

- (arXiv 2020.12) Taming Transformers for High-Resolution **Image Synthesis**, [[Paper]](https://arxiv.org/pdf/2012.09841.pdf), [[Code]](https://github.com/CompVis/taming-transformers)

### 2020.11
- (arXiv 2020.11) End-to-end **Lane Shape Prediction** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.04233), [[Code]](https://github.com/liuruijin17/LSTR)

- (arXiv 2020.11) UP-DETR: Unsupervised Pre-training for Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.09094)

- (arXiv 2020.11) End-to-End Video Instance **Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14503)

- (arXiv 2020.11) Rethinking Transformer-based Set Prediction for Object **Detection**, [[Paper]](https://arxiv.org/pdf/2011.10881)

- (arXiv 2020.11) General Multi-label Image **Classification** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14027}

- (arXiv 2020.11) End-to-End Object **Detection** with Adaptive Clustering Transformer, [[Paper]](https://arxiv.org/pdf/2011.09315)

### before 2020.11
- (arXiv 2020.10) An Image is Worth 16x16 Words: Transformers for **Image Recognition** at Scale, [[Paper]](https://arxiv.org/pdf/2010.11929), [[Code]](https://github.com/google-research/vision_transformer)

- (arXiv 2020.07) Oscar: Object-Semantics Aligned Pre-training for **Vision-and-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2004.06165.pdf), [[Code]](https://github.com/microsoft/Oscar)

- (arXiv 2020.07) Feature **Pyramid** Transformer, [[Paper]](https://arxiv.org/pdf/2007.09451), [[Code]](https://github.com/ZHANGDONG-NJUST/FPT)

- (arXiv 2020.06) Linformer: Self-Attention with **Linear Complexity**, [[Paper]](https://arxiv.org/pdf/2006.04768.pdf)

- (arXiv 2020.06) Visual Transformers: Token-based **Image Representation and Processing** for Computer Vision, [[Paper]](https://arxiv.org/pdf/2006.03677)

- (arXiv 2019.08) LXMERT: Learning **Cross-Modality** Encoder Representations from Transformers, [[Paper]](https://arxiv.org/pdf/1908.07490.pdf), [[Code]](https://github.com/airsplay/lxmert)

- (ICLR'21) IOT: INSTANCE-WISE LAYER REORDERING FOR TRANSFORMER **STRUCTURES**, [[Paper]](https://arxiv.org/pdf/2103.03457.pdf), [[Code]](https://github.com/instance-wise-ordered-transformer/IOT)

- (ICLR'21) UPDET: UNIVERSAL MULTI-AGENT **REINFORCEMENT LEARNING** VIA POLICY DECOUPLING WITH TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2101.08001.pdf), [[Code]](https://github.com/hhhusiyi-monash/UPDeT)

- (ICLR'21) Deformable DETR: Deformable Transformers for End-to-End **Object Detection**, [[Paper]](https://arxiv.org/pdf/2010.04159), [[Code]](https://github.com/fundamentalvision/Deformable-DETR)

- (ICLR'21) LAMBDANETWORKS: MODELING **LONG-RANGE INTERACTIONS** WITHOUT ATTENTION, [[Paper]](https://openreview.net/pdf?id=xTJEN-ggl1b), [[Code]](https://github.com/lucidrains/lambda-networks)

- (ICLR'21) SUPPORT-SET BOTTLENECKS FOR **VIDEO-TEXT REPRESENTATION** LEARNING, [[Paper]](https://arxiv.org/pdf/2010.02824.pdf)

- (ICLR'21) **COLORIZATION** TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2102.04432.pdf), [[Code]](https://github.com/google-research/google-research/tree/master/coltran)

- (ECCV'20) Multi-modal Transformer for **Video Retrieval**, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490205.pdf)

- (ECCV'20) Connecting **Vision and Language** with Localized Narratives, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500630.pdf)

- (ECCV'20) DETR: End-to-End **Object Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2005.12872), [[Code]](https://github.com/facebookresearch/detr)

- (CVPR'20) PaStaNet: Toward **Human Activity** Knowledge Engine, [[Paper]](https://arxiv.org/pdf/2004.00945.pdf), [[Code]](https://github.com/DirtyHarryLYL/HAKE-Action)

- (CVPR'20) Multi-Modality Cross Attention Network for **Image and Sentence Matching**, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Multi-Modality_Cross_Attention_Network_for_Image_and_Sentence_Matching_CVPR_2020_paper.pdf), [[Page]](https://www.robots.ox.ac.uk/~vgg/research/speech2action/)

- (CVPR'20) Learning Texture Transformer Network for **Image Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2006.04139), [[Code]](https://github.com/researchmm/TTSR)

- (CVPR'20) Speech2Action: Cross-modal Supervision for **Action Recognition**, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagrani_Speech2Action_Cross-Modal_Supervision_for_Action_Recognition_CVPR_2020_paper.pdf)

- (ICPR'20) Transformer Encoder **Reasoning** Network, [[Paper]](https://arxiv.org/pdf/2004.09144.pdf), [[Code]](https://github.com/mesnico/TERN)

- (EMNLP'19) Effective Use of Transformer Networks for **Entity Tracking**, [[Paper]](https://arxiv.org/pdf/1909.02635), [[Code]](https://github.com/aditya2211/transformer-entity-tracking)

## TODO
- [x] V-L representation learning (https://arxiv.org/pdf/2103.16110.pdf has provided a detailed table)
