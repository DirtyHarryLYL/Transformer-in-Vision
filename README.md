# Transformer-in-Vision
Recent Transformer-based CV and related works. Welcome to comment/contribute!

Keep updated.

## Resource

- DeepMind Gato: A Generalist Agent, [[Paper]](https://storage.googleapis.com/deepmind-media/A%20Generalist%20Agent/Generalist%20Agent.pdf)

- Google PaLM: Scaling Language Modeling with Pathways, [[Paper]](https://arxiv.org/pdf/2204.02311.pdf)

- OpenAI DALL·E 2 [[Page]](https://openai.com/dall-e-2/), [[Paper]](https://cdn.openai.com/papers/dall-e-2.pdf)

- SCENIC: A JAX Library for Computer Vision Research and Beyond, [[Code]](https://github.com/google-research/scenic)

- V-L joint learning study (with good tables): [[METER]](https://arxiv.org/pdf/2111.02387.pdf), [[Kaleido-BERT]](https://arxiv.org/pdf/2103.16110.pdf)

- Attention is all you need, [[Paper]](https://arxiv.org/pdf/1706.03762.pdf)

- OpenAI CLIP [[Page]](https://openai.com/blog/clip/), [[Paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf), [[Code]](https://github.com/openai/CLIP), [[arXiv]](https://arxiv.org/pdf/2103.00020.pdf)

- OpenAI DALL·E [[Page]](https://openai.com/blog/dall-e/), [[Code]](https://github.com/openai/DALL-E), [[Paper]](https://arxiv.org/pdf/2102.12092.pdf)

- [huggingface/transformers](https://github.com/huggingface/transformers)

- [Kyubyong/transformer](https://github.com/Kyubyong/transformer), TF

- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch), Torch

- [krasserm/fairseq-image-captioning](https://github.com/krasserm/fairseq-image-captioning)

- [PyTorch Transformers Tutorials](https://github.com/abhimishra91/transformers-tutorials)

- [ictnlp/awesome-transformer](https://github.com/ictnlp/awesome-transformer)

- [basicv8vc/awesome-transformer](https://github.com/basicv8vc/awesome-transformer)

- [dk-liang/Awesome-Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer)

- [yuewang-cuhk/awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)

## Survey

- (arXiv 2022.05) Transformers in 3D **Point Clouds**: A Survey, [[Paper]](https://arxiv.org/pdf/2205.07417.pdf)

- (arXiv 2022.04) **Visual Attention** Methods in Deep Learning: An In-Depth Survey, [[Paper]](https://arxiv.org/pdf/2204.07756.pdf)

- (arXiv 2022.04) **Vision-and-Language** Pretrained Models: A Survey, [[Paper]](https://arxiv.org/pdf/2204.07356.pdf)

- (arXiv 2022.03) A Roadmap for **Big Model**, [[Paper]](https://arxiv.org/pdf/2203.14101.pdf)

- (arXiv 2022.03) Transformers Meet **Visual** Learning Understanding: A Comprehensive Review, [[Paper]](https://arxiv.org/pdf/2203.12944.pdf）

- (arXiv 2022.03) Recent Advances in **Vision** Transformer: A Survey and Outlook of Recent Work, [[Paper]](https://arxiv.org/pdf/2203.01536.pdf), [[Project]](https://github.com/khawar512/ViT-Survey)

- (arXiv 2022.02) A Survey of **Vision-Language** Pre-Trained Models, [[Paper]](https://arxiv.org/pdf/2202.10936.pdf)

- (arXiv 2022.02) VLP: A Survey on **Vision-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2202.09061.pdf)

- (arXiv 2022.02) Transformer for **Graphs**: An Overview from Architecture Perspective, [[Paper]](https://arxiv.org/pdf/2202.08455.pdf)

- (arXiv 2022.01) **Video** Transformers: A Survey, [[Paper]](https://arxiv.org/pdf/2201.05991.pdf)

- (arXiv 2021.11) ARE WE READY FOR A NEW PARADIGM SHIFT? A SURVEY ON VISUAL DEEP **MLP**, [[Paper]](https://arxiv.org/pdf/2111.04060.pdf)

- (arXiv 2021.11) A Survey of **Visual** Transformers, [[Paper]](https://arxiv.org/pdf/2111.06091.pdf)

- (arXiv 2021.09) Survey: Transformer based **Video-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2109.09920.pdf)

- (arXiv 2021.06) A Survey of **Transformers**, [[Paper]](https://arxiv.org/pdf/2106.04554.pdf)

- (arXiv 2021.06) **Attention** mechanisms and deep learning for machine vision: A survey of the state of the art, [[Paper]](https://arxiv.org/pdf/2106.07550.pdf)

- (arXiv 2021.06) **Pre-Trained Models**: Past, Present and Future, [[Paper]](https://arxiv.org/pdf/2106.07139.pdf)

- (arXiv 2021.05) Can Attention Enable **MLPs** To Catch Up With CNNs? [[Paper]](https://arxiv.org/pdf/2105.15078.pdf)

- (arXiv 2021.03) A Practical Survey on **Faster** and **Lighter** Transformers, [[Paper]](https://arxiv.org/pdf/2103.14636.pdf)

- (arXiv 2021.03) Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with **Language and Vision**, [[Paper]](https://arxiv.org/pdf/2103.04037.pdf)

- (arXiv 2021.01) A Survey on **Visual** Transformer, [[Paper]](https://arxiv.org/pdf/2012.12556.pdf)

- (arXiv 2020.9) **Efficient** Transformers: A Survey, [[Paper]](https://arxiv.org/pdf/2009.06732.pdf)

- (arXiv 2020.1) **Transformers in Vision**: A Survey, [[Paper]](https://arxiv.org/pdf/2101.01169.pdf)

## Recent Papers
### 2022.05

<!-- - (arXiv 2022.05) , [[Paper]]()

- (arXiv 2022.05) , [[Paper]]()

- (arXiv 2022.05) , [[Paper]]()

- (arXiv 2022.05) , [[Paper]]()

- (arXiv 2022.05) , [[Paper]]()

- (arXiv 2022.05) , [[Paper]]()

- (arXiv 2022.05) , [[Paper]]()

- (arXiv 2022.05) , [[Paper]]() -->

- (arXiv 2022.05) **Masked Autoencoders** As Spatiotemporal Learners, [[Paper]](https://arxiv.org/pdf/2205.09113.pdf)

- (arXiv 2022.05) BodyMap: Learning Full-**Body** Dense **Correspondence** Map, [[Paper]](https://arxiv.org/pdf/2205.09111.pdf), [[Code]](https://nsarafianos.github.io/bodymap)

- (arXiv 2022.05) Unraveling **Attention** via Convex Duality: Analysis and Interpretations of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.08078.pdf)

- (arXiv 2022.05) Avatar**CLIP**: Zero-Shot Text-Driven Generation and Animation of 3D **Avatars**, [[Paper]](https://arxiv.org/pdf/2205.08535.pdf)

- (arXiv 2022.05) Vision Transformer Adapter for **Dense Predictions**, [[Paper]](https://arxiv.org/pdf/2205.08534.pdf), [[Code]](https://github.com/czczup/ViT-Adapter)

- (arXiv 2022.05) Demo: Real-Time **Semantic Communications** with a Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.03886.pdf)

- (arXiv 2022.05) MulT: An End-to-End **Multitask** Learning Transformer, [[Paper]](https://arxiv.org/pdf/2205.08303.pdf), [[Code]](https://ivrl.github.io/MulT/)

- (arXiv 2022.05) A **CLIP**-Hitchhiker’s Guide to Long **Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2205.08508.pdf)

- (arXiv 2022.05) Video **Frame Interpolation** with Transformer, [[Paper]](https://arxiv.org/pdf/2205.07230.pdf), [[Code]](https://github.com/dvlab-research/VFIformer)

- (arXiv 2022.05) Dense residual Transformer for Image **Denoising**, [[Paper]](https://arxiv.org/pdf/2205.06944.pdf)

- (arXiv 2022.05) Learning Lip-Based **Audio-Visual** Speaker Embeddings with AV-HuBERT, [[Paper]](https://arxiv.org/pdf/2205.07180.pdf)

- (arXiv 2022.05) **Robot Cooking** with Stir-fry: Bimanual Non-prehensile Manipulation of Semi-fluid Objects, [[Paper]](https://arxiv.org/pdf/2205.05960.pdf)

- (arXiv 2022.05) Entity-aware and Motion-aware Transformers for Language-driven **Action Localization** in Videos, [[Paper]](https://arxiv.org/pdf/2205.05854.pdf), [[Code]](https://github.com/shuoyang129/EAMAT)

- (arXiv 2022.05) Learning to **Retrieve Videos** by Asking Questions, [[Paper]](https://arxiv.org/pdf/2205.05739.pdf)

- (arXiv 2022.05) One Model, **Multiple Modalities**: A Sparsely Activated Approach for Text, Sound, Image, Video and Code, [[Paper]](https://arxiv.org/pdf/2205.06126.pdf)

- (arXiv 2022.05) Simple Open-Vocabulary Object **Detection** with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.06230.pdf), [[Code]](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)

- (arXiv 2022.05) AggPose: Deep Aggregation Vision Transformer for Infant **Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2205.05277.pdf), [[Code]](https://github.com/SZAR-LAB/AggPose)

- (arXiv 2022.05) An Empirical Study of Self-supervised Learning Approaches for Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2205.05543.pdf), [[Code-DETR]](https://github.com/gokulkarthik/detr), [[Code-Deform-DETR]](https://github.com/gokulkarthik/Deformable-DETR)

- (arXiv 2022.05) Reduce Information Loss in Transformers for Pluralistic Image **Inpainting**, [[Paper]](https://arxiv.org/pdf/2205.05076.pdf), [[Code]](https://github.com/liuqk3/PUT)

- (arXiv 2022.05) Transformer-based Cross-Modal **Recipe** Embeddings with Large Batch Training, [[Paper]](https://arxiv.org/pdf/2205.04948.pdf)

- (arXiv 2022.05) Spatio-Temporal Transformer for Dynamic **Facial Expression Recognition** in the Wild, [[Paper]](https://arxiv.org/pdf/2205.04749.pdf)

- (arXiv 2022.05) Generalizable **Task Planning** through Representation Pretraining, [[Paper]](https://arxiv.org/pdf/2205.07993.pdf), [[Project]](https://sites.google.com/view/gentp)

- (arXiv 2022.05) EdgeViTs: Competing **Light-weight** CNNs on **Mobile Devices** with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.03436.pdf)

- (arXiv 2022.05) Activating More Pixels in Image **Super-Resolution** Transformer, [[Paper]](https://arxiv.org/pdf/2205.04437.pdf), [[Code]](https://github.com/chxy95/HAT)

- (arXiv 2022.05) Row-wise **Accelerator** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.03998.pdf)

- (arXiv 2022.05) SparseTT: Visual **Tracking** with Sparse Transformers, [[Paper]](https://arxiv.org/pdf/2205.03776.pdf), [[Code]](https://github.com/fzh0917/SparseTT)

- (arXiv 2022.05) RoViST: Learning Robust Metrics for **Visual Storytelling**, [[Paper]](https://arxiv.org/pdf/2205.03774.pdf), [[Code]](https://github.com/usydnlp/rovist)

- (arXiv 2022.05) Beyond Bounding Box: Multimodal Knowledge Learning for Object **Detection**, [[Paper]](https://arxiv.org/pdf/2205.04072.pdf)

- (arXiv 2022.05) Multilevel Hierarchical Network with Multiscale Sampling for **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2205.04061.pdf)

- (arXiv 2022.05) Incremental-DETR: Incremental Few-Shot Object **Detection** via Self-Supervised Learning, [[Paper]](https://arxiv.org/pdf/2205.04042.pdf)

- (arXiv 2022.05) Conv**MAE**: Masked **Convolution** Meets Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2205.03892.pdf), [[Code]](https://github.com/Alpha-VL/ConvMAE)

- (arXiv 2022.05) Cross-lingual Adaptation for **Recipe Retrieval** with Mixup, [[Paper]](https://arxiv.org/pdf/2205.03891.pdf)

- (arXiv 2022.05) Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A **Vision-Language** Framework, [[Paper]](https://arxiv.org/pdf/2205.03860.pdf)

- (arXiv 2022.05) Transformer **Tracking** with Cyclic Shifting Window Attention, [[Paper]](https://arxiv.org/pdf/2205.03806.pdf), [[Code]](https://github.com/SkyeSong38/CSWinTT)

- (arXiv 2022.05) Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2205.04363.pdf)

- (arXiv 2022.05) Prompt Distribution Learning, [[Paper]](https://arxiv.org/pdf/2205.03340.pdf)

- (arXiv 2022.05) CLIP-CLOP: **CLIP**-Guided **Collage** and **Photomontage**, [[Paper]](https://arxiv.org/pdf/2205.03146.pdf)

- (arXiv 2022.05) Dual-Level Decoupled Transformer for **Video Captioning**, [[Paper]](https://arxiv.org/pdf/2205.03039.pdf)

- (arXiv 2022.05) Declaration-based Prompt Tuning for **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2205.02456.pdf), [[Code]](https://github.com/CCIIPLab/DPT)

- (arXiv 2022.05) P^3IV: Probabilistic **Procedure Planning** from **Instructional Videos** with Weak Supervision, [[Paper]](https://arxiv.org/pdf/2205.02300.pdf)

- (arXiv 2022.05) Language Models Can See: Plugging **Visual** Controls in **Text Generation**, [[Paper]](https://arxiv.org/pdf/2205.02655.pdf), [[Code]](https://github.com/yxuansu/MAGIC)

- (arXiv 2022.05) YOLOPose: Transformer-based Multi-Object **6D Pose Estimation** using Keypoint Regression, [[Paper]](https://arxiv.org/pdf/2205.02536.pdf)

- (arXiv 2022.05) Cross-view Transformers for real-time Map-view **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2205.02833.pdf), [[Code]](https://github.com/bradyz/cross_view_transformers)

- (arXiv 2022.05) i-Code: An Integrative and Composable **Multimodal** Learning Framework, [[Paper]](https://arxiv.org/pdf/2205.01818.pdf)

- (arXiv 2022.05) **Visual Commonsense** in Pretrained Unimodal and Multimodal Models, [[Paper]](https://arxiv.org/pdf/2205.01850.pdf), [[Project]](https://github.com/ChenyuHeidiZhang/VL-commonsense)

- (arXiv 2022.05) Dual Cross-Attention Learning for Fine-Grained Visual **Categorization** and Object **Re-Identification**, [[Paper]](https://arxiv.org/pdf/2205.02151.pdf)

- (arXiv 2022.05) RecipeSnap - a lightweight **image to recipe** model, [[Paper]](https://arxiv.org/pdf/2205.02141.pdf), [[Code]](https://github.com/jianfa/RecipeSnap-a-lightweight-image-to-recipe-model.git)

- (arXiv 2022.05) CoCa: Contrastive Captioners are **Image-Text** Foundation Models, [[Paper]](https://arxiv.org/pdf/2205.01917.pdf)

- (arXiv 2022.05) Data Determines Distributional **Robustness** in Contrastive Language Image Pre-training (**CLIP**), [[Paper]](https://arxiv.org/pdf/2205.01397.pdf)

- (arXiv 2022.05) Cross-modal Representation Learning for **Zero-shot Action Recognition**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2205/2205.01657.pdf), [[Code]](https://github.com/microsoft/ResT)

- (arXiv 2022.05) Cross-Domain Object **Detection** with Mean-Teacher Transformer, [[Paper]](https://arxiv.org/pdf/2205.01643.pdf)

- (arXiv 2022.05) Better plain ViT baselines for **ImageNet-1k**, [[Paper]](https://arxiv.org/pdf/2205.01580.pdf), [[Code]](https://github.com/google-research/big_vision)

- (arXiv 2022.05) Reinforced Swin-Convs Transformer for **Underwater Image Enhancement**, [[Paper]](https://arxiv.org/pdf/2205.00434.pdf)

- (arXiv 2022.05) UTC: A Unified Transformer with Inter-Task Contrastive Learning for **Visual Dialog**, [[Paper]](https://arxiv.org/pdf/2205.00423.pdf)

- (arXiv 2022.05) Answer-Me: Multi-Task Open-Vocabulary **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2205.00949.pdf)

- (arXiv 2022.05) Center**CLIP**: Token Clustering for Efficient **Text-Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2205.00823.pdf), [[Code]](https://github.com/mzhaoshuai/CenterCLIP)

- (arXiv 2022.05) Arbitrary Shape **Text Detection** via Boundary Transformer, [[Paper]](https://arxiv.org/pdf/2205.05320.pdf), [[Code]](https://github.com/GXYM/TextBPN-Puls-Plus)

- (arXiv 2022.05) HULC: **3D Human Motion Capture** with Pose Manifold Sampling and Dense Contact Guidance, [[Paper]](https://arxiv.org/pdf/2205.05677.pdf), [[Project]](https://vcai.mpi-inf.mpg.de/projects/HULC)

### 2022.04

- (arXiv 2022.04) Learn to Understand Negation in **Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2205.00132.pdf)

- (arXiv 2022.04) LayoutBERT: Masked Language **Layout** Model for Object Insertion, [[Paper]](https://arxiv.org/pdf/2205.00347.pdf)

- (arXiv 2022.04) Improving **Visual Grounding** with Visual-Linguistic Verification and Iterative Reasoning, [[Paper]](https://arxiv.org/pdf/2205.00272.pdf), [[Code]](https://github.com/yangli18/VLTVG)

- (arXiv 2022.04) Coarse-to-Fine **Video Denoising** with Dual-Stage Spatial-Channel Transformer, [[Paper]](https://arxiv.org/pdf/2205.00214.pdf)

- (arXiv 2022.04) SideRT: A Real-time Pure Transformer Architecture for Single Image **Depth Estimation**, [[Paper]](https://arxiv.org/pdf/2204.13892.pdf)

- (arXiv 2022.04) Where in the World is this Image? Transformer-based **Geo-localization** in the Wild, [[Paper]](https://arxiv.org/pdf/2204.13861.pdf)

- (arXiv 2022.04) **Depth Estimation** with Simplified Transformer, [[Paper]](https://arxiv.org/pdf/2204.13791.pdf)

- (arXiv 2022.04) A very preliminary **analysis** of **DALL-E 2**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2204/2204.13807.pdf)

- (arXiv 2022.04) CogView2: Faster and Better **Text-to-Image Generation** via Hierarchical Transformers, [[Paper]](https://arxiv.org/pdf/2204.14217.pdf), [[Code]](https://github.com/THUDM/CogView2)

- (arXiv 2022.04) **CLIP**-Art: Contrastive Pre-training for **Fine-Grained Art Classification**, [[Paper]](https://arxiv.org/pdf/2204.14244.pdf), [[Code]](https://github.com/KeremTurgutlu/clip_art)

- (arXiv 2022.04) TEMOS: **Generating** diverse human **motions** from textual descriptions, [[Paper]](https://arxiv.org/pdf/2204.14109.pdf), [[Project]](https://imagine.enpc.fr/~petrovim/temos)

- (arXiv 2022.04) PyramidCLIP: Hierarchical Feature Alignment for **Vision-language** Model Pretraining, [[Paper]](https://arxiv.org/pdf/2204.14095.pdf)

- (arXiv 2022.04) Symmetric Transformer-based Network for **Unsupervised Image Registration**, [[Paper]](https://arxiv.org/pdf/2204.13575.pdf), [[Code]](https://github.com/MingR-Ma/SymTrans)

- (arXiv 2022.04) Tragedy Plus Time: Capturing **Unintended Human Activities** from Weakly-labeled Videos, [[Paper]](https://arxiv.org/pdf/2204.13548.pdf), [[Code]](https://asu-apg.github.io/TragedyPlusTime)

- (arXiv 2022.04) CapOnImage: Context-driven Dense-**Captioning** on Image, [[Paper]](https://arxiv.org/pdf/2204.12974.pdf)

- (arXiv 2022.04) Self-Supervised Learning of Object Parts for Semantic **Segmentation**, [[Paper]](https://arxiv.org/pdf/2204.13101.pdf), [[Code]](https://github.com/MkuuWaUjinga/leopart)

- (arXiv 2022.04) DearKD: Data-**Efficient** Early **Knowledge Distillation** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.12997.pdf)

- (arXiv 2022.04) CATrans: Context and Affinity Transformer for Few-Shot **Segmentation**, [[Paper]](https://arxiv.org/pdf/2204.12817.pdf)

- (arXiv 2022.04) Self-Driving Car **Steering Angle Prediction**: Let Transformer Be a Car Again, [[Paper]](https://arxiv.org/pdf/2204.12748.pdf), [[Code]](https://github.com/chingisooinar/AI)

- (arXiv 2022.04) ClothFormer: Taming Video **Virtual Try-on** in All Module, [[Paper]](https://arxiv.org/pdf/2204.12151.pdf)

- (arXiv 2022.04) Deeper Insights into ViTs **Robustness** towards Common Corruptions, [[Paper]](https://arxiv.org/pdf/2204.12143.pdf)

- (arXiv 2022.04) VITPOSE: SIMPLE VISION TRANSFORMER BASELINES FOR HUMAN **POSE ESTIMATION**, [[Paper]](https://arxiv.org/pdf/2204.12484.pdf), [[Code]](https://github.com/ViTAE-Transformer/ViTPose)

- (arXiv 2022.04) Understanding The **Robustness** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.12451.pdf), [[Code]](https://github.com/NVlabs/FAN)

- (arXiv 2022.04) MILES: Visual BERT Pre-training with Injected Language Semantics for **Video-text Retrieval**, [[Paper]](https://arxiv.org/pdf/2204.12408.pdf)

- (arXiv 2022.04) Contrastive Language-Action Pre-training for **Temporal Localization**, [[Paper]](https://arxiv.org/pdf/2204.12293.pdf)

- (arXiv 2022.04) Boosting **Adversarial Transferability** of **MLP**-Mixer, [[Paper]](https://arxiv.org/pdf/2204.12204.pdf)

- (arXiv 2022.04) Adaptive **Split-Fusion** Transformer, [[Paper]](https://arxiv.org/pdf/2204.12196.pdf), [[Code]](https://github.com/szx503045266/ASF-former)

- (arXiv 2022.04) Can Foundation Models Perform Zero-Shot Task Specification For **Robot Manipulation**? [[Paper]](https://arxiv.org/pdf/2204.11134.pdf), [[Project]](https://sites.google.com/view/zestproject)

- (arXiv 2022.04) RELVIT: CONCEPT-GUIDED VISION TRANSFORMER FOR **VISUAL RELATIONAL REASONING**, [[Paper]](https://arxiv.org/pdf/2204.11167.pdf)

- (arXiv 2022.04) VISTA: Vision Transformer enhanced by U-Net and Image Colorfulness Frame Filtration for Automatic **Retail Checkout**, [[Paper]](https://arxiv.org/pdf/2204.11024.pdf), [[Code]](https://github.com/istiakshihab/automated-retail-checkout-aicity22)

- (arXiv 2022.04) **CLIP**-DISSECT: AUTOMATIC **DESCRIPTION** OF **NEURON** REPRESENTATIONS IN DEEP VISION NETWORKS, [[Paper]](https://arxiv.org/pdf/2204.10965.pdf)

- (arXiv 2022.04) TEMOS: **Generating** diverse human **motions** from textual descriptions, [[Paper]](https://arxiv.org/pdf/2204.14109.pdf), [[Project]](https://imagine.enpc.fr/~petrovim/temos)

- (arXiv 2022.04) Unsupervised Hierarchical **Semantic Segmentation** with Multiview Cosegmentation and Clustering Transformers, [[Paper]](https://arxiv.org/pdf/2204.11432.pdf)

- (arXiv 2022.04) SwinFuse: A Residual Swin Transformer Fusion Network for **Infrared and Visible Images**, [[Paper]](https://arxiv.org/pdf/2204.11436.pdf), [[Code]](https://github.com/Zhishe-Wang/SwinFuse)

- (arXiv 2022.04) OCFormer: One-Class Transformer Network for **Image Classification**, [[Paper]](https://arxiv.org/pdf/2204.11449.pdf)

- (arXiv 2022.04) DRT: A Lightweight Single Image **Deraining** Recursive Transformer, [[Paper]](https://arxiv.org/pdf/2204.11385.pdf), [[Code]](https://github.com/YC-Liang/DRT)

- (arXiv 2022.04) Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for **Knowledge-based Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2204.10448.pdf), [[Code]](https://github.com/yujungheo/kbvqa-public)

- (arXiv 2022.04) ParkPredict+: **Multimodal Intent** and **Motion Prediction** for **Vehicles** in Parking Lots with CNN and Transformer, [[Paper]](https://arxiv.org/pdf/2204.10777.pdf)

- (arXiv 2022.04) iCAR: Bridging Image Classification and **Image-text** Alignment for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2204.10760.pdf), [[Code]](https://github.com/weiyx16/iCAR)

- (arXiv 2022.04) DIVERSE INSTANCE DISCOVERY: VISION-TRANSFORMER FOR INSTANCE-AWARE **MULTI-LABEL IMAGE RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2204.10731.pdf)

- (arXiv 2022.04) Spatiality-guided Transformer for 3D Dense **Captioning** on **Point Clouds**, [[Paper]](https://arxiv.org/pdf/2204.10688.pdf), [[Code]](https://spacap3d.github.io/)

- (arXiv 2022.04) DFAM-DETR: Deformable feature based attention mechanism DETR on slender object **detection**, [[Paper]](https://arxiv.org/pdf/2204.10667.pdf)

- (arXiv 2022.04) NFormer: Robust **Person Re-identification** with Neighbor Transformer, [[Paper]](https://arxiv.org/pdf/2204.09331.pdf), [[Code]](https://github.com/haochenheheda/NFormer)

- (arXiv 2022.04) **Video Moment Retrieval** from Text Queries via Single Frame Annotation, [[Paper]](https://arxiv.org/pdf/2204.09409.pdf)

- (arXiv 2022.04) GIMO: Gaze-Informed **Human Motion Prediction** in Context, [[Paper]](https://arxiv.org/pdf/2204.09443.pdf)

- (arXiv 2022.04) VQGAN-CLIP: Open Domain **Image Generation and Editing** with Natural Language Guidance, [[Paper]](https://arxiv.org/pdf/2204.08583.pdf)

- (arXiv 2022.04) Sim-2-Sim Transfer for **Vision-and-Language Navigation** in Continuous Environments, [[Paper]](https://arxiv.org/pdf/2204.09667.pdf)

- (arXiv 2022.04) Not All Tokens Are Equal: **Human-centric** Visual **Analysis** via Token Clustering Transformer, [[Paper]](https://arxiv.org/pdf/2204.08680.pdf), [[Code]](https://github.com/zengwang430521/TCFormer.git)

- (arXiv 2022.04) **Multimodal** Token Fusion for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.08721.pdf)

- (arXiv 2022.04) Self-Calibrated Efficient Transformer for Lightweight **Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2204.08913.pdf), [[Code]](https://github.com/AlexZou14/SCET)

- (arXiv 2022.04) Searching Intrinsic **Dimensions** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.07722.pdf)

- (arXiv 2022.04) Towards **Lightweight** Transformer via Group-wise Transformation for **Vision-and-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2204.07780.pdf)

- (arXiv 2022.04) Multimodal Few-Shot Object **Detection** with Meta-Learning Based Cross-Modal Prompting, [[Paper]](https://arxiv.org/pdf/2204.07841.pdf)

- (arXiv 2022.04) Multi-Frame Self-Supervised **Depth** with Transformers, [[Paper]](https://arxiv.org/pdf/2204.07616.pdf), [[Code]](https://sites.google.com/tri.global/depthformer)

- (arXiv 2022.04) MST++: Multi-stage Spectral-wise Transformer for Efficient **Spectral Reconstruction**, [[Paper]](https://arxiv.org/pdf/2204.07908.pdf), [[Code]](https://github.com/caiyuanhao1998/MST-plus-plus)

- (arXiv 2022.04) Vision-Language Pre-Training for Multimodal Aspect-Based **Sentiment Analysis**, [[Paper]](https://arxiv.org/pdf/2204.07955.pdf), [[Code]](https://github.com/NUSTM/VLP-MABSA)

- (arXiv 2022.04) An Extendable, Efficient and Effective Transformer-based Object **Detector**, [[Paper]](https://arxiv.org/pdf/2204.07962.pdf), [[Code]](https://github.com/naver-ai/vidt)

- (arXiv 2022.04) VDTR: **Video Deblurring** with Transformer, [[Paper]](https://arxiv.org/pdf/2204.08023.pdf), [[Code]](https://github.com/ljzycmd/VDTR)

- (arXiv 2022.04) BSRT: Improving Burst **Super-Resolution** with Swin Transformer and Flow-Guided Deformable Alignment, [[Paper]](https://arxiv.org/pdf/2204.08332.pdf), [[Code]](https://github.com/Algolzw/BSRT)

- (arXiv 2022.04) Temporally Efficient Vision Transformer for **Video Instance Segmentation**, [[Paper]](https://arxiv.org/pdf/2204.08412.pdf), [[Code]](https://github.com/hustvl/TeViT)

- (arXiv 2022.04) VSA: Learning Varied-Size Window **Attention** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.08446.pdf), [[Code]](https://github.com/ViTAE-Transformer/ViTAE-VSA)

- (arXiv 2022.04) XDBERT: Distilling Visual Information to **BERT** from Cross-Modal Systems to Improve Language Understanding, [[Paper]](https://arxiv.org/pdf/2204.07316.pdf)

- (arXiv 2022.04) IMPROVING CROSS-MODAL UNDERSTANDING IN **VISUAL DIALOG** VIA CONTRASTIVE LEARNING, [[Paper]](https://arxiv.org/pdf/2204.07302.pdf)

- (arXiv 2022.04) MVSTER: Epipolar Transformer for Efficient **Multi-View Stereo**, [[Paper]](https://arxiv.org/pdf/2204.07346.pdf), [[Code]](https://github.com/JeffWang987/MVSTER)

- (arXiv 2022.04) UNCONDITIONAL **IMAGE-TEXT PAIR GENERATION** WITH MULTIMODAL CROSS QUANTIZER, [[Paper]](https://arxiv.org/pdf/2204.07537.pdf)

- (arXiv 2022.04) Pushing the Limits of Simple Pipelines for **Few-Shot Learning**: External Data and Fine-Tuning Make a Difference, [[Paper]](https://arxiv.org/pdf/2204.07305.pdf)

- (arXiv 2022.04) COTS: Collaborative Two-Stream **Vision-Language** Pre-Training Model for Cross-Modal Retrieval, [[Paper]](https://arxiv.org/pdf/2204.07441.pdf)

- (arXiv 2022.04) Image **Captioning** In the Transformer Age, [[Paper]](https://arxiv.org/pdf/2204.07374.pdf), [[Code]](https://github.com/SjokerLily/awesome-image-captioning)

- (arXiv 2022.04) **ResT** V2: Simpler, Faster and Stronger, [[Paper]](https://arxiv.org/pdf/2204.07366.pdf), [[Code]](https://github.com/wofmanaf/ResT)

- (arXiv 2022.04) Lightweight Bimodal Network for Single-Image **Super-Resolution** via Symmetric CNN and Recursive Transformer, [[Paper]](https://arxiv.org/pdf/2204.13286.pdf), [[Code]](https://github.com/IVIPLab/LBNet)

- (arXiv 2022.04) Temporal Progressive Attention for **Early Action Prediction**, [[Paper]](https://arxiv.org/pdf/2204.13340.pdf), [[Code]](https://github.com/alexandrosstergiou/progressive-action-prediction)

- (arXiv 2022.04) Keep the Caption Information: Preventing Shortcut Learning in Contrastive **Image-Caption** Retrieval, [[Paper]](https://arxiv.org/pdf/2204.13382.pdf)

- (arXiv 2022.04) Flamingo: a **Visual Language** Model for **Few-Shot** Learning, [[Paper]](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf)

- (arXiv 2022.04) RELVIT: CONCEPT-GUIDED VISION TRANSFORMER FOR **VISUAL RELATIONAL REASONING**, [[Paper]](https://arxiv.org/pdf/2204.11167.pdf)

- (arXiv 2022.04) **Unsupervised** Human **Action** Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance, [[Paper]](https://arxiv.org/pdf/2204.10312.pdf), [[Code]](https://github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian)

- (arXiv 2022.04) Learning **Future Object Prediction** with a Spatiotemporal Detection Transformer, [[Paper]](https://arxiv.org/pdf/2204.10321.pdf)

- (arXiv 2022.04) R^2-Trans: **Fine-Grained Visual Categorization** with Redundancy Reduction, [[Paper]](https://arxiv.org/pdf/2204.10095.pdf), [[Code]](https://anonymous.4open.science/r/R-2-Trans)

- (arXiv 2022.04) A New Dataset and Transformer for **Stereoscopic Video Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2204.10039.pdf), [[Code]](https://github.com/H-deep/Trans-SVSR/)

- (arXiv 2022.04) Transformer-Guided Convolutional Neural Network for Cross-View **Geolocalization**, [[Paper]](https://arxiv.org/pdf/2204.09967.pdf)

- (arXiv 2022.04) Multi-Scale Features and Parallel Transformers Based **Image Quality Assessment**, [[Paper]](https://arxiv.org/pdf/2204.09779.pdf), [[Code]](https://github.com/KomalPal9610/IQA)

- (arXiv 2022.04) BTranspose: Bottleneck Transformers for **Human Pose Estimation** with Self-Supervised Pre-Training, [[Paper]](https://arxiv.org/pdf/2204.10209.pdf)

- (arXiv 2022.04) **Human-Object Interaction Detection** via Disentangled Transformer, [[Paper]](https://arxiv.org/pdf/2204.09290.pdf)

- (arXiv 2022.04) ELEVATER: A **Benchmark** and **Toolkit** for Evaluating **Language-Augmented Visual Models**, [[Paper]](https://arxiv.org/pdf/2204.08790.pdf)

- (arXiv 2022.04) Interactiveness Field in **Human-Object Interactions**, [[Paper]](https://arxiv.org/pdf/2204.07718.pdf), [[Code]](https://github.com/Foruck/Interactiveness-Field)

- (arXiv 2022.04) DeiT III: Revenge of the **ViT**, [[Paper]](https://arxiv.org/pdf/2204.07118.pdf)

- (arXiv 2022.04) Residual Swin Transformer Channel Attention Network for **Image Demosaicing**, [[Paper]](https://arxiv.org/pdf/2204.07098.pdf)

- (arXiv 2022.04) Neighborhood **Attention** Transformer, [[Paper]](https://arxiv.org/pdf/2204.07143.pdf), [[Code]](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)

- (arXiv 2022.04) MiniViT: **Compressing** Vision Transformers with Weight Multiplexing, [[Paper]](https://arxiv.org/pdf/2204.07154.pdf), [[Code]](https://github.com/microsoft/Cream)

- (arXiv 2022.04) ViTOL: Vision Transformer for **Weakly Supervised Object Localization**, [[Paper]](https://arxiv.org/pdf/2204.06772.pdf), [[Code]](https://github.com/Saurav-31/ViTOL)

- (arXiv 2022.04) What Matters in Language Conditioned Robotic **Imitation Learning**, [[Paper]](https://arxiv.org/pdf/2204.06252.pdf), [[Code]](http://hulc.cs.uni-freiburg.de/)

- (arXiv 2022.04) Consistency driven Sequential Transformers Attention Model for **Partially Observable Scenes**, [[Paper]](https://arxiv.org/pdf/2204.00656)

- (arXiv 2022.04) ReCLIP: A Strong Zero-Shot Baseline for **Referring Expression Comprehension**, [[Paper]](https://arxiv.org/pdf/2204.05991.pdf)

- (arXiv 2022.04) Are **Multimodal** Transformers **Robust** to Missing Modality? [[Paper]](https://arxiv.org/pdf/2204.05454.pdf)

- (arXiv 2022.04) TopFormer: Token Pyramid Transformer for Mobile **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2204.05525.pdf), [[Code]](https://github.com/hustvl/TopFormer)

- (arXiv 2022.04) X-DETR: A Versatile Architecture for Instance-wise **Vision-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2204.05626.pdf)

- (arXiv 2022.04) **Event** Transformer, [[Paper]](https://arxiv.org/pdf/2204.05172.pdf)

- (arXiv 2022.04) Evaluating Vision Transformer Methods for **Deep Reinforcement Learning** from Pixels, [[Paper]](https://arxiv.org/pdf/2204.04905.pdf)

- (arXiv 2022.04) ManiTrans: Entity-Level **Text-Guided Image Manipulation** via Token-wise Semantic Alignment and Generation, [[Paper]](https://arxiv.org/pdf/2204.04428.pdf), [[Code]](https://jawang19.github.io/manitrans)

- (arXiv 2022.04) Multimodal Transformer for Nursing **Activity Recognition**, [[Paper]](https://arxiv.org/pdf/2204.04564.pdf), [[Code]](https://github.com/Momilijaz96/MMT_for_NCRC)

- (arXiv 2022.04) **Robust** Cross-Modal Representation Learning with Progressive Self-Distillation, [[Paper]](https://arxiv.org/pdf/2204.04588.pdf)

- (arXiv 2022.04) Stripformer: Strip Transformer for Fast Image **Deblurring**, [[Paper]](https://arxiv.org/pdf/2204.04627.pdf)

- (arXiv 2022.04) No Token Left Behind: **Explainability**-Aided Image Classification and Generation, [[Paper]](https://arxiv.org/pdf/2204.04908.pdf)

- (arXiv 2022.04) Fashionformer: A Simple, Effective and Unified Baseline for **Human Fashion Segmentation and Recognition**, [[Paper]](https://arxiv.org/pdf/2204.04654.pdf), [[Code]](https://github.com/xushilin1/FashionFormer)

- (arXiv 2022.04) Panoptic-PartFormer: Learning a Unified Model for **Panoptic Part Segmentation**, [[Paper]](https://arxiv.org/pdf/2204.04655.pdf), [[Code]](https://github.com/lxtGH/Panoptic-PartFormer)

- (arXiv 2022.04) DILEMMA: Self-Supervised **Shape and Texture** Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2204.04788.pdf)

- (arXiv 2022.04) Learning Trajectory-Aware Transformer for **Video Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2204.04216.pdf), [[Code]](https://github.com/researchmm/TTVSR.git)

- (arXiv 2022.04) Learning to Induce **Causal** Structure, [[Paper]](https://arxiv.org/pdf/2204.04875.pdf)

- (arXiv 2022.04) Consistency Learning via Decoding Path Augmentation for Transformers in **Human Object Interaction Detection**, [[Paper]](https://arxiv.org/pdf/2204.04836.pdf), [[Code]](https://github.com/mlvlab/CPChoi)

- (arXiv 2022.04) Category-Aware Transformer Network for Better **Human-Object Interaction Detection**, [[Paper]](https://arxiv.org/pdf/2204.04911.pdf)

- (arXiv 2022.04) Does **Robustness** on ImageNet Transfer to Downstream Tasks?, [[Paper]](https://arxiv.org/pdf/2204.03934.pdf)

- (arXiv 2022.04) POSTER: A Pyramid Cross-Fusion Transformer Network for **Facial Expression Recognition**, [[Paper]](https://arxiv.org/pdf/2204.04083.pdf), [[Code]](https://github.com/zczcwh/POSTER)

- (arXiv 2022.04) Vision Transformers for Single Image **Dehazing**, [[Paper]](https://arxiv.org/pdf/2204.03883.pdf), [[Code]](https://github.com/IDKiro/DehazeFormer)

- (arXiv 2022.04) Underwater **Image Enhancement** Using Pre-trained Transformer, [[Paper]](https://arxiv.org/pdf/2204.04199.pdf)

- (arXiv 2022.04) Event Transformer. A sparse-aware solution for efficient **event data processing**, [[Paper]](https://arxiv.org/pdf/2204.03355.pdf), [[Code]](https://github.com/AlbertoSabater/EventTransformer)

- (arXiv 2022.04) PSTR: End-to-End One-Step **Person Search** With Transformers, [[Paper]](https://arxiv.org/pdf/2204.03340.pdf), [[Code]](https://github.com/JialeCao001/PSTR)

- (arXiv 2022.04) Adapting **CLIP** For **Phrase Localization** Without Further Training, [[Paper]](https://arxiv.org/pdf/2204.03647.pdf), [[Code]](https://github.com/pals-ttic/adapting-CLIP)

- (arXiv 2022.04) FineDiving: A Fine-grained **Dataset** for Procedure-aware **Action Quality Assessment**, [[Paper]](https://arxiv.org/pdf/2204.03646.pdf), [[Project]](https://github.com/xujinglin/FineDiving)

- (arXiv 2022.04) DaViT: Dual **Attention** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.03645.pdf), [[Code]](https://github.com/dingmyu/davit)

- (arXiv 2022.04) Unsupervised Prompt Learning for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2204.03649.pdf), [[Code]](https://github.com/tonyhuang2022/UPL)

- (arXiv 2022.04) **Long Video Generation** with Time-Agnostic VQGAN and Time-Sensitive Transformer, [[Paper]](https://arxiv.org/pdf/2204.03638.pdf), [[Project]](https://songweige.github.io/projects/tats/index.html)

- (arXiv 2022.04) Unified Contrastive Learning in **Image-Text-Label** Space, [[Paper]](https://arxiv.org/pdf/2204.03610.pdf), [[Code]](https://github.com/microsoft/UniCL)

- (arXiv 2022.04) HunYuan_tvr for **Text-Video** Retrivial, [[Paper]](https://arxiv.org/pdf/2204.03382.pdf)

- (arXiv 2022.04) LEARNING TO COMPOSE SOFT PROMPTS FOR **COMPOSITIONAL ZERO-SHOT LEARNING**, [[Paper]]()

- (arXiv 2022.04) End-to-End Zero-Shot **HOI** Detection via **Vision and Language** Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2204.03541.pdf), [[Code]](https://github.com/mrwu-mac/EoID)

- (arXiv 2022.04) **Temporal Alignment** Networks for Long-term Video, [[Paper]](https://arxiv.org/pdf/2204.02968.pdf), [[Code]](https://www.robots.ox.ac.uk/~vgg/research/tan/)

- (arXiv 2022.04) Unleashing Vanilla Vision Transformer with Masked Image Modeling for **Object Detection**, [[Paper]](https://arxiv.org/pdf/2204.02964.pdf), [[Code]](https://github.com/hustvl/MIMDet)

- (arXiv 2022.04) MixFormer: **Mixing Features** across Windows and Dimensions, [[Paper]](https://arxiv.org/pdf/2204.02557.pdf), [[Code]](https://github.com/PaddlePaddle/PaddleClas)

- (arXiv 2022.04) CM3: A **CAUSAL** MASKED **MULTIMODAL** MODEL OF THE INTERNET, [[Paper]](https://arxiv.org/pdf/2201.07520.pdf)

- (arXiv 2022.04) DO AS I CAN, NOT AS I SAY: GROUNDING LANGUAGE IN **ROBOTIC** AFFORDANCES, [[Paper]](https://arxiv.org/pdf/2204.01691.pdf), [[Project]](https://say-can.github.io/)

- (arXiv 2022.04) TransGeo: Transformer Is All You Need for **Cross-view Image Geo-localization**, [[Paper]](https://arxiv.org/pdf/2204.00097.pdf), [[Code]](https://github.com/Jeff-Zilence/TransGeo2022)

- (arXiv 2022.04) Socratic Models: Composing **Zero-Shot Multimodal Reasoning** with Language, [[Paper]](https://arxiv.org/pdf/2204.00598.pdf), [[Project]](https://socraticmodels.github.io/)

- (arXiv 2022.04) Vision Transformer with Cross-attention by Temporal Shift for Efficient **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2204.00452.pdf)

- (arXiv 2022.04) Learning **Audio-Video** Modalities from Image Captions, [[Paper]](https://arxiv.org/pdf/2204.00679.pdf)

- (arXiv 2022.04) Improving Vision Transformers by Revisiting **High-frequency Components**, [[Paper]]()

- (arXiv 2022.04) POS-BERT: **Point Cloud** One-Stage BERT Pre-Training, [[Paper]](https://arxiv.org/pdf/2204.00989.pdf), [[Code]](https://github.com/fukexue/POS-BERT)

- (arXiv 2022.04) BinsFormer: Revisiting Adaptive Bins for **Monocular Depth Estimation**, [[Paper]](https://arxiv.org/pdf/2204.00987.pdf), [[Code]](https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox)

- (arXiv 2022.04) BatchFormerV2: Exploring Sample Relationships for **Dense Representation** Learning, [[Paper]](https://arxiv.org/pdf/2204.01254.pdf)

- (arXiv 2022.04) TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for **Repetitive Action Counting**, [[Paper]](https://arxiv.org/pdf/2204.01018.pdf)

- (arXiv 2022.04) **Long** Movie Clip Classification with State-Space **Video** Models, [[Paper]](https://arxiv.org/pdf/2204.01692.pdf), [[Code]](https://github.com/md-mohaiminul/ViS4mer)

- (arXiv 2022.04) TALLFormer: **Temporal Action Localization** with Long-memory Transformer, [[Paper]](https://arxiv.org/pdf/2204.01680.pdf), [[Code]](https://github.com/klauscc/TALLFormer)

- (arXiv 2022.04) Multi**MAE**: Multi-modal Multi-task Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2204.01678.pdf), [[Project]](https://multimae.epfl.ch/)

- (arXiv 2022.04) “This is my unicorn, Fluffy”: Personalizing frozen **vision-language** representations, [[Paper]](https://arxiv.org/pdf/2204.01694.pdf)

- (arXiv 2022.04) SE(3)-Equivariant Attention Networks for **Shape Reconstruction** in Function Space, [[Paper]](https://arxiv.org/pdf/2204.02394.pdf)

- (arXiv 2022.04) Multi-View Transformer for **3D Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2204.02174.pdf), [[Code]](https://github.com/sega-hsj/MVT-3DVG)

- (arXiv 2022.04) VISION TRANSFORMER EQUIPPED WITH NEURAL RESIZER ON **FACIAL EXPRESSION RECOGNITION** TASK, [[Paper]](https://arxiv.org/pdf/2204.02181.pdf)

- (arXiv 2022.04) Dual-AI: Dual-path Actor Interaction Learning for **Group Activity Recognition**, [[Paper]](https://arxiv.org/pdf/2204.02148.pdf), [[Project]](https://mingfei.info/Dual-AI/)

- (arXiv 2022.04) Detector-Free Weakly Supervised **Group Activity Recognition**, [[Paper]](https://arxiv.org/pdf/2204.02139.pdf)

- (arXiv 2022.04) Joint **Hand Motion and Interaction Hotspots Prediction** from Egocentric Videos, [[Paper]](https://arxiv.org/pdf/2204.01696.pdf), [[Project]](https://stevenlsw.github.io/hoi-forecast)

- (arXiv 2022.04) What to look at and where: Semantic and Spatial Refined Transformer for detecting **human-object interactions**, [[Paper]](https://arxiv.org/pdf/2204.00746.pdf)

- (arXiv 2022.04) MaxViT: **Multi-Axis** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2204.01697.pdf)

### 2022.03

- (arXiv 2022.03) Spatial-Temporal Parallel Transformer for **Arm-Hand Dynamic Estimation**, [[Paper]](https://arxiv.org/pdf/2203.16202.pdf)

- (arXiv 2022.03) ViSTA: **Vision** and **Scene Text** Aggregation for Cross-Modal **Retrieval**, [[Paper]](https://arxiv.org/pdf/2203.16778.pdf)

- (arXiv 2022.03) ReSTR: Convolution-free Referring Image Segmentation Using Transformers, [[Paper]](https://arxiv.org/pdf/2203.16768.pdf), [[Project]](http://cvlab.postech.ac.kr/research/restr/)

- (arXiv 2022.03) CREATE: A **Benchmark** for Chinese Short **Video Retrieval** and **Title Generation**, [[Paper]](https://arxiv.org/pdf/2203.16763.pdf)

- (arXiv 2022.03) **Deformable** **Video** Transformer, [[Paper]](https://arxiv.org/pdf/2203.16795.pdf)

- (arXiv 2022.03) End-to-End **Trajectory** Distribution Prediction Based on Occupancy Grid Maps, [[Paper]](https://arxiv.org/pdf/2203.16910.pdf)

- (arXiv 2022.03) CRAFT: Cross-Attentional Flow Transformer for Robust **Optical Flow**, [[Paper]](https://arxiv.org/pdf/2203.16896.pdf), [[Code]](https://github.com/askerlee/craft)

- (arXiv 2022.03) VL-InterpreT: An Interactive **Visualization** Tool for Interpreting **Vision-Language** Transformers, [[Paper]](https://arxiv.org/pdf/2203.17247.pdf), [[App]](http://vlinterpretenv4env-env.eba-vmhhefup.us-east-2.elasticbeanstalk.com/)

- (arXiv 2022.03) TransEditor: Transformer-Based Dual-Space **GAN** for Highly Controllable **Facial Editing**, [[Paper]](https://arxiv.org/pdf/2203.17266.pdf), [[Code]](https://github.com/BillyXYB/TransEditor)

- (arXiv 2022.03) BEVFormer: Learning **Bird’s-Eye-View** Representation from Multi-Camera Images via Spatiotemporal Transformers, [[Paper]](https://arxiv.org/pdf/2203.17270.pdf), [[Code]](https://github.com/zhiqi-li/BEVFormer)

- (arXiv 2022.03) **Visual Prompting**: Modifying Pixel Space to Adapt Pre-trained Models, [[Paper]](https://arxiv.org/pdf/2203.17274.pdf), [[Code]](https://hjbahng.github.io/visual_prompting/)

- (arXiv 2022.03) Bringing Old **Films** Back to Life, [[Paper]](https://arxiv.org/pdf/2203.17276.pdf), [[Code]](https://github.com/raywzy/Bringing-Old-Films-Back-to-Life)

- (arXiv 2022.03) Learning to Prompt for **Open-Vocabulary Object Detection** with Vision-Language Model, [[Paper]](https://arxiv.org/pdf/2203.14940.pdf), [[Code]](https://github.com/dyabel/detpro)

- (arXiv 2022.03) SeqTR: A Simple yet Universal Network for **Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2203.16265.pdf), [[Code]](https://github.com/sean-zhuh/SeqTR)

- (arXiv 2022.03) InstaFormer: Instance-Aware **Image-to-Image Translation** with Transformer, [[Paper]](https://arxiv.org/pdf/2203.16248.pdf)

- (arXiv 2022.03) Omni-DETR: **Omni-Supervised** Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.16089.pdf), [[Code]](https://github.com/amazon-research/omni-detr)

- (arXiv 2022.03) Learning **Program Representations** for Food Images and Cooking Recipes, [[Paper]](https://arxiv.org/pdf/2203.16071.pdf), [[Project]](http://cookingprograms.csail.mit.edu/)

- (arXiv 2022.03) ITTR: **Unpaired Image-to-Image Translation** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.16015.pdf)

- (arXiv 2022.03) VPTR: **Efficient** Transformers for **Video Prediction**, [[Paper]](https://arxiv.org/pdf/2203.15836.pdf), [[Code]](https://github.com/XiYe20/VPTR)

- (arXiv 2022.03) Parameter-**efficient** Fine-tuning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.16329.pdf)

- (arXiv 2022.03) TubeDETR: Spatio-Temporal Video **Grounding** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.16434.pdf), [[Code]](https://antoyang.github.io/tubedetr.html)

- (arXiv 2022.03) Exploring Plain Vision Transformer Backbones for Object **Detection**, [[Paper]](https://arxiv.org/pdf/2203.16527.pdf)

- (arXiv 2022.03) PROMPTDET: EXPAND YOUR **DETECTOR** VOCABULARY WITH UNCURATED IMAGES, [[Paper]](https://arxiv.org/pdf/2203.16513.pdf), [[Code]](https://fcjian.github.io/promptdet)

- (arXiv 2022.03) **Few-Shot** Object **Detection** with Fully Cross-Transformer, [[Paper]](https://arxiv.org/pdf/2203.15021.pdf)

- (arXiv 2022.03) Unified Transformer Tracker for Object **Tracking**, [[Paper]](https://arxiv.org/pdf/2203.15175.pdf)

- (arXiv 2022.03) X-Pool: Cross-Modal **Language-Video** Attention for Text-Video Retrieval, [[Paper]](https://arxiv.org/pdf/2203.15086.pdf), [[Code]](https://layer6ai-labs.github.io/xpool/)

- (arXiv 2022.03) Fine-tuning Image Transformers using Learnable **Memory**, [[Paper]](https://arxiv.org/pdf/2203.15243.pdf)

- (arXiv 2022.03) MAT: Mask-Aware Transformer for Large Hole Image **Inpainting**, [[Paper]](https://arxiv.org/pdf/2203.15270.pdf), [[Code]](https://github.com/fenglinglwb/MAT)

- (arXiv 2022.03) mc-BEiT: Multi-choice Discretization for **Image BERT** Pre-training, [[Paper]](https://arxiv.org/pdf/2203.15371.pdf)

- (arXiv 2022.03) End-to-End Transformer Based Model for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2203.15350.pdf)

- (arXiv 2022.03) Hybrid Routing Transformer for **Zero-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2203.15310.pdf)

- (arXiv 2022.03) TREATMENT LEARNING TRANSFORMER FOR **NOISY IMAGE CLASSIFICATION**, [[Paper]](https://arxiv.org/pdf/2203.15529.pdf)

- (arXiv 2022.03) Do **Vision-Language** Pretrained Models Learn **Primitive Concepts**?, [[Paper]](https://arxiv.org/pdf/2203.17271.pdf)

- (arXiv 2022.03) Transformer Inertial Poser: Attention-based Real-time **Human Motion Reconstruction** from Sparse **IMUs**, [[Paper]](https://arxiv.org/pdf/2203.15720.pdf)

- (arXiv 2022.03) SepViT: **Separable** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.15380.pdf)

- (arXiv 2022.03) MatteFormer: Transformer-Based **Image Matting** via Prior-Tokens, [[Paper]](https://arxiv.org/pdf/2203.15662.pdf), [[Code]](https://github.com/webtoon/matteformer)

- (arXiv 2022.03) Feature Selective Transformer for **Semantic Image Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.14124.pdf)

- (arXiv 2022.03) Bridge-Prompt: Towards **Ordinal Action Understanding** in Instructional Videos, [[Paper]](https://arxiv.org/pdf/2203.14104.pdf), [[Code]](https://github.com/ttlmh/Bridge-Prompt)

- (arXiv 2022.03) RSTT: Real-time Spatial Temporal Transformer for Space-Time **Video Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2203.14186.pdf), [[Code]](https://github.com/llmpass/RSTT)

- (arXiv 2022.03) Single-Stream Multi-Level Alignment for **Vision-Language** Pretraining, [[Paper]](https://arxiv.org/pdf/2203.14395.pdf)

- (arXiv 2022.03) Beyond Masking: Demystifying **Token-Based Pre-Training** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.14313.pdf), [[Code]](https://github.com/sunsmarterjie/beyond_masking)

- (arXiv 2022.03) Collaborative Transformers for **Grounded Situation Recognition**, [[Paper]](https://arxiv.org/pdf/2203.16518.pdf), [[Code]](https://github.com/jhcho99/CoFormer)

- (arXiv 2022.03) Object Memory Transformer for Object Goal **Navigation**, [[Paper]](https://arxiv.org/pdf/2203.14708.pdf)

- (arXiv 2022.03) Brain-inspired **Multilayer Perceptron** with **Spiking Neurons**, [[Paper]](https://arxiv.org/pdf/2203.14679.pdf), [[Code]](https://gitee.com/mindspore/models/tree/master/research/cv/snn_mlp)

- (arXiv 2022.03) HandOccNet: Occlusion-Robust **3D Hand Mesh Estimation** Network, [[Paper]](https://arxiv.org/pdf/2203.14564.pdf), [[Code]](https://github.com/namepllet/HandOccNet)

- (arXiv 2022.03) REGTR: End-to-end **Point Cloud Correspondences** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.14517.pdf), [[Code]](https://github.com/yewzijian/RegTR)

- (arXiv 2022.03) Automated Progressive Learning for **Efficient Training** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.14509.pdf)

- (arXiv 2022.03) Stratified Transformer for 3D **Point Cloud Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.14508.pdf), [[Code]](https://github.com/dvlab-research/Stratified-Transformer)

- (arXiv 2022.03) NOC-REK: Novel Object **Captioning** with Retrieved Vocabulary from External Knowledge, [[Paper]](https://arxiv.org/pdf/2203.14499.pdf)

- (arXiv 2022.03) **FACIAL EXPRESSION RECOGNITION** WITH SWIN TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2203.13472.pdf)

- (arXiv 2022.03) Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch **Robustness**, [[Paper]](https://arxiv.org/pdf/2203.13639.pdf)

- (arXiv 2022.03) Efficient Visual **Tracking** via Hierarchical Cross-Attention Transformer, [[Paper]](https://arxiv.org/pdf/2203.13537.pdf), [[Code]](https://github.com/chenxin-dlut/HCAT)

- (arXiv 2022.03) High-Performance Transformer **Tracking**, [[Paper]](https://arxiv.org/pdf/2203.13533.pdf), [[Code]](https://github.com/chenxin-dlut/TransT-M)

- (arXiv 2022.03) RayTran: **3D pose estimation** and **shape reconstruction** of multiple objects from videos with ray-traced transformers, [[Paper]](https://arxiv.org/pdf/2203.13296.pdf)

- (arXiv 2022.03) Multi-modal Multi-label **Facial Action Unit Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2203.13301.pdf)

- (arXiv 2022.03) MonoDETR: Depth-aware Transformer for Monocular **3D** Object **Detection**, [[Paper]](https://arxiv.org/pdf/2203.13310.pdf), [[Code]](https://github.com/ZrrSkywalker/MonoDETR.git)

- (arXiv 2022.03) **Text to Mesh** Without 3D Supervision Using Limit Subdivision, [[Paper]](https://arxiv.org/pdf/2203.13333.pdf), [[Project]](https://www.nasir.lol/clipmesh)

- (arXiv 2022.03) GEN-VLKT: Simplify Association and Enhance Interaction Understanding for **HOI Detection**, [[Paper]](https://arxiv.org/pdf/2203.13954.pdf), [[Code]](https://github.com/YueLiao/gen-vlkt)

- (arXiv 2022.03) CrossFormer: Cross Spatio-Temporal Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2203.13387.pdf)

- (arXiv 2022.03) FitCLIP: Refining Large-Scale Pretrained **Image-Text** Models for Zero-Shot Video Understanding Tasks, [[Paper]](https://arxiv.org/pdf/2203.13371.pdf), [[Code]](https://github.com/bryant1410/)

- (arXiv 2022.03) Vision Transformer **Compression** with Structured Pruning and Low Rank Approximation, [[Paper]](https://arxiv.org/pdf/2203.13444.pdf)

- (arXiv 2022.03) Multi-Modal Learning for **AU Detection** Based on Multi-Head Fused Transformers, [[Paper]](https://arxiv.org/pdf/2203.11441.pdf)

- (arXiv 2022.03) MSTR: Multi-Scale Transformer for End-to-End **Human-Object Interaction Detection**, [[Paper]](https://arxiv.org/pdf/2203.14709.pdf)

- (arXiv 2022.03) Learning Patch-to-Cluster **Attention** in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.11987.pdf)

- (arXiv 2022.03) Visual **Prompt Tuning**, [[Paper]](https://arxiv.org/pdf/2203.12119.pdf)

- (arXiv 2022.03) Training-free Transformer **Architecture Search**, [[Paper]](https://arxiv.org/pdf/2203.12217.pdf)

- (arXiv 2022.03) VideoMAE: Masked Autoencoders are Data-Efficient Learners for **Self-Supervised Video Pre-Training**, [[Paper]](https://arxiv.org/pdf/2203.12602.pdf), [[Code]](https://github.com/MCG-NJU/VideoMAE)

- (arXiv 2022.03) METAMORPH: LEARNING **UNIVERSAL CONTROLLERS** WITH TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2203.11931.pdf), [[Project]](https://metamorph-iclr.github.io/site/)

- (arXiv 2022.03) A Prompt Array Keeps the Bias Away: **Debiasing** **Vision-Language** Models with Adversarial Learning, [[Paper]](https://arxiv.org/pdf/2203.11933.pdf)

- (arXiv 2022.03) Reshaping **Robot Trajectories** Using Natural **Language** Commands: A Study of Multi-Modal Data Alignment Using Transformers, [[Paper]](https://arxiv.org/pdf/2203.13411.pdf), [[Project]](https://arthurfenderbucker.github.io/NL_trajectory_reshaper/)

- (arXiv 2022.03) Associating Objects with Scalable Transformers for **Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.11442.pdf), [[Project]](https://github.com/z-x-yang/AOT0

- (arXiv 2022.03) HOP: History-and-Order Aware Pre-training for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2203.11591.pdf), [[Code]](https://github.com/YanyuanQiao/HOP-VLN)

- (arXiv 2022.03) Learning to **generate line drawings** that convey geometry and semantics, [[Paper]](https://arxiv.org/pdf/2203.12691.pdf), [[Project]](https://carolineec.github.io/informative_drawings/)

- (arXiv 2022.03) UMT: Unified Multi-modal Transformers for Joint **Video Moment Retrieval** and **Highlight Detection**, [[Paper]](https://arxiv.org/pdf/2203.12745.pdf), [[Code]](https://github.com/TencentARC/UMT)

- (arXiv 2022.03) AIMusicGuru: Music Assisted **Human Pose Correction**, [[Paper]](https://arxiv.org/pdf/2203.12829.pdf)

- (arXiv 2022.03) What to Hide from Your Students: Attention-Guided **Masked Image Modeling**, [[Paper]](https://arxiv.org/pdf/2203.12719.pdf)

- (arXiv 2022.03) Towards Efficient and Elastic **Visual Question Answering** with Doubly Slimmable Transformer, [[Paper]](https://arxiv.org/pdf/2203.12814.pdf)

- (arXiv 2022.03) ViT-FOD: A Vision Transformer based **Fine-grained Object Discriminator**, [[Paper]](https://arxiv.org/pdf/2203.12816.pdf)

- (arXiv 2022.03) **Keypoints Tracking** via Transformer Networks, [[Paper]](https://arxiv.org/pdf/2203.12848.pdf), [[Code]](https://github.com/LexaNagiBator228/Keypoints-Tracking-via-Transformer-Networks/)

- (arXiv 2022.03) Beyond Fixation: **Dynamic Window** Visual Transformer, [[Paper]](https://arxiv.org/pdf/2203.12856.pdf), [[Code]](https://github.com/pzhren/DW-ViT)

- (arXiv 2022.03) Make-A-Scene: Scene-Based **Text-to-Image** Generation with Human Priors, [[Paper]](https://arxiv.org/pdf/2203.13131.pdf)

- (arXiv 2022.03) Self-supervised Video-centralised Transformer for **Video Face Clustering**, [[Paper]](https://arxiv.org/pdf/2203.13166.pdf)

- (arXiv 2022.03) Towards Exemplar-Free **Continual Learning** in Vision Transformers: an Account of Attention, Functional and Weight Regularization, [[Paper]](https://arxiv.org/pdf/2203.13167.pdf)

- (arXiv 2022.03) Global **Tracking** Transformers, [[Paper]](https://arxiv.org/pdf/2203.13250.pdf), [[Code]](https://github.com/xingyizhou/GTR)

- (arXiv 2022.03) **Video Instance Segmentation** via Multi-scale Spatio-temporal Split Attention Transformer, [[Paper]](https://arxiv.org/pdf/2203.13253.pdf), [[Code]](https://github.com/OmkarThawakar/MSSTS-VIS)

- (arXiv 2022.03) QS-Craft: Learning to Quantize, Scrabble and Craft for **Conditional Human Motion Animation**, [[Paper]](https://arxiv.org/pdf/2203.11632.pdf)

- (arXiv 2022.03) Look for the Change: Learning **Object States** and **State-Modifying Actions** from Untrimmed Web Videos, [[Paper]](https://arxiv.org/pdf/2203.11637.pdf), [[Project]](https://data.ciirc.cvut.cz/public/projects/2022LookForTheChange/)

- (arXiv 2022.03) GradViT: **Gradient Inversion** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.11894.pdf), [[Code]](https://gradvit.github.io/)

- (arXiv 2022.03) **Mask Usage Recognition** using Vision Transformer with Transfer Learning and Data Augmentation, [[Paper]](https://arxiv.org/pdf/2203.11542.pdf)

- (arXiv 2022.03) Under the Hood of Transformer Networks for **Trajectory Forecasting**, [[Paper]](https://arxiv.org/pdf/2203.11878.pdf)

- (arXiv 2022.03) **Open-Vocabulary DETR** with Conditional Matching, [[Paper]](https://arxiv.org/pdf/2203.11876.pdf)

- (arXiv 2022.03) Meta-attention for ViT-backed **Continual Learning**, [[Paper]](https://arxiv.org/pdf/2203.11684.pdf), [[Code]](https://github.com/zju-vipa/MEAT-TIL)

- (arXiv 2022.03) CNNs and Transformers Perceive **Hybrid Images** Similar to Humans, [[Paper]](https://arxiv.org/pdf/2203.11678.pdf), [[Code]](https://github.com/aliborji/hybrid_images.git)

- (arXiv 2022.03) Bailando: **3D Dance Generation** by Actor-Critic GPT with Choreographic Memory, [[Paper]](https://arxiv.org/pdf/2203.13055.pdf), [[Code]](https://github.com/lisiyao21/Bailando/)

- (arXiv 2022.03) Affective Feedback Synthesis Towards Multimodal **Text and Image** Data, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2203/2203.12692.pdf)

- (arXiv 2022.03) ViewFormer: **NeRF-free Neural Rendering** from Few Images Using Transformers, [[Paper]](https://arxiv.org/pdf/2203.10157.pdf)

- (arXiv 2022.03) **CLIP** on Wheels: Zero-Shot Object **Navigation** as Object Localization and Exploration, [[Paper]](https://arxiv.org/pdf/2203.10421.pdf)

- (arXiv 2022.03) Voxel Set Transformer: A Set-to-Set Approach to **3D Object Detection** from Point Clouds, [[Paper]](https://arxiv.org/pdf/2203.10314.pdf), [[Code]](https://github.com/skyhehe123/VoxSeT)

- (arXiv 2022.03) HIPA: Hierarchical Patch Transformer for Single Image **Super Resolution**, [[Paper]](https://arxiv.org/pdf/2203.10247.pdf)

- (arXiv 2022.03) DirecFormer: A Directed Attention in Transformer Approach to **Robust Action Recognition**, [[Paper]](https://arxiv.org/pdf/2203.10233.pdf), [[Code]](https://github.com/uark-cviu/DirecFormer)

- (arXiv 2022.03) MixFormer: End-to-End **Tracking** with Iterative Mixed Attention, [[Paper]](https://arxiv.org/pdf/2203.11082.pdf), [[Code]](https://github.com/MCG-NJU/MixFormer)

- (arXiv 2022.03) PersFormer: **3D Lane Detection** via Perspective Transformer and the OpenLane Benchmark, [[Paper]](https://arxiv.org/pdf/2203.11089.pdf), [[Code]](https://github.com/OpenPerceptionX/OpenLane)

- (arXiv 2022.03) Relationformer: A Unified Framework for **Image-to-Graph** Generation, [[Paper]](https://arxiv.org/pdf/2203.10202.pdf), [[Code]](https://github.com/suprosanna/relationformer)

- (arXiv 2022.03) **CLIP** meets GamePhysics: Towards **bug identification** in gameplay videos using zero-shot transfer learning, [[Paper]](https://arxiv.org/pdf/2203.11096.pdf), [[Code]](https://asgaardlab.github.io/CLIPxGamePhysics/)

- (arXiv 2022.03) **Hyperbolic** Vision Transformers: Combining Improvements in Metric Learning, [[Paper]](https://arxiv.org/pdf/2203.10833.pdf), [[Code]](https://github.com/htdt/hyp_metric)

- (arXiv 2022.03) MonoDTR: Monocular **3D Object Detection** with Depth-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2203.10981.pdf), [[Code]](https://github.com/kuanchihhuang/MonoDTR)

- (arXiv 2022.03) Transformer-based **HTR** for Historical Documents, [[Paper]](https://arxiv.org/pdf/2203.11008.pdf)

- (arXiv 2022.03) simCrossTrans: A Simple **Cross-Modality** Transfer Learning for Object **Detection** with ConvNets or Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.10456.pdf), [[Code]](https://github.com/liketheflower/simCrossTrans)

- (arXiv 2022.03) End-to-End **Human-Gaze-Target Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.10433.pdf)

- (arXiv 2022.03) End-to-End **Video Text Spotting** with Transformer, [[Paper]](https://arxiv.org/pdf/2203.10539.pdf), [[Code]](https://github.com/weijiawu/TransDETR)

- (arXiv 2022.03) Open-Vocabulary One-Stage **Detection** with Hierarchical **Visual-Language** Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2203.10593.pdf), [[Code]](https://github.com/mengqiDyangge/HierKD)

- (arXiv 2022.03) V2X-ViT: **Vehicle**-to-Everything Cooperative Perception with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.10638.pdf)

- (arXiv 2022.03) LocATe: End-to-end **Localization of Actions** in 3D with Transformers, [[Paper]](https://arxiv.org/pdf/2203.10719.pdf)

- (arXiv 2022.03) AnoViT: **Unsupervised Anomaly Detection and Localization** with Vision Transformer-based Encoder-Decoder, [[Paper]](https://arxiv.org/pdf/2203.10808.pdf)

- (arXiv 2022.03) ViM: **Out-Of-Distribution** with Virtual-logit Matching, [[Paper]](https://arxiv.org/pdf/2203.10807.pdf), [[Code]](https://github.com/haoqiwang/vim)

- (arXiv 2022.03) ScalableViT: Rethinking the Context-oriented **Generalization** of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.10790.pdf)

- (arXiv 2022.03) Iwin: **Human-Object Interaction Detection** via Transformer with Irregular Windows, [[Paper]](https://arxiv.org/pdf/2203.10537.pdf)

- (arXiv 2022.03) Vision Transformer with Convolutions **Architecture Search**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2203/2203.10435.pdf)

- (arXiv 2022.03) Cascade Transformers for End-to-End **Person Search**, [[Paper]](https://arxiv.org/pdf/2203.09642.pdf), [[Code]](https://github.com/Kitware/COAT)

- (arXiv 2022.03) CodedVTR: Codebook-based Sparse **Voxel** Transformer with Geometric Guidance, [[Paper]](https://arxiv.org/pdf/2203.09887.pdf)

- (arXiv 2022.03) MatchFormer: Interleaving Attention in Transformers for **Feature Matching**, [[Paper]](https://arxiv.org/pdf/2203.09645.pdf), [[Code]](https://github.com/jamycheung/MatchFormer)

- (arXiv 2022.03) Local-Global Context Aware Transformer for **Language-Guided Video Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.09773.pdf), [[Code]](https://github.com/leonnnop/Locater)

- (arXiv 2022.03) **Three things** everyone should know about Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.09795.pdf)

- (arXiv 2022.03) Are Vision Transformers **Robust** to Spurious Correlations? [[Paper]](https://arxiv.org/pdf/2203.09125.pdf), [[Code]](https://github.com/deeplearning-wisc/vit-spurious-robustness)

- (arXiv 2022.03) MUTUAL GENERATIVE TRANSFORMER LEARNING FOR **CROSS-VIEW GEO-LOCALIZATION**, [[Paper]](https://arxiv.org/pdf/2203.09135.pdf)

- (arXiv 2022.03) DU-VLG: Unifying **Vision-and-Language** Generation via Dual Sequence-to-Sequence Pre-training, [[Paper]](https://arxiv.org/pdf/2203.09052.pdf)

- (arXiv 2022.03) Semantic-aligned Fusion Transformer for **One-shot** Object **Detection**, [[Paper]](https://arxiv.org/pdf/2203.09093.pdf)

- (arXiv 2022.03) UNIMO-2: End-to-End Unified **Vision-Language** Grounded Learning, [[Paper]](https://arxiv.org/pdf/2203.09067.pdf), [[Code]](https://unimo-ptm.github.io/)

- (arXiv 2022.03) Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for **Few-shot Learning**, [[Paper]](https://arxiv.org/pdf/2203.09064.pdf), [[Code]](https://github.com/StomachCold/HCTransformers)

- (arXiv 2022.03) One-Shot Adaptation of **GAN** in Just One **CLIP**, [[Paper]](https://arxiv.org/pdf/2203.09301.pdf)

- (arXiv 2022.03) PanoFormer: Panorama Transformer for Indoor 360° **Depth Estimation**, [[Paper]](https://arxiv.org/pdf/2203.09283.pdf)

- (arXiv 2022.03) PreTR: Spatio-Temporal Non-Autoregressive **Trajectory Prediction** Transformer, [[Paper]](https://arxiv.org/pdf/2203.09293.pdf)

- (arXiv 2022.03) Look Outside the Room: **Synthesizing** A Consistent Long-Term **3D Scene Video** from A Single Image, [[Paper]](https://arxiv.org/pdf/2203.09457.pdf), [[Code]](https://xrenaa.github.io/look-outside-room/)

- (arXiv 2022.03) Transframer: Arbitrary **Frame Prediction** with Generative Models, [[Paper]](https://arxiv.org/pdf/2203.09494.pdf)

- (arXiv 2022.03) Towards Data-**Efficient** Detection Transformers, [[Paper]](https://arxiv.org/pdf/2203.09507.pdf), [[Code]](https://github.com/encounter1997/DE-DETRs)

- (arXiv 2022.03) Bi-directional Object-Context Prioritization Learning for **Saliency** Ranking, [[Paper]](https://arxiv.org/pdf/2203.09416.pdf), [[Code]](https://github.com/GrassBro/OCOR)

- (arXiv 2022.03) PATCH-FOOL: ARE VISION TRANSFORMERS ALWAYS ROBUST AGAINST **ADVERSARIAL** PERTURBATIONS? [[Paper]](https://arxiv.org/pdf/2203.08392.pdf), [[Code]](https://github.com/RICE-EIC/Patch-Fool)

- (arXiv 2022.03) WegFormer: Transformers for Weakly Supervised **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.08421.pdf)

- (arXiv 2022.03) **Open Set Recognition** using Vision Transformer with an Additional Detection Head, [[Paper]](https://arxiv.org/pdf/2203.08441.pdf), [[Code]](https://github.com/feiyang-cai/osr_vit.git)

- (arXiv 2022.03) UNIFIED VISUAL TRANSFORMER **COMPRESSION**, [[Paper]](https://arxiv.org/pdf/2203.08243.pdf), [[Code]](https://github.com/VITA-Group/UVC)

- (arXiv 2022.03) Towards Practical **Certifiable Patch Defense** with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.08519.pdf)

- (arXiv 2022.03) EDTER: **Edge Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2203.08566.pdf), [[Code]](https://github.com/MengyangPu/EDTER)

- (arXiv 2022.03) ActFormer: A GAN Transformer Framework towards General Action-Conditioned **3D Human Motion Generation**, [[Paper]](https://arxiv.org/pdf/2203.07706.pdf)

- (arXiv 2022.03) Rich CNN-Transformer Feature Aggregation Networks for **Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2203.07682.pdf)

- (arXiv 2022.03) Revitalize Region Feature for Democratizing **Video-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2203.07720.pdf), [[Code]](https://github.com/CuthbertCai/DemoVLP)

- (arXiv 2022.03) Inverted Pyramid Multi-task Transformer for **Dense Scene Understanding**, [[Paper]](https://arxiv.org/pdf/2203.07997.pdf)

- (arXiv 2022.03) Smoothing Matters: Momentum Transformer for Domain Adaptive **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.07988.pdf), [[Code]](https://github.com/alpc91/TransDA)

- (arXiv 2022.03) Style Transformer for **Image Inversion** and **Editing**, [[Paper]](https://arxiv.org/pdf/2203.07932.pdf), [[Code]](https://github.com/sapphire497/style-transformer)

- (arXiv 2022.03) MotionCLIP: Exposing Human **Motion Generation** to **CLIP** Space, [[Paper]](https://arxiv.org/pdf/2203.08063.pdf), [[Project]](https://guytevet.github.io/motionclip-page/)

- (arXiv 2022.03) The Principle of **Diversity**: Training Stronger Vision Transformers Calls for Reducing All Levels of **Redundancy**, [[Paper]](https://arxiv.org/pdf/2203.06345.pdf), [[Code]](https://github.com/VITA-Group/Diverse-ViT)

- (arXiv 2022.03) Enabling **Multimodal Generation** on CLIP via Vision-Language Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2203.06386.pdf)

- (arXiv 2022.03) Sparse Local Patch Transformer for Robust **Face Alignment** and **Landmarks Inherent Relation** Learning, [[Paper]](https://arxiv.org/pdf/2203.06541.pdf), [[Code]](https://github.com/Jiahao-UTS/SLPT-master)

- (arXiv 2022.03) Joint CNN and Transformer Network via weakly supervised Learning for efficient **crowd counting**, [[Paper]](https://arxiv.org/pdf/2203.06388.pdf)

- (arXiv 2022.03) DFTR: Depth-supervised Hierarchical Feature Fusion Transformer for **Salient Object Detection**, [[Paper]](https://arxiv.org/pdf/2203.06429.pdf)

- (arXiv 2022.03) DATR: Domain-adaptive transformer for **multi-domain landmark detection**, [[Paper]](https://arxiv.org/pdf/2203.06433.pdf)

- (arXiv 2022.03) EventFormer: AU Event Transformer for **Facial Action** Unit Event Detection, [[Paper]](https://arxiv.org/pdf/2203.06355.pdf)

- (arXiv 2022.03) Accelerating **DETR** **Convergence** via Semantic-Aligned Matching, [[Paper]](https://arxiv.org/pdf/2203.06883.pdf), [[Code]](https://github.com/ZhangGongjie/SAM-DETR)

- (arXiv 2022.03) All in One: Exploring Unified **Video-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2203.07303.pdf), [[Code]](https://github.com/showlab/all-in-one)

- (arXiv 2022.03) CLIP Models are **Few-shot** Learners: Empirical Studies on VQA and Visual Entailment, [[Paper]](https://arxiv.org/pdf/2203.07190.pdf)

- (arXiv 2022.03) EIT: **Efficiently** Lead Inductive Biases to ViT, [[Paper]](https://arxiv.org/pdf/2203.07116.pdf), [[Code]](https://github.com/MrHaiPi/EIT)

- (arXiv 2022.03) Self-Promoted Supervision for **Few-Shot** Transformer, [[Paper]](https://arxiv.org/pdf/2203.07057.pdf), [[Code]](https://github.com/DongSky/few-shot-vit)

- (arXiv 2022.03) MDMMT-2: Multidomain Multimodal Transformer for **Video Retrieval**, One More Step Towards Generalization, [[Paper]](https://arxiv.org/pdf/2203.07086.pdf)

- (arXiv 2022.03) Disentangled Representation Learning for **Text-Video** Retrieval, [[Paper]](https://arxiv.org/pdf/2203.07111.pdf)

- (arXiv 2022.03) TransCAM: Transformer Attention-based CAM Refinement for **Weakly Supervised Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.07239.pdf), [[Code]](https://github.com/liruiwen/TransCAM)

- (arXiv 2022.03) Synopses of Movie Narratives: a **Video-Language Dataset** for Story Understanding, [[Paper]](https://arxiv.org/pdf/2203.05711.pdf), [[Dataset]](https://github.com/insundaycathy/SYMON)

- (arXiv 2022.03) Visualizing and Understanding **Patch Interactions** in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.05922.pdf)

- (arXiv 2022.03) **ANTI-OVERSMOOTHING** IN DEEP VISION TRANSFORMERS VIA THE FOURIER DOMAIN ANALYSIS: FROM THEORY TO PRACTICE, [[Paper]](https://arxiv.org/pdf/2203.05962.pdf), [[Code]](https://github.com/VITA-Group/ViT-Anti-Oversmoothing)

- (arXiv 2022.03) Democratizing Contrastive **Language-Image** Pre-training: A CLIP **Benchmark** of Data, Model, and Supervision, [[Paper]](https://arxiv.org/pdf/2203.05796.pdf), [[Code]](https://github.com/Sense-GVT/DeCLIP)

- (arXiv 2022.03) ActiveMLP: An **MLP**-like Architecture with Active Token Mixer, [[Paper]](https://arxiv.org/pdf/2203.06108.pdf), [[Code]](https://github.com/microsoft/ActiveMLP)

- (arXiv 2022.03) **Zero-Shot Action Recognition** with Transformer-based Video Semantic Embedding, [[Paper]](https://arxiv.org/pdf/2203.05156.pdf)

- (arXiv 2022.03) TrueType Transformer: **Character and Font Style Recognition** in Outline Format, [[Paper]](https://arxiv.org/pdf/2203.05338.pdf)

- (arXiv 2022.03) LOOPITR: Combining Dual and Cross Encoder Architectures for **Image-Text** Retrieval, [[Paper]](https://arxiv.org/pdf/2203.05465.pdf)

- (arXiv 2022.03) MVP: **Multimodality**-guided Visual Pre-training, [[Paper]](https://arxiv.org/pdf/2203.05175.pdf)

- (arXiv 2022.03) DEER: Detection-agnostic End-to-End Recognizer for **Scene Text Spotting**, [[Paper]](https://arxiv.org/pdf/2203.05122.pdf)

- (arXiv 2022.03) **Multi-Modal** Mixup for **Robust** Fine-tuning, [[Paper]](https://arxiv.org/pdf/2203.03897.pdf)

- (arXiv 2022.03) AssistQ: Affordance-centric Question-driven Task Completion for **Egocentric Assistant**, [[Paper]](https://arxiv.org/pdf/2203.04203.pdf), [[Project]](https://showlab.github.io/assistq/)

- (arXiv 2022.03) **Coarse-to-Fine** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.03821.pdf), [[Code]](https://github.com/ChenMnZ/CF-ViT)

- (arXiv 2022.03) Monocular Robot **Navigation** with Self-Supervised Pretrained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.03682.pdf)

- (arXiv 2022.03) WAVEMIX: RESOURCE-**EFFICIENT** TOKEN MIXING FOR IMAGES, [[Paper]](https://arxiv.org/pdf/2203.03689.pdf)

- (arXiv 2022.03) VOVIT: LOW LATENCY GRAPH-BASED **AUDIO-VISUAL** VOICE SEPARATION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2203.04099.pdf), [[Code]](https://ipcv.github.io/VoViT/)

- (arXiv 2022.03) Graph Attention Transformer Network for **Multi-Label** Image **Classification**, [[Paper]](https://arxiv.org/pdf/2203.04049.pdf)

- (arXiv 2022.03) EDGEFORMER: IMPROVING **LIGHT-WEIGHT CONVNETS** BY LEARNING FROM VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2203.03952.pdf), [[Code]](https://github.com/hkzhang91/EdgeFormer)

- (arXiv 2022.03) Skating-Mixer: Multimodal **MLP** for **Scoring Figure Skating**, [[Paper]](https://arxiv.org/pdf/2203.03990.pdf)

- (arXiv 2022.03) Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group **Attention**, [[Paper]](https://arxiv.org/pdf/2203.03937.pdf)

- (arXiv 2022.03) CP-ViT: Cascade Vision Transformer **Pruning** via Progressive Sparsity Prediction, [[Paper]](https://arxiv.org/pdf/2203.04570.pdf)

- (arXiv 2022.03) Model-Agnostic Multitask Fine-tuning for Few-shot **Vision-Language** **Transfer Learning**, [[Paper]](https://arxiv.org/pdf/2203.04904.pdf)

- (arXiv 2022.03) ChiTransformer: Towards Reliable **Stereo** from Cues, [[Paper]](https://arxiv.org/pdf/2203.04554.pdf)

- (arXiv 2022.03) A Unified Transformer Framework for Group-based Segmentation: **Co-Segmentation**,** Co-Saliency Detection** and **Video Salient Object Detection**, [[Paper]](https://arxiv.org/pdf/2203.04708.pdf), [[Code]](https://github.com/suyukun666/UFO)

- (arXiv 2022.03) Coarse-to-Fine Sparse Transformer for **Hyperspectral Image Reconstruction**, [[Paper]](https://arxiv.org/pdf/2203.04845.pdf)

- (arXiv 2022.03) CMX: Cross-Modal Fusion for **RGB-X Semantic Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.04838.pdf), [[Code]](https://github.com/huaaaliu/RGBX_Semantic_Segmentation)

- (arXiv 2022.03) Multiscale Transformer for **Hyperspectral Image Classification**, [[Paper]](https://arxiv.org/pdf/2203.04771.pdf)

- (arXiv 2022.03) Mind the Gap: Understanding the Modality Gap in **Multi-modal Contrastive Representation** Learning, [[Paper]](https://arxiv.org/pdf/2203.02053.pdf), [[Code]](https://modalitygap.readthedocs.io/)

- (arXiv 2022.03) Autoregressive **Image Generation** using Residual Quantization, [[Paper]](https://arxiv.org/pdf/2203.01941.pdf)

- (arXiv 2022.03) CONTEXTFORMER: A TRANSFORMER WITH SPATIO-CHANNEL ATTENTION FOR CONTEXT MODELING IN LEARNED **IMAGE COMPRESSION**, [[Paper]](https://arxiv.org/pdf/2203.02452.pdf)

- (arXiv 2022.03) Patch Similarity Aware Data-Free **Quantization** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.02250.pdf)

- (arXiv 2022.03) ViT-P: Rethinking Data-**efficient** Vision Transformers from Locality, [[Paper]](https://arxiv.org/pdf/2203.02358.pdf)

- (arXiv 2022.03) DIT: SELF-SUPERVISED PRE-TRAINING FOR **DOCUMENT IMAGE** TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2203.02378.pdf)

- (arXiv 2022.03) Towards **Efficient** and **Scalable** Sharpness-Aware Minimization, [[Paper]](https://arxiv.org/pdf/2203.02714.pdf)

- (arXiv 2022.03) HyperTransformer: A Textural and Spectral Feature Fusion Transformer for **Pansharpening**, [[Paper]](https://arxiv.org/pdf/2203.02503.pdf), [[Code]](https://github.com/wgcban/HyperTransformer)

- (arXiv 2022.03) UVCGAN: UNET VISION TRANSFORMER CYCLE-CONSISTENT GAN FOR **UNPAIRED IMAGE-TO-IMAGE TRANSLATION**, [[Paper]](https://arxiv.org/pdf/2203.02557.pdf), [[Code]](https://github.com/LS4GAN/uvcgan)

- (arXiv 2022.03) Show Me What and Tell Me How: **Video Synthesis** via Multimodal Conditioning, [[Paper]](https://arxiv.org/pdf/2203.02573.pdf), [[Code]](https://github.com/snap-research/MMVID)

- (arXiv 2022.03) PANFORMER: A TRANSFORMER BASED MODEL FOR **PAN-SHARPENING**, [[Paper]](https://arxiv.org/pdf/2203.02916.pdf), [[Code]](https://github.com/zhysora/PanFormer)

- (arXiv 2022.03) Multi-class Token Transformer for **Weakly Supervised Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.02891.pdf), [[Code]](https://github.com/xulianuwa/MCTformer)

- (arXiv 2022.03) Cross Language Image Matching for **Weakly Supervised Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.02668.pdf)

- (arXiv 2022.03) Learning Affinity from Attention: End-to-End **Weakly-Supervised Semantic Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.02664.pdf), [[Code]](https://github.com/rulixiang/afa)

- (arXiv 2022.03) DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object **Detection**, [[Paper]](https://arxiv.org/pdf/2203.03605.pdf), [[Code]](https://github.com/IDEACVR/DINO)

- (arXiv 2022.03) MetaFormer : A Unified Meta Framework for **Fine-Grained Recognition**, [[Paper]](https://arxiv.org/pdf/2203.02751.pdf), [[Code]](https://github.com/dqshuai/MetaFormer)

- (arXiv 2022.03) **Audio-visual** Generalised Zero-shot Learning with Cross-modal Attention and Language, [[Paper]](https://arxiv.org/pdf/2203.03598.pdf)

- (arXiv 2022.03) Knowledge Amalgamation for Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2203.03187.pdf)

- (arXiv 2022.03) Learnable Irrelevant Modality Dropout for **Multimodal Action Recognition** on Modality-Specific Annotated Videos, [[Paper]](https://arxiv.org/pdf/2203.03014.pdf)

- (arXiv 2022.03) Modeling Coreference Relations in **Visual Dialog**, [[Paper]](https://arxiv.org/pdf/2203.02986.pdf), [[Code]](https://github.com/Mingxiao-Li/Modeling-Coreference-Relations-in-Visual-Dialog)

- (arXiv 2022.03) VITRANSPAD: VIDEO TRANSFORMER USING CONVOLUTION AND SELF-ATTENTION FOR **FACE PRESENTATION ATTACK DETECTION**, [[Paper]](https://arxiv.org/pdf/2203.01562.pdf)

- (arXiv 2022.03) Multi-Tailed Vision Transformer for **Efficient Inference**, [[Paper]](https://arxiv.org/pdf/2203.01587.pdf)

- (arXiv 2022.03) Bending Reality: Distortion-aware Transformers for Adapting to Panoramic **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2203.01452.pdf), [[Code]](https://github.com/jamycheung/Trans4PASS)

- (arXiv 2022.03) Ensembles of Vision Transformers as a New Paradigm for Automated Classification in **Ecology**, [[Paper]](https://arxiv.org/pdf/2203.01726.pdf)

- (arXiv 2022.03) LGT-Net: Indoor Panoramic Room **Layout Estimation** with Geometry-Aware Transformer Network, [[Paper]](https://arxiv.org/pdf/2203.01824.pdf), [[Code]](https://github.com/zhigangjiang/LGT-Net)

- (arXiv 2022.03) LatentFormer: Multi-Agent Transformer-Based **Interaction Modeling** and **Trajectory Prediction**, [[Paper]](https://arxiv.org/pdf/2203.01880.pdf)

- (arXiv 2022.03) DCT-Former: **Efficient** Self-Attention with Discrete Cosine Transform, [[Paper]](https://arxiv.org/pdf/2203.01178.pdf), [[Code]](https://github.com/cscribano/DCT-Former-Public)

- (arXiv 2022.03) Unsupervised **Vision-and-Language** Pre-training via Retrieval-based Multi-Granular Alignment, [[Paper]](https://arxiv.org/pdf/2203.00242.pdf)

- (arXiv 2022.03) Spatiotemporal Transformer Attention Network for 3D Voxel Level Joint Segmentation and Motion Prediction in **Point Cloud**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2203/2203.00138.pdf)

- (arXiv 2022.03) **CLIP**-GEN: Language-Free Training of a **Text-to-Image** Generator with CLIP, [[Paper]]()

- (arXiv 2022.03) MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for **3D** Human **Pose** Estimation in Video, [[Paper]](https://arxiv.org/pdf/2203.00859.pdf)

- (arXiv 2022.03) X -Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for **3D Dense Captioning**, [[Paper]](https://arxiv.org/pdf/2203.00843.pdf)

- (arXiv 2022.03) 3DCTN: 3D Convolution-Transformer Network for **Point Cloud** Classification, [[Paper]](https://arxiv.org/pdf/2203.00828.pdf)

- (arXiv 2022.03) DeciWatch: A Simple Baseline for 10× **Efficient** 2D and 3D **Pose** Estimation, [[Paper]](https://arxiv.org/pdf/2203.08713.pdf)

- (arXiv 2022.03) D_2ETR: **Decoder-Only DETR** with Computationally Efficient Cross-Scale Attention, [[Paper]](https://arxiv.org/pdf/2203.00860.pdf)

- (arXiv 2022.03) Incremental Transformer Structure Enhanced Image **Inpainting** with Masking Positional Encoding, [[Paper]](https://arxiv.org/pdf/2203.00867.pdf), [[Code]](https://github.com/DQiaole/ZITS_inpainting)

- (arXiv 2022.03) Self-supervised Transformer for **Deepfake Detection**, [[Paper]](https://arxiv.org/pdf/2203.01265.pdf)

- (arXiv 2022.03) Aggregated **Pyramid** Vision Transformer: Splittransform-merge Strategy for Image Recognition without Convolutions, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2203/2203.00960.pdf)

- (arXiv 2022.03) TransDARC: Transformer-based **Driver Activity Recognition** with Latent Space Feature Calibration, [[Paper]](https://arxiv.org/pdf/2203.00927.pdf), [[Code]](https://github.com/KPeng9510/TransDARC)

- (arXiv 2022.03) DN-DETR: **Accelerate** DETR **Training** by Introducing Query DeNoising, [[Paper]](https://arxiv.org/pdf/2203.01305.pdf), [[Code]](https://github.com/FengLi-ust/DN-DETR)

- (arXiv 2022.03) **Protecting Celebrities** with Identity Consistency Transformer, [[Paper]](https://arxiv.org/pdf/2203.01318.pdf)

- (arXiv 2022.03) Masked Visual Pre-training for **Motor Control**, [[Paper]](https://arxiv.org/pdf/2203.06173.pdf), [[Project]](https://tetexiao.com/projects/mvp)

- (arXiv 2022.03) NLX-GPT: A Model for Natural Language Explanations in Vision and **Vision-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2203.05081.pdf), [[Code]](https://github.com/fawazsammani/nlxgpt)

- (arXiv 2022.03) Conditional Prompt Learning for Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2203.05557.pdf), [[Code]](https://github.com/KaiyangZhou/CoOp)

- (arXiv 2022.03) **Lane Detection** with Versatile AtrousFormer and Local Semantic Guidance, [[Paper]](https://arxiv.org/pdf/2203.04067.pdf)

- (arXiv 2022.03) DALL-EVAL: Probing the Reasoning Skills and Social Biases of **Text-to-Image** Generative Transformers, [[Paper]](https://arxiv.org/pdf/2202.04053.pdf), [[Code]](https://github.com/j-min/DallEval)

- (arXiv 2022.03) **Forecasting** Characteristic **3D Poses** of Human Actions
, [[Paper]](https://arxiv.org/pdf/2011.15079.pdf), [[Code]](https://github.com/chrdiller/characteristic3dposes)

### 2022.02

- (arXiv 2022.02) **Bayesian Structure Learning** with Generative Flow Networks, [[Paper]](https://arxiv.org/pdf/2202.13903.pdf)

- (arXiv 2022.02) Towards **Unsupervised Domain Adaptation** via Domain-Transformer, [[Paper]](https://arxiv.org/pdf/2202.13777.pdf)

- (arXiv 2022.02) An End-to-End Transformer Model for **Crowd Localization**, [[Paper]](https://arxiv.org/pdf/2202.13065.pdf)

- (arXiv 2022.02) Instantaneous **Physiological Estimation** using Video Transformers, [[Paper]](https://arxiv.org/pdf/2202.12368.pdf), [[Code]](https://github.com/revanurambareesh/instantaneous_transformer)

- (arXiv 2022.02) Style**CLIP**Draw: Coupling Content and Style in **Text-to-Drawing** Translation, [[Paper]](https://arxiv.org/pdf/2202.12362.pdf), [[Code]](https://github.com/pschaldenbrand/StyleCLIPDraw)

- (arXiv 2022.02) ATTENTION ENABLES ZERO **APPROXIMATION** ERROR, [[Paper]](https://arxiv.org/pdf/2202.12166.pdf)

- (arXiv 2022.02) When Transformer Meets **Robotic Grasping**: Exploits Context for Efficient Grasp Detection, [[Paper]](https://arxiv.org/pdf/2202.11911.pdf), [[Code]](https://github.com/WangShaoSUN/grasp-transformer)

- (arXiv 2022.02) **AUTO-SCALING** VISION TRANSFORMERS WITHOUT TRAINING, [[Paper]](https://arxiv.org/pdf/2202.11921.pdf), [[Code]](https://github.com/VITA-Group/AsViT)

- (arXiv 2022.02) Think Global, Act Local: Dual-scale Graph Transformer for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2202.11742.pdf), [[Project]](https://cshizhe.github.io/projects/vln_duet.html)

- (arXiv 2022.02) LEARNING TO **MERGE TOKENS** IN VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2202.12015.pdf)

- (arXiv 2022.02) ProFormer: Learning **Data-efficient** Representations of **Body Movement** with Prototype-based Feature Augmentation and Visual Transformers, [[Paper]](https://arxiv.org/pdf/2202.11423.pdf), [[Code]](https://github.com/KPeng9510/ProFormer)

- (arXiv 2022.02) SELF-SUPERVISED TRANSFORMERS FOR **UNSUPERVISED OBJECT DISCOVERY** USING NORMALIZED CUT, [[Paper]](https://arxiv.org/pdf/2202.11539.pdf), [[Project]](https://www.m-psi.fr/Papers/TokenCut2022/)

- (arXiv 2022.02) Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer for Universal **Texture Synthesis**, [[Paper]](https://arxiv.org/pdf/2202.11703.pdf)

- (arXiv 2022.02) CaMEL: Mean Teacher Learning for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2202.10492.pdf)

- (arXiv 2022.02) **Hierarchical** Perceiver, [[Paper]](https://arxiv.org/pdf/2202.10890.pdf)

- (arXiv 2022.02) **Movies2Scenes**: Learning Scene Representations Using Movie Similarities, [[Paper]](https://arxiv.org/pdf/2202.10650.pdf)

- (arXiv 2022.02) GroupViT: **Semantic Segmentation** Emerges from Text Supervision, [[Paper]](https://arxiv.org/pdf/2202.11094.pdf), [[Code

- (arXiv 2022.02) Snowflake Point Deconvolution for **Point Cloud** Completion and Generation with Skip-Transformer, [[Paper]](https://arxiv.org/pdf/2202.09367.pdf), [[Code]](https://github.com/AllenXiangX/SnowflakeNet)

- (arXiv 2022.02) Audio Visual Scene-Aware **Dialog Generation** with Transformer-based Video Representations, [[Paper]](https://arxiv.org/pdf/2202.09979.pdf)

- (arXiv 2022.02) ViTAEv2: Vision Transformer Advanced by Exploring **Inductive Bias** for Image Recognition and Beyond, [[Paper]](https://arxiv.org/pdf/2202.10108.pdf)

- (arXiv 2022.02) PMP-Net++: **Point Cloud Completion** by Transformer-Enhanced Multi-step Point Moving Paths, [[Paper]](https://arxiv.org/pdf/2202.09507.pdf), [[Code]](https://github.com/diviswen/PMP-Net)

- (arXiv 2022.02) DataMUX: **Data Multiplexing** for Neural Networks, [[Paper]](https://arxiv.org/pdf/2202.09318.pdf), [[Code]](https://github.com/princeton-nlp/DataMUX)

- (arXiv 2022.02) On Guiding Visual **Attention** with **Language** Specification, [[Paper]](https://arxiv.org/pdf/2202.08926.pdf)

- (arXiv 2022.02) SPATIO-TEMPORAL OUTDOOR **LIGHTING AGGREGATION** ON IMAGE SEQUENCES USING TRANSFORMER NETWORKS, [[Paper]](https://arxiv.org/pdf/2202.09206.pdf)

- (arXiv 2022.02) **MISINFORMATION DETECTION** IN SOCIAL MEDIA **VIDEO** POSTS, [[Paper]](https://arxiv.org/pdf/2202.07706.pdf)

- (arXiv 2022.02) Can Deep Learning be Applied to Model-Based **Multi-Object Tracking**? [[Paper]](https://arxiv.org/pdf/2202.07909.pdf)

- (arXiv 2022.02) NOT ALL PATCHES ARE WHAT YOU NEED: EXPEDITING VISION TRANSFORMERS VIA **TOKEN REORGANIZATIONS**, [[Paper]](https://arxiv.org/pdf/2202.07800.pdf), [[Code]](https://github.com/youweiliang/evit)

- (arXiv 2022.02) ActionFormer: **Localizing** Moments of **Actions** with Transformers, [[Paper]](https://arxiv.org/pdf/2202.07925.pdf), [[Code]](https://github.com/happyharrycn/actionformer_release)

- (arXiv 2022.02) One Step at a Time: Long-Horizon **Vision-and-Language Navigation** with Milestones, [[Paper]](https://arxiv.org/pdf/2202.07028.pdf)

- (arXiv 2022.02) XAI for Transformers: Better **Explanations** through Conservative Propagation, [[Paper]](https://arxiv.org/pdf/2202.07304.pdf)

- (arXiv 2022.02) MeshLeTemp: Leveraging the Learnable Vertex-Vertex Relationship to Generalize Human **Pose** and **Mesh Reconstruction** for In-the-Wild Scenes, [[Paper]](https://arxiv.org/pdf/2202.07228.pdf)

- (arXiv 2022.02) ViNTER: **Image Narrative Generation** with Emotion-Arc-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2202.07305.pdf)

- (arXiv 2022.02) Hyper-relationship Learning Network for **Scene Graph** Generation, [[Paper]](https://arxiv.org/pdf/2202.07271.pdf)

- (arXiv 2022.02) CommerceMM: Large-Scale Commerce **MultiModal Representation** Learning with Omni Retrieval, [[Paper]](https://arxiv.org/pdf/2202.07247.pdf)

- (arXiv 2022.02) Flowformer: **Linearizing** Transformers with Conservation Flows, [[Paper]](https://arxiv.org/pdf/2202.06258.pdf)

- (arXiv 2022.02) DialFRED: Dialogue-Enabled Agents for **Embodied** Instruction Following, [[Paper]](https://arxiv.org/pdf/2202.13330.pdf), [[Code]](https://github.com/anonrabit/DialFRED)

- (arXiv 2022.02) CATs++: Boosting **Cost Aggregation** with Convolutions and Transformers, [[Paper]](https://arxiv.org/pdf/2202.06817.pdf)

- (arXiv 2022.02) Geometric Transformer for Fast and Robust **Point Cloud Registration**, [[Paper]](https://arxiv.org/pdf/2202.06688.pdf), [[Code]](https://arxiv.org/pdf/2202.06688.pdf)

- (arXiv 2022.02) I-Tuning: Tuning Language Models with Image for **Caption** Generation, [[Paper]](I-Tuning: Tuning Language Models with Image for Caption Generation)

- (arXiv 2022.02) Multi-direction and Multi-scale Pyramid in Transformer for Video-based **Pedestrian Retrieval**, [[Paper]](https://arxiv.org/pdf/2202.06014.pdf), [[Code]](https://git.openi.org.cn/zangxh/PiT.git)

- (arXiv 2022.02) **Visual Acoustic** Matching, [[Paper]](https://arxiv.org/pdf/2202.06875.pdf)

- (arXiv 2022.02) LighTN: **Light**-weight Transformer Network for Performance-overhead Tradeoff in **Point Cloud Downsampling**, [[Paper]](https://arxiv.org/pdf/2202.06263.pdf)

- (arXiv 2022.02) BViT: Broad **Attention** based Vision Transformer, [[Paper]](https://arxiv.org/pdf/2202.06268.pdf), [[Code]](https://github.com/DRL-CASIA/Dense_ViT)

- (arXiv 2022.02) Task-Adaptive Feature Transformer with Semantic Enrichment for **Few-Shot Segmentation**, [[Paper]](https://arxiv.org/pdf/2202.06498.pdf)

- (arXiv 2022.02) Domain Adaptation via **Prompt** Learning, [[Paper]](https://arxiv.org/pdf/2202.06687.pdf)

- (arXiv 2022.02) Mixing and Shifting: Exploiting Global and Local Dependencies in **Vision MLPs**, [[Paper]](https://arxiv.org/pdf/2202.06510.pdf), [[Code]](https://github.com/JegZheng/MS-MLP)

- (arXiv 2022.02) Wukong: 100 Million Large-scale Chinese **Cross-modal Pre-training** Dataset and A Foundation Framework, [[Paper]](https://arxiv.org/pdf/2202.06767.pdf), [[Project]](https://wukong-dataset.github.io/wukong-dataset/)

- (arXiv 2022.02) HOW DO VISION TRANSFORMERS WORK? [[Paper]](https://arxiv.org/pdf/2202.06709.pdf), [[Code]](https://github.com/xxxnell/how-do-vits-work)

- (arXiv 2022.02) ACORT: A Compact Object Relation Transformer for Parameter Efficient Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2202.05451.pdf), [[Code]](https://github.com/jiahuei/sparse-image-captioning)

- (arXiv 2022.02) **CLIP**asso: Semantically-Aware **Object Sketching**, [[Paper]](https://arxiv.org/pdf/2202.05822.pdf), [[Code]](https://clipasso.github.io/clipasso/)

- (arXiv 2022.02) Towards Weakly-Supervised **Text Spotting** using a Multi-Task Transformer, [[Paper]](https://arxiv.org/pdf/2202.05508.pdf)

- (arXiv 2022.02) DEEP **SOCCER CAPTIONING** WITH TRANSFORMER: DATASET, SEMANTICS-RELATED LOSSES, AND MULTI-LEVEL EVALUATION, [[Paper]](https://arxiv.org/pdf/2202.05728.pdf), [[Project]](https://sites.google.com/view/soccercaptioning)

- (arXiv 2022.02) ENTROFORMER: A TRANSFORMER-BASED ENTROPY MODEL FOR LEARNED **IMAGE COMPRESSION**, [[Paper]](https://arxiv.org/pdf/2202.05492.pdf), [[Code]](https://github.com/mx54039q/entroformer)

- (arXiv 2022.02) Image Difference Captioning with Pre-training and Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2202.04298.pdf), [[Code]](https://github.com/yaolinli/IDC)

- (arXiv 2022.02) MaskGIT: Masked **Generative** **Image** Transformer, [[Paper]](https://arxiv.org/pdf/2202.04200.pdf)

- (arXiv 2022.02) Distillation with Contrast is All You Need for **Self-Supervised** **Point Cloud** Representation Learning, [[Paper]](https://arxiv.org/pdf/2202.04241.pdf)

- (arXiv 2022.02) Motion-Aware Transformer For **Occluded Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2202.04243.pdf)

- (arXiv 2022.02) Conditional **Motion In-betweening**, [[Paper]](https://arxiv.org/pdf/2202.04307.pdf), [[Code]](https://arxiv.org/pdf/2202.04307.pdf)

- (arXiv 2022.02) Memory-based **gaze prediction** in deep imitation learning for **robot manipulation**, [[Paper]](https://arxiv.org/pdf/2202.04877.pdf)

- (arXiv 2022.02) **Spherical** Transformer, [[Paper]](https://arxiv.org/pdf/2202.04942.pdf)

- (arXiv 2022.02) OWL (Observe, Watch, Listen): **Localizing Actions** in **Egocentric Video** via Audiovisual Temporal Context, [[Paper]](https://arxiv.org/pdf/2202.04947.pdf)

- (arXiv 2022.02) The Abduction of Sherlock Holmes: A **Dataset** for **Visual Abductive Reasoning**, [[Paper]](https://arxiv.org/pdf/2202.04800.pdf), [[Project]](http://www.visualabduction.com/)

- (arXiv 2022.02) DALL-EVAL: Probing the Reasoning Skills and Social Biases of **Text-to-Image** Generative Transformers, [[Paper]](https://arxiv.org/pdf/2202.04053.pdf), [[Code]](https://github.com/j-min/DallEval)

- (arXiv 2022.02) Pre-Trained Language Models for **Interactive Decision-Making**, [[Paper]](https://arxiv.org/pdf/2202.01771.pdf)

- (arXiv 2022.02) TransFollower: Long-Sequence Car-Following **Trajectory Prediction** through Transformer, [[Paper]](https://arxiv.org/pdf/2202.03183.pdf)

- (arXiv 2022.02) The devil is in the labels: **Semantic segmentation** from sentences, [[Paper]](https://arxiv.org/pdf/2202.02002.pdf)

- (arXiv 2022.02) Webly Supervised Concept Expansion for **General Purpose Vision Models**, [[Paper]](https://arxiv.org/pdf/2202.02317.pdf), [[Project]](https://prior.allenai.org/projects/gpv2)

- (arXiv 2022.02) VU-BERT: A UNIFIED FRAMEWORK FOR **VISUAL DIALOG**, [[Paper]](https://arxiv.org/pdf/2202.10787.pdf)

- (arXiv 2022.02) **UNIFYING** ARCHITECTURES, TASKS, AND MODALITIES THROUGH A SIMPLE SEQUENCE-TO-SEQUENCE LEARNING FRAMEWORK, [[Paper]](https://arxiv.org/pdf/2202.03052.pdf), [[Code]](https://github.com/OFA-Sys/OFA)

- (arXiv 2022.02) Transformers in Self-Supervised **Monocular Depth Estimation** with Unknown Camera Intrinsics, [[Paper]](https://arxiv.org/pdf/2202.03131.pdf)

- (arXiv 2022.02) TRANSDREAMER: **REINFORCEMENT LEARNING** WITH TRANSFORMER WORLD MODELS, [[Paper]](https://arxiv.org/pdf/2202.09481.pdf)

- (arXiv 2022.02) **Vision-Language** Pre-Training with Triple Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2202.10401.pdf), [[Code]](https://github.com/uta-smile/TCL)

- (arXiv 2022.02) Corrupted Image Modeling for **Self-Supervised** Visual **Pre-Training**, [[Paper]](https://arxiv.org/pdf/2202.03382.pdf)

- (arXiv 2022.02) BLIP: Bootstrapping Language-Image Pre-training for Unified **Vision-Language** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2201.12086.pdf), [[Code]](https://github.com/salesforce/BLIP)

- (arXiv 2022.02) DNNFuser: Generative Pre-Trained Transformer as a Generalized Mapper for Layer Fusion in **DNN Accelerators**, [[Paper]](https://arxiv.org/pdf/2201.11218.pdf)

- (arXiv 2022.02) Interactron: **Embodied** Adaptive **Object Detection**, [[Paper]](https://arxiv.org/pdf/2202.00660.pdf)

- (arXiv 2022.02) Local Feature Matching with Transformers for low-end devices **LoFTR** method adaptation approach, [[Paper]](https://arxiv.org/pdf/2202.00770.pdf), [[Code]](https://github.com/Kolkir/Coarse_LoFTR_TRT)

- (arXiv 2022.02) Pre-Trained Language Models for **Interactive Decision-Making**, [[Paper]](https://arxiv.org/pdf/2202.01771.pdf)

- (arXiv 2022.02) Can Transformers be Strong **Treatment Effect Estimators**?, [[Paper]](https://arxiv.org/pdf/2202.01336.pdf)

- (arXiv 2022.02) Improving **Sample Efficiency of Value** Based Models Using Attention and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2202.00710.pdf)

- (arXiv 2022.02) Detecting **Human-Object Interactions** with Object-Guided Cross-Modal Calibrated Semantics, [[Paper]](https://arxiv.org/pdf/2202.00259.pdf), [[Code]](https://github.com/JacobYuan7/OCN-HOI-Benchmark)

### 2022.01

- (arXiv 2022.01) O-ViT: Orthogonal Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12133.pdf)

- (arXiv 2022.01) DynaMixer: A Vision **MLP** Architecture with Dynamic Mixing, [[Paper]](https://arxiv.org/pdf/2201.12083.pdf)

- (arXiv 2022.01) VRT: A **Video Restoration** Transformer, [[Paper]](https://arxiv.org/pdf/2201.12288.pdf), [[Code]](https://github.com/JingyunLiang/VRT)

- (arXiv 2022.01) DAB-DETR: DYNAMIC **ANCHOR** BOXES ARE BETTER QUERIES FOR **DETR**, [[Paper]](https://arxiv.org/pdf/2201.12329.pdf), [[Code]](https://github.com/SlongLiu/DAB-DETR)

- (arXiv 2022.01) Plug-In Inversion: Model-Agnostic **Inversion** for Vision with Data Augmentations, [[Paper]](https://arxiv.org/pdf/2201.12961.pdf)

- (arXiv 2022.01) MVP: Multi-Stage **Vision-Language** Pre-Training via Multi-Level Semantic Alignment, [[Paper]](https://arxiv.org/pdf/2201.12596.pdf)

- (arXiv 2022.01) VC-GPT: Visual Conditioned GPT for End-to-End Generative **Vision-and-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2201.12723.pdf)

- (arXiv 2022.01) BOAT: Bilateral Local **Attention** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.13027.pdf)

- (arXiv 2022.01) GRAPH SELF-ATTENTION FOR LEARNING **GRAPH** REPRESENTATION WITH TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2201.12787.pdf)

- (arXiv 2022.01) Aggregating **Global** Features into **Local** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12903.pdf), [[Code]](https://github.com/krushi1992/MOA-transformer)

- (arXiv 2022.01) Transformer Module Networks for Systematic Generalization in **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2201.11316.pdf)

- (arXiv 2022.01) Generalised Image **Outpainting** with U-Transformer, [[Paper]](https://arxiv.org/pdf/2201.11403.pdf)

- (arXiv 2022.01) RelTR: Relation Transformer for **Scene Graph Generation**, [[Paper]](https://arxiv.org/pdf/2201.11460.pdf)

- (arXiv 2022.01) DocSegTr: An Instance-Level End-to-End **Document Image Segmentation** Transformer, [[Paper]](https://arxiv.org/pdf/2201.11438.pdf)

- (arXiv 2022.01) Pre-Trained **Language** Transformers are Universal **Image** Classifiers, [[Paper]](https://arxiv.org/pdf/2201.10182.pdf)

- (arXiv 2022.01) Explore and Match: End-to-End **Video Grounding** with Transformer, [[Paper]](https://arxiv.org/pdf/2201.10168.pdf)

- (arXiv 2022.01) TGFuse: An **Infrared** and **Visible Image Fusion** Approach Based on Transformer and Generative Adversarial Network, [[Paper]](https://arxiv.org/pdf/2201.10147.pdf)

- (arXiv 2022.01) ViT-HGR: Vision Transformer-based **Hand Gesture Recognition** from High Density Surface EMG Signals, [[Paper]](https://arxiv.org/pdf/2201.10060.pdf)

- (arXiv 2022.01) ShapeFormer: Transformer-based **Shape Completion** via Sparse Representation, [[Paper]](https://arxiv.org/pdf/2201.10326.pdf), [[Project]](https://shapeformer.github.io/)

- (arXiv 2022.01) **CONVOLUTIONAL** XFORMERS FOR VISION, [[Paper]](https://arxiv.org/pdf/2201.10271.pdf), [[Code]](https://github.com/pranavphoenix/CXV)

- (arXiv 2022.01) DocEnTr: An End-to-End **Document Image Enhancement** Transformer, [[Paper]](https://arxiv.org/pdf/2201.10252.pdf), [[Code]](https://github.com/dali92002/DocEnTR)

- (arXiv 2022.01) Zero-Shot **Sketch** Based **Image Retrieval** using Graph Transformer, [[Paper]](https://arxiv.org/pdf/2201.10185.pdf)

- (arXiv 2022.01) SA-**VQA**: Structured Alignment of Visual and Semantic Representations for Visual Question Answering, [[Paper]](https://arxiv.org/pdf/2201.10654.pdf)

- (arXiv 2022.01) DUAL-TASKS SIAMESE TRANSFORMER FRAMEWORK FOR **BUILDING DAMAGE ASSESSMENT**, [[Paper]](https://arxiv.org/pdf/2201.10953.pdf)

- (arXiv 2022.01) When **Shift Operation** Meets Vision Transformer: An Extremely Simple Alternative to **Attention** Mechanism, [[Paper]](https://arxiv.org/pdf/2201.10801.pdf), [[Code]](https://github.com/microsoft/SPACH)

- (arXiv 2022.01) Self-supervised 3D Semantic Representation Learning for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2201.10788.pdf)

- (arXiv 2022.01) **Training** Vision Transformers with Only 2040 Images, [[Paper]](https://arxiv.org/pdf/2201.10728.pdf)

- (arXiv 2022.01) Learning To Recognize **Procedural Activities** with Distant Supervision, [[Paper]](https://arxiv.org/pdf/2201.10990.pdf)

- (arXiv 2022.01) EVALUATING **LANGUAGE**-BIASED **IMAGE** CLASSIFICATION BASED ON SEMANTIC REPRESENTATIONS, [[Paper]](https://arxiv.org/pdf/2201.11014.pdf)

- (arXiv 2022.01) A Comprehensive Study of Vision Transformers on **Dense Prediction Tasks**, [[Paper]](https://arxiv.org/pdf/2201.08683.pdf)

- (arXiv 2022.01) UniFormer: Unifying **Convolution** and **Self-attention** for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2201.09450.pdf), [[Code]](https://github.com/Sense-X/UniFormer)

- (arXiv 2022.01) **Patches** Are All You Need? [[Paper]](https://arxiv.org/pdf/2201.09792.pdf), [[Code]](https://github.com/locuslab/convmixer)

- (arXiv 2022.01) Reading-strategy Inspired Visual Representation Learning for **Text-to-Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2201.09168.pdf)

- (arXiv 2022.01) LEARNING TO ACT WITH AFFORDANCE-AWARE **MULTIMODAL** NEURAL **SLAM**, [[Paper]](https://arxiv.org/pdf/2201.09862.pdf)

- (arXiv 2022.01) Visual Information Guided **Zero-Shot Paraphrase Generation**, [[Paper]](https://arxiv.org/pdf/2201.09107.pdf)

- (arXiv 2022.01) TerViT: An **Efficient** **Ternary** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.08050.pdf)

- (arXiv 2022.01) End-to-end Generative Pretraining for **Multimodal Video Captioning**, [[Paper]](https://arxiv.org/pdf/2201.08264.pdf)

- (arXiv 2022.01) OMNIVORE: A Single Model for **Many** Visual **Modalities**, [[Paper]](https://arxiv.org/pdf/2201.08377.pdf), [[Project]](https://facebookresearch.github.io/omnivore/)

- (arXiv 2022.01) MeMViT: Memory-Augmented Multiscale Vision Transformer for **Efficient Long-Term Video Recognition**, [[Paper]](https://arxiv.org/pdf/2201.08383.pdf)

- (arXiv 2022.01) The CLEAR Benchmark: **Continual LEArning** on Real-World Imagery, [[Paper]](https://arxiv.org/pdf/2201.06289.pdf), [[Project]](https://clear-benchmark.github.io/)

- (arXiv 2022.01) ProposalCLIP: **Unsupervised** Open-Category Object **Proposal** Generation via Exploiting **CLIP** Cues, [[Paper]](https://arxiv.org/pdf/2201.06696.pdf)

- (arXiv 2022.01) Cross-modal Contrastive Distillation for **Instructional Activity Anticipation**, [[Paper]](https://arxiv.org/pdf/2201.06734.pdf)

- (arXiv 2022.01) Transformers in Action: **Weakly Supervised Action Segmentation**, [[Paper]](https://arxiv.org/pdf/2201.05675.pdf)

- (arXiv 2022.01) VAQF: Fully Automatic **Software-hardware Co-design** Framework for **Low-bit** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.06618.pdf)

- (arXiv 2022.01) CLIP-TD: **CLIP** Targeted Distillation for **Vision-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2201.05729.pdf)

- (arXiv 2022.01) **Domain Adaptation** via Bidirectional Cross-Attention Transformer, [[Paper]](https://arxiv.org/pdf/2201.05887.pdf)

- (arXiv 2022.01) Continual Transformers: Redundancy-Free Attention for **Online Inference**, [[Paper]](https://arxiv.org/pdf/2201.06268.pdf)

- (arXiv 2022.01) **Motion Inbetweening** via Deep ∆-Interpolator, [[Paper]](https://arxiv.org/pdf/2201.06701.pdf)

- (arXiv 2022.01) RePre: Improving **Self-Supervised** Vision Transformer with Reconstructive Pre-training, [[Paper]](https://arxiv.org/pdf/2201.06857.pdf)

- (arXiv 2022.01) GTrans: Spatiotemporal Autoregressive Transformer with Graph Embeddings for **Nowcasting Extreme Events**, [[Paper]](https://arxiv.org/pdf/2201.06717.pdf)

- (arXiv 2022.01) TransFuse: A Unified Transformer-based **Image Fusion** Framework using Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2201.07451.pdf)

- (arXiv 2022.01) Q-ViT: Fully Differentiable **Quantization** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.07703.pdf)

- (arXiv 2022.01) Disentangled Latent Transformer for **Interpretable Monocular Height Estimation**, [[Paper]](https://arxiv.org/pdf/2201.06357.pdf), [[Project]](https://github.com/ShadowXZT/DLT-Height-Estimation.pytorch)

- (arXiv 2022.01) Poseur: Direct Human **Pose Regression** with Transformers*, [[Paper]](https://arxiv.org/pdf/2201.07412.pdf)

- (arXiv 2022.01) SWINUNET3D - A HIERARCHICAL ARCHITECTURE FOR DEEP **TRAFFIC PREDICTION** USING SHIFTED WINDOW TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2201.06390.pdf), [[Code]](https://github.com/bojesomo/Traffic4Cast2021-SwinUNet3D)

- (arXiv 2022.01) SWIN-POSE: SWIN TRANSFORMER BASED HUMAN **POSE ESTIMATION**, [[Paper]](https://arxiv.org/pdf/2201.07384.pdf)

- (arXiv 2022.01) Look Closer: Bridging Egocentric and Third-Person Views with Transformers for **Robotic Manipulation**, [[Paper]](https://arxiv.org/pdf/2201.07779.pdf), [[Project]](https://jangirrishabh.github.io/lookcloser/)

- (arXiv 2022.01) ViT2Hash: Unsupervised Information-Preserving **Hashing**, [[Paper]](https://arxiv.org/pdf/2201.05541.pdf)

- (arXiv 2022.01) LANGUAGE-DRIVEN **SEMANTIC SEGMENTATION**, [[Paper]](https://arxiv.org/pdf/2201.03546.pdf), [[Code]](https://github.com/isl-org/lang-seg)

- (arXiv 2022.01) **Pedestrian Detection**: Domain Generalization, CNNs, Transformers and Beyond, [[Paper]](https://arxiv.org/pdf/2201.03176.pdf), [[Code]](https://github.com/hasanirtiza/Pedestron)

- (arXiv 2022.01) ImageSubject: A Large-scale Dataset for **Subject Detection**, [[Paper]](https://arxiv.org/pdf/2201.03101.pdf)

- (arXiv 2022.01) **Detecting** Twenty-thousand Classes using Image-level Supervision, [[Paper]](https://arxiv.org/pdf/2201.02605.pdf), [[Code]](https://github.com/facebookresearch/Detic)

- (arXiv 2022.01) Generalized **Category Discovery**, [[Paper]](https://arxiv.org/pdf/2201.02609.pdf), [[Code]](https://github.com/sgvaze/generalized-category-discovery)

- (arXiv 2022.01) Video **Summarization** Based on **Video-text** Modelling, [[Paper]](https://arxiv.org/pdf/2201.02494.pdf)

- (arXiv 2022.01) Spatio-Temporal Tuples Transformer for **Skeleton-Based Action Recognition**, [[Paper]](https://arxiv.org/pdf/2201.02849.pdf), [[Code]](https://github.com/heleiqiu/STTFormer)

- (arXiv 2022.01) **QUADTREE ATTENTION** FOR VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2201.02767.pdf), [[Code]](https://github.com/Tangshitao/QuadtreeAttention)

- (arXiv 2022.01) A Comprehensive Empirical Study of **Vision-Language** Pre-trained Model for Supervised Cross-Modal Retrieval, [[Paper]](https://arxiv.org/pdf/2201.02772.pdf), [[Project]](https://github.com/zhixiongz/CLIP4CMR)

- (arXiv 2022.01) MERLOT Reserve: Neural Script Knowledge through **Vision and Language and Sound**, [[Paper]](https://arxiv.org/pdf/2201.02639.pdf), [[Project]](https://rowanzellers.com/merlotreserve)

- (arXiv 2022.01) On the Efficacy of Co-Attention Transformer Layers in **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2201.03965.pdf)

- (arXiv 2022.01) Pyramid Fusion Transformer for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2201.04019.pdf)

- (arXiv 2022.01) Multiview Transformers for **Video Recognition**, [[Paper]](https://arxiv.org/pdf/2201.04288.pdf)

- (arXiv 2022.01) HYPERTRANSFORMER: MODEL GENERATION FOR SUPERVISED AND SEMI-SUPERVISED **FEW-SHOT LEARNING**, [[Paper]](https://arxiv.org/pdf/2201.04182.pdf)

- (arXiv 2022.01) UNIFORMER: UNIFIED TRANSFORMER FOR **EFFICIENT SPATIOTEMPORAL** REPRESENTATION LEARNING, [[Paper]](https://arxiv.org/pdf/2201.04676.pdf), [[Code]](https://github.com/Sense-X/UniFormer)

- (arXiv 2022.01) BridgeFormer: Bridging **Video-text** Retrieval with Multiple Choice Questions, [[Paper]](https://arxiv.org/pdf/2201.04850.pdf), [[Project]](https://geyuying.github.io/MCQ.html)

- (arXiv 2022.01) TransVOD: End-to-end **Video Object Detection** with Spatial-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2201.05047.pdf)

- (arXiv 2022.01) **CLIP**-Event: Connecting Text and Images with **Event** Structures, [[Paper]](https://arxiv.org/pdf/2201.05078.pdf), [[Code]](https://github.com/limanling/clip-event)

- (arXiv 2022.01) Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular **Vision-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2201.04026.pdf)

- (arXiv 2022.01) Lawin Transformer: Improving **Semantic Segmentation** Transformer with Multi-Scale Representations via Large Window Attention, [[Paper]](https://arxiv.org/pdf/2201.01615.pdf), [[Code]](https://github.com/yan-hao-tian/lawin)

- (arXiv 2022.01) **Self-Training** **Vision Language** BERTs with a Unified Conditional Model, [[Paper]](https://arxiv.org/pdf/2201.02010.pdf)

- (arXiv 2022.01) TransVPR: Transformer-based TransVPR: Transformer-based place recognition with multi-level attention aggregation with multi-level attention aggregation, [[Paper]](https://arxiv.org/pdf/2201.02001.pdf)

- (arXiv 2022.01) Compact Bidirectional Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2201.01984.pdf), [[Code]](https://github.com/YuanEZhou/CBTrans)

- (arXiv 2022.01) Flow-Guided Sparse Transformer for **Video Deblurring**, [[Paper]](https://arxiv.org/pdf/2201.01893.pdf)

- (arXiv 2022.01) **Stochastic Layers** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.15111.pdf)

- (arXiv 2022.01) ERNIE-VILG: UNIFIED GENERATIVE PRE-TRAINING FOR **BIDIRECTIONAL VISION-LANGUAGE GENERATION**, [[Paper]](https://arxiv.org/pdf/2112.15283.pdf)

- (arXiv 2022.01) InverseMV: **Composing Piano Scores** with a Convolutional **Video-Music** Transformer, [[Paper]](https://arxiv.org/pdf/2112.15320.pdf), [[Code]](https://github.com/linchintung/VMT)

- (arXiv 2022.01) CSformer: Bridging Convolution and Transformer for **Compressive Sensing**, [[Paper]](https://arxiv.org/pdf/2112.15299.pdf)

- (arXiv 2022.01) Persformer: A Transformer Architecture for **Topological Machine Learning**, [[Paper]](https://arxiv.org/pdf/2112.15210.pdf)

- (arXiv 2022.01) Vision Transformer **Slimming**: Multi-Dimension Searching in Continuous Optimization Space, [[Paper]](https://arxiv.org/pdf/2201.00814.pdf)

- (arXiv 2022.01) Language as Queries for **Referring Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2201.00487.pdf), [[Code]](https://github.com/wjn922/ReferFormer)

- (arXiv 2022.01) PyramidTNT: Improved **Transformer-in-Transformer** Baselines with Pyramid Architecture, [[Paper]](https://arxiv.org/pdf/2201.00978.pdf), [[Code]](https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch)

- (arXiv 2022.01) A TRANSFORMER-BASED SIAMESE NETWORK FOR **CHANGE DETECTION**, [[Paper]](https://arxiv.org/pdf/2201.01293.pdf), [[Code]](https://github.com/wgcban/ChangeFormer)

- (arXiv 2022.01) Vision Transformer with **Deformable Attention**, [[Paper]](https://arxiv.org/pdf/2201.00520.pdf), [[Code]](https://github.com/LeapLabTHU/DAT)

- (arXiv 2022.01) Splicing ViT Features for **Semantic Appearance Transfer**, [[Paper]](https://arxiv.org/pdf/2201.00424.pdf), [[Project]](https://splice-vit.github.io/)

- (arXiv 2022.01) Detail-Preserving Transformer for **Light Field Image Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2201.00346.pdf), [[Code]](https://github.com/BITszwang/DPT)

### 2021.12

- (arXiv 2021.12) Multi-Dimensional **Model Compression** of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.00043.pdf)

- (arXiv 2021.12) Siamese Network with Interactive Transformer for **Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.13983.pdf), [[Code]](https://github.com/LANMNG/SITVOS)

- (arXiv 2021.12) Pale Transformer: A General Vision Transformer **Backbone** with Pale-Shaped **Atention**, [[Paper]](https://arxiv.org/pdf/2112.14000.pdf), [[Code]](https://github.com/BR-IDL/PaddleViT)

- (arXiv 2021.12) APRIL: Finding the Achilles’ Heel on **Privacy** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.14087.pdf)

- (arXiv 2021.12) Synchronized Audio-Visual Frames with Fractional Positional Encoding for Transformers in **Video-to-Text Translation**, [[Paper]](https://arxiv.org/pdf/2112.14088.pdf)

- (arXiv 2021.12) Does **CLIP** Benefit **Visual Question Answering** in the Medical Domain as Much as it Does in the General Domain?, [[Paper]](https://arxiv.org/pdf/2112.13906.pdf)

- (arXiv 2021.12) SPViT: Enabling **Faster** Vision Transformers via Soft Token Pruning, [[Paper]](https://arxiv.org/pdf/2112.13890.pdf)

- (arXiv 2021.12) A FISTFUL OF WORDS: LEARNING TRANSFERABLE VISUAL MODELS FROM BAG-OF-WORDS SUPERVISION, [[Paper]](https://arxiv.org/pdf/2112.13884.pdf)

- (arXiv 2021.12) StyleGAN-V: A Continuous **Video** Generator with the Price, Image Quality and Perks of **StyleGAN2**, [[Paper]](https://arxiv.org/pdf/2112.14683.pdf), [[Code]](https://universome.github.io/stylegan-v)

- (arXiv 2021.12) A Simple Baseline for **Zero-shot Semantic Segmentation** with Pre-trained **Vision-language** Model, [[Paper]](https://arxiv.org/pdf/2112.14757.pdf), [[Code]](https://github.com/MendelXu/zsseg.baseline)

- (arXiv 2021.12) Miti-DETR: Object **Detection** based on Transformers with Mitigatory Self-Attention Convergence, [[Paper]](https://arxiv.org/pdf/2112.13310.pdf)

- (arXiv 2021.12) SIMVIT: EXPLORING A SIMPLE VISION TRANSFORMER WITH **SLIDING WINDOWS**, [[Paper]](https://arxiv.org/pdf/2112.13085.pdf), [[Code]](https://github.com/ucasligang/SimViT)

- (arXiv 2021.12) SGTR: End-to-end **Scene Graph Generation** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.12970.pdf)

- (arXiv 2021.12) **Video** Joint Modelling Based on Hierarchical Transformer for **Co-summarization**, [[Paper]](https://arxiv.org/pdf/2112.13478.pdf)

- (arXiv 2021.12) Vision Transformer for **Small-Size Datasets**, [[Paper]](https://arxiv.org/pdf/2112.13492.pdf)

- (arXiv 2021.12) Learning **Generative** Vision Transformer with Energy-Based Latent Space for **Saliency Prediction**, [[Paper]](https://arxiv.org/pdf/2112.13528.pdf)

- (arXiv 2021.12) ViR: the Vision **Reservoir**, [[Paper]](https://arxiv.org/pdf/2112.13545.pdf)

- (arXiv 2021.12) SeMask: Semantically Masked Transformers for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.12782.pdf), [[Code]](https://github.com/Picsart-AI-Research/SeMask-Segmentation)

- (arXiv 2021.12) Open-Vocabulary Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.12143.pdf)

- (arXiv 2021.12) ELSA: Enhanced Local **Self-Attention** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.12786.pdf), [[Code]](https://github.com/damo-cv/ELSA)

- (arXiv 2021.12) LaTr: Layout-Aware Transformer for **Scene-Text** **VQA**, [[Paper]](https://arxiv.org/pdf/2112.12494.pdf)

- (arXiv 2021.12) **Multimodal Personality Recognition** using Cross-Attention Transformer and Behaviour Encoding, [[Paper]](https://arxiv.org/pdf/2112.12180.pdf)

- (arXiv 2021.12) Fine-grained **Multi-Modal Self-Supervised Learning**, [[Paper]](https://arxiv.org/pdf/2112.12182.pdf)

- (arXiv 2021.12) SLIP: Self-supervision meets **Language-Image** Pre-training, [[Paper]](https://arxiv.org/pdf/2112.12750.pdf), [[Code]](https://github.com/facebookresearch/SLIP)

- (arXiv 2021.12) CLEVR3D: Compositional Language and Elementary Visual Reasoning for **Question Answering** in **3D Real-World Scenes**, [[Paper]](https://arxiv.org/pdf/2112.11691.pdf)

- (arXiv 2021.12) MIA-Former: **Efficient** and **Robust** Vision Transformers via Multi-grained Input Adaptation, [[Paper]](https://arxiv.org/pdf/2112.11542.pdf)

- (arXiv 2021.12) iSegFormer: Interactive Image **Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.11325.pdf), [[Code]](https://github.com/qinliuliuqin/iSegFormer.git)

- (arXiv 2021.12) Contrastive Object **Detection** Using Knowledge Graph Embeddings, [[Paper]](https://arxiv.org/pdf/2112.11366.pdf)

- (arXiv 2021.12) RepMLPNet: Hierarchical Vision **MLP** with Re-parameterized **Locality**, [[Paper]](https://arxiv.org/pdf/2112.11081.pdf), [[Code]](https://github.com/DingXiaoH/RepMLP)

- (arXiv 2021.12) **Lite** Vision Transformer with Enhanced **Self-Attention**, [[Paper]](https://arxiv.org/pdf/2112.10809.pdf), [[Code]](https://github.com/Chenglin-Yang/LVT)

- (arXiv 2021.12) MPViT : Multi-Path Vision Transformer for **Dense Prediction**, [[Paper]](https://arxiv.org/pdf/2112.11010.pdf), [[Code]](https://git.io/MPViT)

- (arXiv 2021.12) SOIT: **Segmenting** Objects with Instance-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2112.11037.pdf), [[Code]](https://github.com/yuxiaodongHRI/SOIT)

- (arXiv 2021.12) Learned Queries for Efficient Local **Attention**, [[Paper]](https://arxiv.org/pdf/2112.11435.pdf), [[Code]](https://github.com/moabarar/qna)

- (arXiv 2021.12) On **Efficient** Transformer and Image Pre-training for **Low-level** Vision, [[Paper]](https://arxiv.org/pdf/2112.10175.pdf), [[Code]](https://github.com/fenglinglwb/EDT)

- (arXiv 2021.12) LOCFORMER: Enabling Transformers to Perform **Temporal Moment Localization** on Long Untrimmed Videos With a Feature Sampling Approach, [[Paper]](https://arxiv.org/pdf/2112.10066.pdf)

- (arXiv 2021.12) Tell me what you see: A zero-shot **action recognition** method based on natural language descriptions, [[Paper]](https://arxiv.org/pdf/2112.09976.pdf), [[Code]](https://github.com/valterlej/zsarcap)

- (arXiv 2021.12) Pre-Training Transformers for **Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2112.09965.pdf)

- (arXiv 2021.12) ScanQA: 3D Question Answering for Spatial Scene Understanding, [[Paper]](https://arxiv.org/pdf/2112.10482.pdf)

- (arXiv 2021.12) Are Large-scale Datasets Necessary for Self-Supervised Pre-training? [[Paper]](https://arxiv.org/pdf/2112.10740.pdf)

- (arXiv 2021.12) StyleSwin: Transformer-based GAN for High-resolution **Image Generation**, [[Paper]](https://arxiv.org/pdf/2112.10762.pdf), [[Code]](https://github.com/microsoft/StyleSwin)

- (arXiv 2021.12) Mask2Former for **Video Instance Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.10764.pdf), [[Code]](https://github.com/facebookresearch/Mask2Former)

- (arXiv 2021.12) GLIDE: Towards Photorealistic **Image Generation** and **Editing** with **Text**-Guided Diffusion Models, [[Paper]](https://arxiv.org/pdf/2112.10741.pdf), [[Code]](https://github.com/openai/glide-text2im)

- (arXiv 2021.12) **Efficient** Visual **Tracking** with Exemplar Transformers, [[Paper]](https://arxiv.org/pdf/2112.09686.pdf), [[Code]](https://github.com/visionml/pytracking)

- (arXiv 2021.12) **Neuromorphic Camera Denoising** using Graph Neural Network-driven Transformers, [[Paper]](https://arxiv.org/pdf/2112.09685.pdf)

- (arXiv 2021.12) Align and Prompt: **Video-and-Language** Pre-training with Entity Prompts, [[Paper]](https://arxiv.org/pdf/2112.09583.pdf), [[Code]](https://github.com/salesforce/ALPRO)

- (arXiv 2021.12) DATA **EFFICIENT** **LANGUAGE-SUPERVISED ZEROSHOT RECOGNITION** WITH OPTIMAL TRANSPORT DISTILLATION, [[Paper]](https://arxiv.org/pdf/2112.09445.pdf)

- (arXiv 2021.12) SiamTrans: Zero-Shot Multi-Frame **Image Restoration** with Pre-Trained Siamese Transformers, [[Paper]](https://arxiv.org/pdf/2112.09426.pdf)

- (arXiv 2021.12) Full Transformer Framework for Robust **Point Cloud Registration** with Deep Information Interaction, [[Paper]](https://arxiv.org/pdf/2112.09385.pdf), [[Code]](https://github.com/CGuangyan-BIT/DIT)

- (arXiv 2021.12) ZeroVL: A Strong Baseline for Aligning **Vision-Language** Representations with **Limited Resources**, [[Paper]](https://arxiv.org/pdf/2112.09331.pdf)

- (arXiv 2021.12) Towards End-to-End **Image Compression and Analysis** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.09300.pdf)

- (arXiv 2021.12) How to **augment** your ViTs? Consistency loss and StyleAug, a random style transfer augmentation, [[Paper]](https://arxiv.org/pdf/2112.09260.pdf)

- (arXiv 2021.12) Learning to Prompt for **Continual Learning**, [[Paper]](https://arxiv.org/pdf/2112.08654.pdf), [[Code]](https://github.com/google-research/l2p)

- (arXiv 2021.12) Distilled Dual-Encoder Model for **Vision-Language** Understanding, [[Paper]](https://arxiv.org/pdf/2112.08723.pdf), [[Code]](https://github.com/kugwzk/Distilled-DualEncoder)

- (arXiv 2021.12) Dense Video **Captioning** Using Unsupervised Semantic Information, [[Paper]](https://arxiv.org/pdf/2112.08455.pdf), [[Code]](https://github.com/valterlej/dvcusi)

- (arXiv 2021.12) Looking Outside the Box to **Ground Language** in **3D** Scenes, [[Paper]](https://arxiv.org/pdf/2112.08879.pdf), [[Code]](https://github.com/nickgkan/beauty_detr)

- (arXiv 2021.12) Region**CLIP**: Region-based **Language-Image** Pretraining, [[Paper]](https://arxiv.org/pdf/2112.09106.pdf), [[Code]](https://github.com/microsoft/RegionCLIP)

- (arXiv 2021.12) DProST: **6-DoF Object Pose Estimation** Using Space Carving and Dynamic Projective Spatial Transformer, [[Paper]](https://arxiv.org/pdf/2112.08775.pdf)

- (arXiv 2021.12) Masked Feature Prediction for **Self-Supervised** Visual Pre-Training, [[Paper]](https://arxiv.org/pdf/2112.09133.pdf)

- (arXiv 2021.12) SGEITL: Scene Graph Enhanced Image-Text Learning for **Visual Commonsense Reasoning**, [[Paper]](https://arxiv.org/pdf/2112.08587.pdf)

- (arXiv 2021.12) TransZero++: Cross Attribute-Guided Transformer for **Zero-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2112.08643.pdf), [[Code]](https://github.com/shiming-chen/TransZero_pp)

- (arXiv 2021.12) Vision Transformer Based **Video Hashing Retrieval** for Tracing the Source of Fake Videos, [[Paper]](https://arxiv.org/pdf/2112.08117.pdf), [[Code]](https://github.com/lajlksdf/vtl)

- (arXiv 2021.12) Co-training Transformer with Videos and Images Improves **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2112.07175.pdf)

- (arXiv 2021.12) QAHOI: Query-Based Anchors for **Human-Object Interaction** Detection, [[Paper]](https://arxiv.org/pdf/2112.08647.pdf), [[Code]](https://github.com/cjw2021/QAHOI)

- (arXiv 2021.12) AdaViT: Adaptive Tokens for **Efficient** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.07658.pdf)

- (arXiv 2021.12) **CLIP**-Lite: Information **Efficient** Visual Representation Learning from Textual Annotations, [[Paper]](https://arxiv.org/pdf/2112.07133.pdf)

- (arXiv 2021.12) Towards a Unified Foundation Model: Jointly Pre-Training Transformers on **Unpaired Images and Text**, [[Paper]](https://arxiv.org/pdf/2112.07074.pdf)

- (arXiv 2021.12) Deep ViT Features as Dense Visual **Descriptors**, [[Paper]](https://arxiv.org/pdf/2112.05814.pdf), [[Project]](https://dino-vit-features.github.io/)

- (arXiv 2021.12) Geometry-Contrastive Transformer for Generalized 3D Pose Transfer, [[Paper]](https://arxiv.org/pdf/2112.07374.pdf), [[Code]](https://github.com/mikecheninoulu/CGT)

- (arXiv 2021.12) Temporal Transformer Networks with Self-Supervision for **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2112.07338.pdf)

- (arXiv 2021.12) COMPOSER: Compositional Learning of **Group Activity** in Videos, [[Paper]](https://arxiv.org/pdf/2112.05892.pdf)

- (arXiv 2021.12) Short and Long Range Relation Based Spatio-Temporal Transformer for **Micro-Expression Recognition**, [[Paper]](https://arxiv.org/pdf/2112.05851.pdf)

- (arXiv 2021.12) Improving and Diagnosing Knowledge-Based **Visual Question Answering** via Entity Enhanced Knowledge Injection, [[Paper]](https://arxiv.org/pdf/2112.06888.pdf)

- (arXiv 2021.12) SVIP: **Sequence VerIfication** for Procedures in **Videos**, [[Paper]](https://arxiv.org/pdf/2112.06447.pdf)

- (arXiv 2021.12) Improving Vision Transformers for **Incremental Learning**, [[Paper]](https://arxiv.org/pdf/2112.06103.pdf)

- (arXiv 2021.12) VL-ADAPTER: Parameter-Efficient Transfer Learning for **Vision-and-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2112.06825.pdf), [[Code]](https://github.com/ylsung/VL_adapter)

- (arXiv 2021.12) Embracing Single Stride **3D Object Detector** with Sparse Transformer, [[Paper]](https://arxiv.org/pdf/2112.06375.pdf), [[Code]](https://github.com/TuSimple/SST)

- (arXiv 2021.12) PartGlot: Learning **Shape Part Segmentation** from Language Reference Games, [[Paper]](https://arxiv.org/pdf/2112.06390.pdf)

- (arXiv 2021.12) **Pedestrian Trajectory Prediction** via Spatial Interaction Transformer Network, [[Paper]](https://arxiv.org/pdf/2112.06624.pdf)

- (arXiv 2021.12) LEARNING SEMANTIC-ALIGNED FEATURE REPRESENTATION FOR **TEXT-BASED PERSON SEARCH**, [[Paper]](https://arxiv.org/pdf/2112.06714.pdf)

- (arXiv 2021.12) L-Verse: Bidirectional **Generation** Between **Image** and **Text**, [[Paper]](https://arxiv.org/pdf/2111.11133.pdf)

- (arXiv 2021.12) **SELF-ATTENTION** DOES NOT NEED O(n^2) MEMORY, [[Paper]](https://arxiv.org/pdf/2112.05682.pdf)

- (arXiv 2021.12) Are Vision Transformers **Robust** to Patch Perturbations? [[Paper]](https://arxiv.org/pdf/2111.10659.pdf)

- (arXiv 2021.12) Mesa: A **Memory-saving Training** Framework for Transformers, [[Paper]](https://arxiv.org/pdf/2111.11124.pdf), [[Code]](https://github.com/zhuang-group/Mesa)

- (arXiv 2021.12) Injecting Semantic Concepts into End-to-End Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2112.05230.pdf)

- (arXiv 2021.12) MAGMA – Multimodal **Augmentation** of **Generative** Models through Adapter-based Finetuning, [[Paper]](https://arxiv.org/pdf/2112.05253.pdf)

- (arXiv 2021.12) LCTR: On Awakening the Local Continuity of Transformer for **Weakly Supervised Object Localization**, [[Paper]](https://arxiv.org/pdf/2112.05291.pdf)

- (arXiv 2021.12) FaceFormer: **Speech-Driven 3D Facial Animation** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.05329.pdf)

- (arXiv 2021.12) Rethinking the Two-Stage Framework for **Grounded Situation Recognition**, [[Paper]](https://arxiv.org/pdf/2112.05375.pdf), [[Code]](https://github.com/kellyiss/SituFormer)

- (arXiv 2021.12) **CLIP**2Style**GAN**: Unsupervised Extraction of StyleGAN Edit Directions, [[Paper]](https://arxiv.org/pdf/2112.05219.pdf)

- (arXiv 2021.12) Couplformer: Rethinking Vision Transformer with Coupling **Attention** Map, [[Paper]](https://arxiv.org/pdf/2112.05425.pdf)

- (arXiv 2021.12) Unified Multimodal Pre-training and Prompt-based Tuning for **Vision-Language** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2112.05587.pdf)

- (arXiv 2021.12) Visual Transformers with Primal Object Queries for **Multi-Label Image Classification**, [[Paper]](https://arxiv.org/pdf/2112.05485.pdf)

- (arXiv 2021.12) Colossal-AI: A Unified Deep Learning System For **Large-Scale Parallel Training**, [[Paper]](https://arxiv.org/pdf/2110.14883.pdf), [[Code]](https://github.com/hpcaitech/ColossalAI)

- (arXiv 2021.12) MS-TCT: Multi-Scale Temporal ConvTransformer for **Action Detection**, [[Paper]](https://arxiv.org/pdf/2112.03902.pdf)

- (arXiv 2021.12) Grounded **Language-Image** Pre-training, [[Paper]](https://arxiv.org/pdf/2112.03857.pdf), [[Code]](https://github.com/microsoft/GLIP)

- (arXiv 2021.12) U^2-Former: A Nested U-shaped Transformer for **Image Restoration**, [[Paper]](https://arxiv.org/pdf/2112.02279.pdf)

- (arXiv 2021.12) ADAPTIVE CHANNEL ENCODING TRANSFORMER FOR **POINT CLOUD** ANALYSIS, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2112/2112.02507.pdf)

- (arXiv 2021.12) Pose-guided Feature Disentangling for Occluded Person **Re-identification** Based on Transformer, [[Paper]](https://arxiv.org/pdf/2112.02466.pdf), [[Code]](https://github.com/WangTaoAs/PFD_Net)

- (arXiv 2021.12) VT-CLIP: Enhancing **Vision-Language** Models with Visual-guided Texts, [[Paper]](https://arxiv.org/pdf/2112.02399.pdf)

- (arXiv 2021.12) PointCLIP: **Point Cloud** Understanding by **CLIP**, [[Paper]](https://arxiv.org/pdf/2112.02413.pdf), [[Code]](https://github.com/ZrrSkywalker/PointCLIP)

- (arXiv 2021.12) Learning **Tracking** Representations via Dual-Branch Fully Transformer Networks, [[Paper]](https://arxiv.org/pdf/2112.02571.pdf), [[Code]](https://github.com/phiphiphi31/DualTFR)

- (arXiv 2021.12) DYNAMIC TOKEN **NORMALIZATION** IMPROVES VISION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2112.02624.pdf), [[Code]](https://github.com/wqshao126/DTN)

- (arXiv 2021.12) PTTR: Relational 3D **Point Cloud Object Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.02857.pdf), [[Code]](https://github.com/Jasonkks/PTTR)

- (arXiv 2021.12) GETAM: Gradient-weighted Element-wise Transformer Attention Map for **Weakly-supervised Semantic segmentation**, [[Paper]](https://arxiv.org/pdf/2112.02841.pdf)

- (arXiv 2021.12) **Text2Mesh**: Text-Driven Neural Stylization for Meshes, [[Paper]](https://arxiv.org/pdf/2112.03221.pdf), [[Project]](https://threedle.github.io/text2mesh/)

- (arXiv 2021.12) LMR-CBT: Learning Modality-fused Representations with CB-Transformer for Multimodal **Emotion Recognition** from Unaligned Multimodal Sequences, [[Paper]](https://arxiv.org/pdf/2112.01697.pdf)

- (arXiv 2021.12) Make A Long Image Short: Adaptive **Token** Length for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.01686.pdf)

- (arXiv 2021.12) FuseDream: Training-Free **Text-to-Image Generation** with Improved **CLIP**+GAN Space Optimization, [[Paper]](https://arxiv.org/pdf/2112.01573.pdf), [[Code]](https://github.com/gnobitab/FuseDream)

- (arXiv 2021.12) TransZero: Attribute-guided Transformer for **Zero-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2112.01683.pdf), [[Code]](https://github.com/shiming-chen/TransZero)

- (arXiv 2021.12) Learning Generalizable **Vision-Tactile** Robotic **Grasping** Strategy for Deformable Objects via Transformer, [[Paper]](https://arxiv.org/pdf/2112.06374.pdf), [[Code]](https://github.com/GTLIDAR/DeformableObjectsGrasping.git)

- (arXiv 2021.12) Hformer: Hybrid CNN-Transformer for **Fringe Order Prediction** in Phase Unwrapping of Fringe Projection, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2112/2112.06759.pdf)

- (arXiv 2021.12) Pre-training and Fine-tuning Transformers for **fMRI Prediction** Tasks, [[Paper]](https://arxiv.org/pdf/2112.05761.pdf)

- (arXiv 2021.12) Transformer based **trajectory prediction**, [[Paper]](https://arxiv.org/pdf/2112.04350.pdf)

- (arXiv 2021.12) Evaluating Transformers for Lightweight **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2111.09641.pdf)

- (arXiv 2021.12) Contextualized Spatio-Temporal **Contrastive Learning** with Self-Supervision, [[Paper]](https://arxiv.org/pdf/2112.05181.pdf)

- (arXiv 2021.12) CMA-CLIP: Cross-Modality Attention **CLIP** for **Image-Text** Classification, [[Paper]](https://arxiv.org/pdf/2112.03562.pdf)

- (arXiv 2021.12) **Bootstrapping** ViTs: Towards Liberating Vision Transformers from Pre-training, [[Paper]](https://arxiv.org/pdf/2112.03552.pdf)

- (arXiv 2021.12) Decision-based Black-box **Attack** Against Vision Transformers via Patch-wise Adversarial Removal, [[Paper]](https://arxiv.org/pdf/2112.03492.pdf), [[Code]](https://github.com/shiyuchengTJU/PAR)

- (arXiv 2021.12) DoodleFormer: Creative **Sketch Drawing** with Transformers, [[Paper]](https://arxiv.org/pdf/2112.03258.pdf)

- (arXiv 2021.12) Creating **Multimodal Interactive Agents** with Imitation and Self-Supervised Learning, [[Paper]](https://arxiv.org/pdf/2112.03763.pdf)

- (arXiv 2021.12) **AUDIO-VISUAL** SYNCHRONISATION IN THE WILD, [[Paper]](https://arxiv.org/pdf/2112.04432.pdf), [[Project]](https://www.robots.ox.ac.uk/~vgg/research/avs)

- (arXiv 2021.12) **Classification**-Then-**Grounding**: Reformulating **Video** Scene Graphs as Temporal Bipartite Graphs, [[Paper]](https://arxiv.org/pdf/2112.04222.pdf)

- (arXiv 2021.12) Garment4D: **Garment Reconstruction** from Point Cloud Sequences, [[Paper]](https://arxiv.org/pdf/2112.04159.pdf), [[Code]](https://github.com/hongfz16/Garment4D)

- (arXiv 2021.12) Locally Shifted **Attention****** With Early Global Integration, [[Paper]](https://arxiv.org/pdf/2112.05080.pdf), [[Code]](https://github.com/shellysheynin/Locally-SAG-Transformer)

- (arXiv 2021.12) BLT: Bidirectional Layout Transformer for Controllable **Layout Generation**, [[Paper]](https://arxiv.org/pdf/2112.05112.pdf)

- (arXiv 2021.12) PE-former: **Pose Estimation** Transformer, [[Paper]](https://arxiv.org/pdf/2112.04981.pdf), [[Project]](https://www.ics.forth.gr/hccv/)

- (arXiv 2021.12) Hair**CLIP**: **Design** Your Hair by Text and Reference Image, [[Paper]](https://arxiv.org/pdf/2112.05142.pdf), [[Project]](https://github.com/wty-ustc/HairCLIP)

- (arXiv 2021.12) **CLIP**-**NeRF**: Text-and-Image Driven Manipulation of Neural Radiance Fields, [[Paper]](https://arxiv.org/pdf/2112.05139.pdf), [[Code]](https://cassiepython.github.io/clipnerf/)

- (arXiv 2021.12) A Bilingual, Open World Video Text **Dataset** and End-to-end **Video Text Spotter** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.04888.pdf), [[Code]](https://github.com/weijiawu/TransVTSpotter), [[Dataset]](https://github.com/weijiawu/BOVText-Benchmark)

- (arXiv 2021.12) DualFormer: Local-Global Stratified Transformer for **Efficient Video Recognition**, [[Paper]](https://arxiv.org/pdf/2112.04674.pdf), [[Code]](https://github.com/sail-sg/dualformer)

- (arXiv 2021.12) Recurrent Glimpse-based Decoder for **Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.04632.pdf), [[Code]](https://github.com/zhechen/Deformable-DETR-REGO)

- (arXiv 2021.12) Fast **Point** Transformer, [[Paper]](https://arxiv.org/pdf/2112.04702.pdf)

- (arXiv 2021.12) Assistive Tele-op: Leveraging Transformers to **Collect Robotic Task Demonstrations**, [[Paper]](https://arxiv.org/pdf/2112.05129.pdf), [[Project]](https://sites.google.com/view/assistive-teleop)

- (arXiv 2021.12) Cross-Modality Fusion Transformer for **Multispectral Object Detection**, [[Paper]](https://arxiv.org/pdf/2111.00273.pdf)

- (arXiv 2021.12) PatchFormer: An **Efficient** **Point** Transformer with Patch Attention, [[Paper]](https://arxiv.org/pdf/2111.00207.pdf)

- (arXiv 2021.12) Transformer-Based Approach for Joint **Handwriting** and **Named Entity Recognition** in Historical documents, [[Paper]](https://arxiv.org/pdf/2112.04189.pdf)

- (arXiv 2021.12) **MLP** Architectures for **Vision-and-Language** Modeling: An Empirical Study, [[Paper]](https://arxiv.org/pdf/2112.04453.pdf), [[Code]](https://github.com/easonnie/mlp-vil)

- (arXiv 2021.12) Everything at Once – Multi-modal Fusion Transformer for **Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2112.04446.pdf)

- (arXiv 2021.12) Prompting **Visual-Language** Models for Efficient Video Understanding, [[Paper]](https://arxiv.org/pdf/2112.04478.pdf), [[Project]](https://ju-chen.github.io/efficient-prompt/)

- (arXiv 2021.12) FLAVA: A Foundational **Language And Vision** Alignment Model, [[Paper]](https://arxiv.org/pdf/2112.04482.pdf)

- (arXiv 2021.12) Embedding Arithmetic for **Text-driven Image Transformation**, [[Paper]](https://arxiv.org/pdf/2112.03162.pdf)

- (arXiv 2021.12) LAVT: Language-Aware Vision Transformer for **Referring Image Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.02244.pdf)

- (arXiv 2021.12) Look at What I’m Doing: Self-Supervised **Spatial Grounding** of Narrations in Instructional Videos, [[Paper]](https://arxiv.org/pdf/2110.10596.pdf), [[Project]](https://cs-people.bu.edu/rxtan/projects/grounding_narrations/)

- (arXiv 2021.12) Uni-Perceiver: Pre-training Unified Architecture for **Generic Perception** for **Zero-shot and Few-shot** Tasks, [[Paper]](https://arxiv.org/pdf/2112.01522.pdf)

- (arXiv 2021.12) Dense**CLIP**: Language-Guided **Dense** Prediction with Context-Aware Prompting, [[Paper]](https://arxiv.org/pdf/2112.01518.pdf), [[Code]](https://github.com/raoyongming/DenseCLIP)

- (arXiv 2021.12) Self-supervised **Video** Transformer, [[Paper]](https://arxiv.org/pdf/2112.01514.pdf), [[Code]](https://git.io/J1juJ)

- (arXiv 2021.12) OW-DETR: **Open-world Detection** Transformer, [[Paper]](https://arxiv.org/pdf/2112.01513.pdf)

- (arXiv 2021.12) Zero-Shot **Text-Guided Object Generation** with Dream Fields, [[Paper]](https://arxiv.org/pdf/2112.01455.pdf), [[Project]](https://ajayj.com/dreamfields)

- (arXiv 2021.12) **Video-Text** Pre-training with Learned Regions, [[Paper]](https://arxiv.org/pdf/2112.01194.pdf), [[Code]](https://github.com/ruiyan1995/Region_Learner)

- (arXiv 2021.12) MTFNet: Mutual-Transformer Fusion Network for **RGB-D Salient Object Detection**, [[Paper]](https://arxiv.org/pdf/2112.01177.pdf)

- (arXiv 2021.12) TCTN: A 3D-Temporal Convolutional Transformer Network for **Spatiotemporal** Predictive Learning, [[Paper]](https://arxiv.org/pdf/2112.01085.pdf)

- (arXiv 2021.12) DenseCLIP: Extract Free **Dense** Labels from **CLIP**, [[Paper]](https://arxiv.org/pdf/2112.01071.pdf)

- (arXiv 2021.12) TransMEF: A Transformer-Based **Multi-Exposure Image Fusion** Framework using Self-Supervised Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2112.01030.pdf)

- (arXiv 2021.12) SwinTrack: A Simple and Strong Baseline for Transformer **Tracking**, [[Paper]](https://arxiv.org/pdf/2112.00995.pdf), [[Code]](https://github.com/LitingLin/SwinTrack)

- (arXiv 2021.12) Object-Centric Unsupervised Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2112.00969.pdf)

- (arXiv 2021.12) Vision Pair Learning: An **Efficient** Training Framework for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2112.00965.pdf)

- (arXiv 2021.12) Visual-Semantic Transformer for **Scene Text Recognition**, [[Paper]](https://arxiv.org/pdf/2112.00948.pdf)

- (arXiv 2021.12) Differentiable **Spatial Planning** using Transformers, [[Paper]](https://arxiv.org/pdf/2112.01010.pdf), [[Project]](https://devendrachaplot.github.io/projects/spatial-planning-transformers)

- (arXiv 2021.12) Improved **Multiscale** Vision Transformers for **Classification** and **Detection**, [[Paper]](https://arxiv.org/pdf/2112.01526.pdf)

- (arXiv 2021.12) Masked-attention Mask Transformer for Universal Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2112.01527.pdf), [[Code]](https://bowenc0221.github.io/mask2former)

- (arXiv 2021.12) BEVT: BERT Pretraining of **Video** Transformers, [[Paper]](https://arxiv.org/pdf/2112.01529.pdf)

- (arXiv 2021.12) **Human-Object Interaction Detection** via Weak Supervision, [[Paper]](https://arxiv.org/pdf/2112.00492.pdf)

- (arXiv 2021.12) Learning Transformer Features for **Image Quality Assessment**, [[Paper]](https://arxiv.org/pdf/2112.00485.pdf)

- (arXiv 2021.12) **CLIP**styler: **Image Style Transfer** with a Single Text Condition, [[Paper]](https://arxiv.org/pdf/2112.00374.pdf)

- (arXiv 2021.12) **Multi-View Stereo** with Transformer, [[Paper]](https://arxiv.org/pdf/2112.00336.pdf)

- (arXiv 2021.12) VoRTX: **Volumetric 3D Reconstruction** With Transformers for Voxelwise View Selection and Fusion, [[Paper]](https://arxiv.org/pdf/2112.00236.pdf), [[Code]](https://noahstier.github.io/vortx)

- (arXiv 2021.12) Object-aware **Video-language** Pre-training for Retrieval, [[Paper]](https://arxiv.org/pdf/2112.00656.pdf), [[Code]](https://github.com/FingerRec/OA-Transformer)

### 2021.11

- (arXiv 2021.11) Multi-modal Transformers Excel at **Class-agnostic** Object **Detection**, [[Paper]](https://arxiv.org/pdf/2111.11430.pdf), [[Code]](https://git.io/J1HPY)

- (arXiv 2021.11) Predict, Prevent, and Evaluate: Disentangled **Text-Driven Image Manipulation** Empowered by Pre-Trained Vision-Language Model, [[Paper]](https://arxiv.org/pdf/2111.13333.pdf)

- (arXiv 2021.11) NomMer: Nominate Synergistic Context in Vision Transformer for **Visual Recognition**, [[Paper]](https://arxiv.org/pdf/2111.12994.pdf), [[Code]](https://github.com/NomMer1125/NomMer)

- (arXiv 2021.11) PolyViT: **Co-training** Vision Transformers on **Images**, **Videos** and **Audio**, [[Paper]](https://arxiv.org/pdf/2111.12993.pdf)

- (arXiv 2021.11) SWAT: Spatial Structure Within and Among Tokens, [[Paper]](https://arxiv.org/pdf/2111.13677.pdf)

- (arXiv 2021.11) ADAPTIVE **FOURIER** NEURAL OPERATORS: **EFFICIENT** TOKEN MIXERS FOR TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2111.13587.pdf)

- (arXiv 2021.11) DyTox: Transformers for **Continual Learning** with DYnamic TOken eXpansion, [[Paper]](https://arxiv.org/pdf/2111.11326.pdf), [[Code]](https://github.com/arthurdouillard/dytox)

- (arXiv 2021.11) DABS: A Domain-Agnostic **Benchmark** for **Self-Supervised** Learning, [[Paper]](https://arxiv.org/pdf/2111.12062.pdf), [[Code]](https://github.com/alextamkin/dabs)

- (arXiv 2021.11) Ice hockey **player identification** via transformers, [[Paper]](https://arxiv.org/pdf/2111.11535.pdf)

- (arXiv 2021.11) DBIA: Data-free Backdoor Injection **Attack** against Transformer Networks, [[Paper]](https://arxiv.org/pdf/2111.11870.pdf), [[Code]](https://anonymous.4open.science/r/DBIA-825D)

- (arXiv 2021.11) Sparse Fusion for **Multimodal** Transformers, [[Paper]](https://arxiv.org/pdf/2111.11992.pdf)

- (arXiv 2021.11) PhysFormer: **Facial Video-based Physiological Measurement** with Temporal Difference Transformer, [[Paper]](https://arxiv.org/pdf/2111.12082.pdf), [[Code]](https://github.com/ZitongYu/PhysFormer)

- (arXiv 2021.11) Self-Supervised Pre-Training for Transformer-Based Person **Re-Identification**, [[Paper]](https://arxiv.org/pdf/2111.12084.pdf), [[Code]](https://github.com/michuanhaohao/TransReID-SSL)

- (arXiv 2021.11) DISCRETE REPRESENTATIONS STRENGTHEN VISION TRANSFORMER **ROBUSTNESS**, [[Paper]](https://arxiv.org/pdf/2111.10493.pdf)

- (arXiv 2021.11) TRAVLR: Now You See It, Now You Don’t! Evaluating Cross-Modal Transfer of **Visio-Linguistic Reasoning**, [[Paper]](https://arxiv.org/pdf/2111.10756.pdf)

- (arXiv 2021.11) Crossing the Format Boundary of Text and Boxes: Towards Unified **Vision-Language** Modeling, [[Paper]](https://arxiv.org/pdf/2111.12085.pdf)

- (arXiv 2021.11) **Semi-Supervised** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.11067.pdf)

- (arXiv 2021.11) CpT: Convolutional Point Transformer for 3D **Point Cloud** Processing, [[Paper]](https://arxiv.org/pdf/2111.10866.pdf)

- (arXiv 2021.11) ZERO-SHOT CERTIFIED **DEFENSE** AGAINST **ADVERSARIAL** PATCHES WITH VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2111.10481.pdf)

- (arXiv 2021.11) PointMixer: MLP-Mixer for **Point Cloud** Understanding, [[Paper]](https://arxiv.org/pdf/2111.11187.pdf)

- (arXiv 2021.11) **MetaFormer** is Actually What You Need for Vision, [[Paper]](https://arxiv.org/pdf/2111.11418.pdf), [[Code]](https://github.com/sail-sg/poolformer)

- (arXiv 2021.11) Florence: A New **Foundation Model** for Computer Vision, [[Paper]](https://arxiv.org/pdf/2111.11432.pdf)

- (arXiv 2021.11) Benchmarking **Detection Transfer Learning** with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.11429.pdf)

- (arXiv 2021.11) Learning to **Compose Visual Relations**, [[Paper]](https://arxiv.org/pdf/2111.09297.pdf), [[Project]](https://composevisualrelations.github.io/)

- (arXiv 2021.11) REFERENCE-BASED **MAGNETIC RESONANCE IMAGE RECONSTRUCTION** USING TEXTURE TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2111.09492.pdf)

- (arXiv 2021.11) Induce, Edit, Retrieve: Language Grounded Multimodal Schema for **Instructional Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2111.09276.pdf)

- (arXiv 2021.11) **Swin Transformer V2**: Scaling Up Capacity and Resolution, [[Paper]](https://arxiv.org/pdf/2111.09883.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer)

- (arXiv 2021.11) SimMIM: A Simple Framework for **Masked Image Modeling**, [[Paper]](https://arxiv.org/pdf/2111.09886.pdf), [[Code]](https://github.com/microsoft/SimMIM)

- (arXiv 2021.11) Restormer: Efficient Transformer for **High-Resolution Image Restoration**, [[Paper]](https://arxiv.org/pdf/2111.09881.pdf), [[Code]](https://github.com/swz30/Restormer)

- (arXiv 2021.11) Simple but Effective: **CLIP** Embeddings for **Embodied AI**, [[Paper]](https://arxiv.org/pdf/2111.09888.pdf)

- (arXiv 2021.11) ClipCap: CLIP Prefix for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2111.09734.pdf), [[Code]](https://github.com/rmokady/CLIP_prefix_caption)

- (arXiv 2021.11) TransMix: Attend to **Mix** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.09833.pdf), [[Code]](https://github.com/Beckschen/TransMix)

- (arXiv 2021.11) TRIG: Transformer-Based **Text Recognizer** with Initial Embedding Guidance, [[Paper]](https://arxiv.org/pdf/2111.08314.pdf)

- (arXiv 2021.11) Multi-Grained **Vision Language** Pre-Training: Aligning Texts with Visual Concepts, [[Paper]](https://arxiv.org/pdf/2111.08276.pdf), [[Code]](https://github.com/zengyan-97/X-VLM)

- (arXiv 2021.11) Explainable Semantic Space by **Grounding Language to Vision** with Cross-Modal Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2111.07180.pdf), [[Code]](https://github.com/yizhen-zhang/VG-Bert)

- (arXiv 2021.11) Semantically Grounded Object Matching for Robust **Robotic Scene Rearrangement**, [[Paper]](https://arxiv.org/pdf/2111.07975.pdf), [[Code]](https://github.com/applied-ai-lab/object_matching)

- (arXiv 2021.11) **Tracking** People with **3D** Representations, [[Paper]](https://arxiv.org/pdf/2111.07868.pdf), [[Code]](https://brjathu.github.io/T3DP)

- (arXiv 2021.11) LiT: Zero-Shot Transfer with Locked-**image** **Text** **Tuning**, [[Paper]](https://arxiv.org/pdf/2111.07991.pdf)

- (arXiv 2021.11) FILIP: FINE-GRAINED INTERACTIVE **LANGUAGE-IMAGE** PRE-TRAINING, [[Paper]](https://arxiv.org/pdf/2111.07783.pdf)

- (arXiv 2021.11) Graph Relation Transformer: Incorporating **pairwise object features** into the Transformer architecture, [[Paper]](https://arxiv.org/pdf/2111.06075.pdf), [[Code]](https://github.com/derikclive/transformers)

- (arXiv 2021.11) **Attention** Approximates Sparse Distributed Memory, [[Paper]](https://arxiv.org/pdf/2111.05498.pdf)

- (arXiv 2021.11) SLICED **RECURSIVE** TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2111.05297.pdf), [[Code]](https://github.com/szq0214/SReT)

- (arXiv 2021.11) HYBRID **BYOL-VIT**: EFFICIENT APPROACH TO DEAL WITH **SMALL DATASETS**, [[Paper]](https://arxiv.org/pdf/2111.04845.pdf)

- (arXiv 2021.11) Tip-Adapter: Training-free **CLIP**-Adapter for Better **Vision-Language** Modeling, [[Paper]](https://arxiv.org/pdf/2111.03930.pdf), [[Code]](https://github.com/gaopengcuhk/Tip-Adapter)

- (arXiv 2021.11) Improving Visual Quality of **Image Synthesis** by A Token-based Generator with Transformers, [[Paper]](https://arxiv.org/pdf/2111.03481.pdf)

- (arXiv 2021.11) Style**CLIP**Draw: Coupling Content and Style in **Text-to-Drawing Synthesis**, [[Paper]](https://arxiv.org/pdf/2111.03133.pdf), [[Code]](https://github.com/pschaldenbrand/StyleCLIPDraw)

- (arXiv 2021.11) Revisiting **spatio-temporal** layouts for **compositional action recognition**, [[Paper]](https://arxiv.org/pdf/2111.01936.pdf), [[Code]](https://github.com/gorjanradevski/revisiting-spatial-temporal-layouts)

- (arXiv 2021.11) PatchGame: Learning to Signal Mid-level Patches in **Referential Games**, [[Paper]](https://arxiv.org/pdf/2111.01785.pdf), [[Code]](https://kampta.github.io/patch-game)

- (arXiv 2021.11) CAN VISION TRANSFORMERS PERFORM **CONVOLUTION**? [[Paper]](https://arxiv.org/pdf/2111.01353.pdf)

- (arXiv 2021.11) Livestock Monitoring with Transformer, [[Paper]](https://arxiv.org/pdf/2111.00801.pdf)

- (arXiv 2021.11) With a Little Help from my Temporal Context: Multimodal **Egocentric Action Recognition**, [[Paper]](https://arxiv.org/pdf/2111.01024.pdf), [[Code]](https://github.com/ekazakos/MTCN)

- (arXiv 2021.11) IconQA: A New Benchmark for Abstract Diagram Understanding and **Visual Language Reasoning**, [[Paper]](https://arxiv.org/pdf/2110.13214.pdf), [[Project]](https://iconqa.github.io/)

- (arXiv 2021.11) BoxeR: **Box-Attention** for 2D and 3D Transformers, [[Paper]](https://arxiv.org/pdf/2111.13087.pdf)

- (arXiv 2021.11) VLDeformer: **Vision-Language** Decomposed Transformer for Fast **Cross-Modal Retrieval**, [[Paper]](https://arxiv.org/pdf/2110.11338.pdf)

- (arXiv 2021.11) Multi-Person **3D Motion Prediction** with Multi-Range Transformers, [[Paper]](https://arxiv.org/pdf/2111.12073.pdf), [[Code]](https://jiashunwang.github.io/MRT/)

- (arXiv 2021.11) Scene Representation Transformer: Geometry-Free **Novel View Synthesis** Through Set-Latent Scene Representations, [[Paper]](https://arxiv.org/pdf/2111.13152.pdf), [[Project]](https://srt-paper.github.io/)

- (arXiv 2021.11) **Global Interaction Modelling** in Vision Transformer via Super Tokens, [[Paper]](https://arxiv.org/pdf/2111.13156.pdf)

- (arXiv 2021.11) ML-Decoder: Scalable and Versatile **Classification Head**, [[Paper]](https://arxiv.org/pdf/2111.12933.pdf), [[Code]](https://github.com/Alibaba-MIIL/ML_Decoder)

- (arXiv 2021.11) Exploiting Both Domain-specific and Invariant Knowledge via a Win-win Transformer for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2111.12941.pdf)

- (arXiv 2021.11) SWINBERT: End-to-End Transformers with Sparse Attention for **Video Captioning**, [[Paper]](https://arxiv.org/pdf/2111.13196.pdf)

- (arXiv 2021.11) Amortized Prompt: Lightweight Fine-Tuning for **CLIP** in **Domain Generalization**, [[Paper]](https://arxiv.org/pdf/2111.12853.pdf)

- (arXiv 2021.11) Universal Captioner: Long-Tail **Vision-and-Language** Model Training through Content-Style Separation, [[Paper]](https://arxiv.org/pdf/2111.12727.pdf)

- (arXiv 2021.11) **Sparse** is Enough in Scaling Transformers, [[Paper]](https://arxiv.org/pdf/2111.12763.pdf)

- (arXiv 2021.11) An implementation of the “**Guess who**?” game using CLIP, [[Paper]](https://arxiv.org/pdf/2112.00599.pdf), [[Code]](https://github.com/ArnauDIMAI/CLIP-GuessWho)

- (arXiv 2021.11) HEAT: Holistic Edge Attention Transformer for **Structured Reconstruction**, [[Paper]](https://arxiv.org/pdf/2111.15143.pdf)

- (arXiv 2021.11) A Unified **Pruning** Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.15127.pdf)

- (arXiv 2021.11) Pyramid **Adversarial Training** Improves ViT Performance, [[Paper]](https://arxiv.org/pdf/2111.15121.pdf)

- (arXiv 2021.11) AssistSR: Affordance-centric Question-driven **Video Segment Retrieval**, [[Paper]](https://arxiv.org/pdf/2111.15050.pdf), [[Code & Data]](https://github.com/StanLei52/AQVSR)

- (arXiv 2021.11) DAFormer: Improving Network Architectures and Training Strategies for **Domain-Adaptive Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2111.14887.pdf), [[Code]](https://github.com/lhoyer/DAFormer)

- (arXiv 2021.11) , [[Paper]](https://arxiv.org/pdf/2111.14887.pdf)

- (arXiv 2021.11) AdaViT: Adaptive Vision Transformers for **Efficient** Image Recognition, [[Paper]](https://arxiv.org/pdf/2111.15668.pdf)

- (arXiv 2021.11) ATS: Adaptive Token Sampling For **Efficient** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.15667.pdf)

- (arXiv 2021.11) **CLIP** Meets Video Captioners: Attribute-Aware Representation Learning Promotes Accurate **Captioning**, [[Paper]](https://arxiv.org/pdf/2111.15162.pdf)

- (arXiv 2021.11) CRIS: **CLIP**-Driven Referring Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2111.15174.pdf)

- (arXiv 2021.11) Shunted **Self-Attention** via Multi-Scale Token Aggregation, [[Paper]](https://arxiv.org/pdf/2111.15193.pdf), [[Code]](https://github.com/OliverRensu/Shunted-Transformer)

- (arXiv 2021.11) MC-SSL0.0: Towards Multi-Concept **Self-Supervised** Learning, [[Paper]](https://arxiv.org/pdf/2111.15340.pdf)

- (arXiv 2021.11) TransWeather: Transformer-based **Restoration of Images** Degraded by Adverse Weather Conditions, [[Paper]](https://arxiv.org/pdf/2111.14813.pdf), [[Code]](https://github.com/jeya-maria-jose/TransWeather)

- (arXiv 2021.11) Searching the **Search Space** of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.14725.pdf), [[Code]](https://github.com/microsoft/Cream)

- (arXiv 2021.11) TransMVSNet: Global Context-aware **Multi-view Stereo** Network with Transformers, [[Paper]](https://arxiv.org/pdf/2111.14600.pdf), [[Code]](https://github.com/MegviiRobot/TransMVSNet)

- (arXiv 2021.11) **Recurrent** Vision Transformer for Solving Visual **Reasoning** Problems, [[Paper]]()

- (arXiv 2021.11) **Video Frame Interpolation** Transformer, [[Paper]](https://arxiv.org/pdf/2111.13817.pdf)

- (arXiv 2021.11) FQ-ViT: Fully **Quantized** Vision Transformer without Retraining, [[Paper]](https://arxiv.org/pdf/2111.13824.pdf), [[Code]](https://github.com/linyang-zhh/FQ-ViT)

- (arXiv 2021.11) LAFITE : Towards Language-Free Training for **Text-to-Image Generation**, [[Paper]](https://arxiv.org/pdf/2111.13792.pdf)

- (arXiv 2021.11) SPARSE DETR: **EFFICIENT** END-TO-END OBJECT **DETECTION** WITH LEARNABLE SPARSITY, [[Paper]](https://arxiv.org/pdf/2111.14330.pdf), [[Code]](https://github.com/kakaobrain/sparse-detr)

- (arXiv 2021.11) End-to-End **Referring Video Object Segmentation** with Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2111.14821.pdf), [[Code]](https://github.com/mttr2021/MTTR)

- (arXiv 2021.11) Point-BERT: Pre-training 3D **Point Cloud** Transformers with Masked Point Modeling, [[Paper]](https://arxiv.org/pdf/2111.14819.pdf), [[Code]](https://github.com/lulutang0608/Point-BERT)

- (arXiv 2021.11) Zero-Shot **Image-to-Text Generation** for Visual-Semantic Arithmetic, [[Paper]](https://arxiv.org/pdf/2111.14447.pdf), [[Code]](https://github.com/YoadTew/zero-shot-image-to-text)

- (arXiv 2021.11) Blended Diffusion for **Text-driven Editing** of **Natural Images**, [[Paper]](https://arxiv.org/pdf/2111.14818.pdf), [[Code]](https://github.com/omriav/blended-diffusion)

- (arXiv 2021.11) Mask Transfiner for High-Quality **Instance Segmentation**, [[Paper]](https://arxiv.org/pdf/2111.13673.pdf), [[Code]](http://vis.xyz/pub/transfiner)

- (arXiv 2021.11) MHFormer: Multi-Hypothesis Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2111.12707.pdf), [[Code]](https://github.com/Vegetebird/MHFormer)

- (arXiv 2021.11) PeCo: Perceptual Codebook for **BERT Pre-training** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12710.pdf), [[Code]](https://github.com/microsoft/PeCo)

- (arXiv 2021.11) Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast **High-Resolution Image Generation** from Vector-Quantized Codes, [[Paper]](https://arxiv.org/pdf/2111.12701.pdf), [[COde]](https://github.com/samb-t/unleashing-transformers)

- (arXiv 2021.11) Towards Tokenized **Human Dynamics** Representation, [[Paper]](https://arxiv.org/pdf/2111.11433.pdf), [[Code]](https://github.com/likenneth/acton)

- (arXiv 2021.11) **Self-slimmed** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.12624.pdf)

- (arXiv 2021.11) VIOLET: End-to-End **Video-Language** Transformers with Masked Visual-token Modeling, [[Paper]](https://arxiv.org/pdf/2111.12681.pdf), [[Code]](https://github.com/tsujuifu/pytorch_violet)

- (arXiv 2021.11) A Lightweight Graph Transformer Network for **Human Mesh Reconstruction** from 2D Human Pose, [[Paper]](https://arxiv.org/pdf/2111.12696.pdf)

- (arXiv 2021.11) MorphMLP: A Self-Attention Free, **MLP**-Like Backbone for Image and Video, [[Paper]](https://arxiv.org/pdf/2111.12527.pdf)

- (arXiv 2021.11) Octree Transformer: Autoregressive **3D Shape Generation** on Hierarchically Structured Sequences, [[Paper]](https://arxiv.org/pdf/2111.12480.pdf)

- (arXiv 2021.11) Hierarchical Modular Network for **Video Captioning**, [[Paper]](https://arxiv.org/pdf/2111.12476.pdf)

- (arXiv 2021.11) NU¨WA: **Visual Synthesis Pre-training** for Neural visUal World creAtion, [[Paper]](https://arxiv.org/pdf/2111.12417.pdf), [[Code]](https://github.com/microsoft/NUWA)

- (arXiv 2021.11) An Image Patch is a Wave: Phase-Aware Vision **MLP**, [[Paper]](https://arxiv.org/pdf/2111.12294.pdf)

- (arXiv 2021.11) PTQ4ViT: Post-Training **Quantization** Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12293.pdf)

- (arXiv 2021.11) PU-Transformer: **Point Cloud Upsampling** Transformer, [[Paper]](https://arxiv.org/pdf/2111.12242.pdf)

- (arXiv 2021.11) Scaling Up **Vision-Language Pre-training** for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2111.12233.pdf)

- (arXiv 2021.11) Cerberus Transformer: Joint **Semantic, Affordance and Attribute Parsing**, [[Paper]](https://arxiv.org/pdf/2111.12608.pdf), [[Code]](https://github.com/OPEN-AIR-SUN/Cerberus)

- (arXiv 2021.11) Efficient **Video** Transformers with Spatial-Temporal Token Selection, [[Paper]](https://arxiv.org/pdf/2111.11591.pdf)

- (arXiv 2021.11) RedCaps: Web-curated **image-text data** created by the people, for the people, [[Paper]](https://arxiv.org/pdf/2111.11431.pdf), [[Project]](https://redcaps.xyz/)

- (arXiv 2021.11) EMScore: Evaluating **Video Captioning** via Coarse-Grained and Fine-Grained Embedding Matching, [[Paper]](https://arxiv.org/pdf/2111.08919.pdf), [[Code]](https://github.com/ShiYaya/emscore)

- (arXiv 2021.11) Compositional Transformers for **Scene Generation**, [[Paper]](https://arxiv.org/pdf/2111.08960.pdf), [[Code]](https://github.com/dorarad/gansformer)

- (arXiv 2021.11) Vis-TOP: Visual Transformer **Overlay Processor**, [[Paper]](https://arxiv.org/pdf/2110.10957.pdf)

- (arXiv 2021.11) **Grounded Situation Recognition** with Transformers, [[Paper]](https://arxiv.org/pdf/2111.10135.pdf), [[Code]](https://github.com/jhcho99/gsrtr)

- (arXiv 2021.11) Rethinking **Query, Key, and Value** Embedding in Vision Transformer under **Tiny Model** Constraints, [[Paper]](https://arxiv.org/pdf/2111.10017.pdf)

- (arXiv 2021.11) UFO: A UniFied TransfOrmer for **Vision-Language** Representation Learning, [[Paper]](https://arxiv.org/pdf/2111.10023.pdf)

- (arXiv 2021.11) Advancing High-Resolution **Video-Language** Representation with Large-Scale Video Transcriptions, [[Paper]](https://arxiv.org/pdf/2111.10337.pdf)

- (arXiv 2021.11) Combined Scaling for **Zero-shot Transfer Learning**, [[Paper]](https://arxiv.org/pdf/2111.10050.pdf)

- (arXiv 2021.11) Simple but Effective: **CLIP** Embeddings for **Embodied AI**, [[Paper]](https://arxiv.org/pdf/2111.09888.pdf)

- (arXiv 2021.11) Improved **Robustness** of Vision Transformer via PreLayerNorm in Patch Embedding, [[Paper]](https://arxiv.org/pdf/2111.08413.pdf)

- (arXiv 2021.11) IBOT: **IMAGE BERT PRE-TRAINING** WITH ONLINE TOKENIZER, [[Paper]](https://arxiv.org/pdf/2111.07832.pdf), [[Code]](https://github.com/bytedance/ibot)

- (arXiv 2021.11) **Masked Autoencoders** Are Scalable Vision Learners, [[Paper]](https://arxiv.org/pdf/2111.06377.pdf)

- (arXiv 2021.11) Mask-guided Spectral-wise Transformer for Efficient **Hyperspectral Image Reconstruction**, [[Paper]](https://arxiv.org/pdf/2111.07910.pdf)

- (arXiv 2021.11) Are Transformers More **Robust** Than CNNs?, [[Paper]](https://arxiv.org/pdf/2111.05464.pdf), [[Code]](https://github.com/ytongbai/ViTs-vs-CNNs)

- (arXiv 2021.11) CLIP2TV: An Empirical Study on Transformer-based Methods for **Video-Text Retrieval**, [[Paper]](https://arxiv.org/pdf/2111.05610.pdf)

- (arXiv 2021.11) Multimodal Transformer with Variable-length Memory for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2111.05759.pdf)

- (arXiv 2021.11) Improving **Visual Quality** of **Image Synthesis** by A Token-based Generator with Transformers, [[Paper]](https://arxiv.org/abs/2111.03481)

- (arXiv 2021.11) VLMO: Unified **Vision-Language** Pre-Training with Mixture-of-Modality-Experts, [[Paper]](https://arxiv.org/pdf/2111.02358.pdf), [[Code]](https://aka.ms/vlmo)

- (arXiv 2021.11) LAION-400M: Open **Dataset** of **CLIP**-Filtered 400 Million **Image-Text** Pairs, [[Paper]](https://arxiv.org/pdf/2111.02114.pdf), [[Project]](https://laion.ai/laion-400-open-dataset/)

- (arXiv 2021.11) An Empirical Study of **Training** End-to-End **Vision-and-Language** Transformers, [[Paper]](https://arxiv.org/pdf/2111.02387.pdf), [[Code]](https://github.com/zdou0830/METER)

- (arXiv 2021.11) CAN VISION TRANSFORMERS PERFORM **CONVOLUTION**? [[Paper]](https://arxiv.org/pdf/2111.01353.pdf)

- (arXiv 2021.11) HRViT: **Multi-Scale High-Resolution** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.01236.pdf)

### 2021.10

- (arXiv 2021.10) **Visual Keyword Spotting** with Attention, [[Paper]](https://arxiv.org/pdf/2110.15957.pdf), [[Project]](Visual Keyword Spotting with Attention)

- (arXiv 2021.10) Learning **Co-segmentation** by Segment Swapping for Retrieval and Discovery, [[Paper]](https://arxiv.org/pdf/2110.15904.pdf), [[Data & Code]](http://imagine.enpc.fr/~shenx/SegSwap/)

- (arXiv 2021.10) Visual Spatio-Temporal Relation-Enhanced Network for Cross-Modal **Text-Video Retrieval**, [[Paper]](https://arxiv.org/pdf/2110.15609.pdf), [[Code]](https://https//github.com/Lionel-Hing/VSR-Net)

- (arXiv 2021.10) Dispensed Transformer Network for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2110.14944.pdf)

- (arXiv 2021.10) Scatterbrain: Unifying **Sparse** and **Low-rank Attention** Approximation, [[Paper]](https://arxiv.org/pdf/2110.15343.pdf)

- (arXiv 2021.10) **3D Object Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2110.14921.pdf), [[Code]](https://github.com/3bobo/lttr)

- (arXiv 2021.10) Blending **Anti-Aliasing** into Vision Transformer, [[Paper]](https://arxiv.org/pdf/2110.15156.pdf), [[Code]](https://github.com/amazon-research/anti-aliasing-transformer)

- (arXiv 2021.10) UltraPose: **Synthesizing** Dense Pose with 1 Billion Points by **Human-body** Decoupling **3D** Model, [[Paper]](https://arxiv.org/pdf/2110.15267.pdf), [[Data & Code]](https://github.com/MomoAILab/ultrapose)

- (arXiv 2021.10) SOAT: A Scene- and Object-Aware Transformer for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2110.14143.pdf)

- (arXiv 2021.10) Bangla Image **Caption Generation** through CNN-Transformer based Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2110.12442.pdf)

- (arXiv 2021.10) History Aware Multimodal Transformer for **Vision-and-Language Navigation**, [[Paper]](https://arxiv.org/pdf/2110.13309.pdf), [[Project]](https://cshizhe.github.io/projects/vln_hamt.html)

- (arXiv 2021.10) TriBERT: Full-body Human-centric Audio-visual Representation Learning for **Visual Sound Separation**, [[Paper]](https://arxiv.org/pdf/2110.13412.pdf)

- (arXiv 2021.10) TNTC: TWO-STREAM NETWORK WITH TRANSFORMER-BASED COMPLEMENTARITY FOR GAIT-BASED **EMOTION RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2110.13708.pdf)

- (arXiv 2021.10) Contextual Similarity Aggregation with Self-attention for **Visual Re-ranking**, [[Paper]](https://arxiv.org/pdf/2110.13430.pdf), [[Code]](https://github.com/MCC-WH/CSA)

- (arXiv 2021.10) IIP-Transformer: Intra-Inter-Part Transformer for **Skeleton-Based Action Recognition**, [[Paper]](https://arxiv.org/pdf/2110.13385.pdf), [[Code]](https://github.com/qtwang0035/IIP-Transformer)

- (arXiv 2021.10) IMAGE-BASED **CLIP**-GUIDED ESSENCE TRANSFER, [[Paper]](https://arxiv.org/pdf/2110.12427.pdf), [[Code]](https://github.com/hila-chefer/TargetCLIP)

- (arXiv 2021.10) Sinkformers: Transformers with Doubly Stochastic **Attention**, [[Paper]](https://arxiv.org/pdf/2110.11773.pdf)

- (arXiv 2021.10) ILLITERATE **DALL·E** LEARNS TO COMPOSE, [[Paper]](https://arxiv.org/pdf/2110.11405.pdf), [[Project]](https://sites.google.com/view/slate-autoencoder), [[Code]](https://github.com/singhgautam/slate)

- (arXiv 2021.10) Learning Text-Image Joint Embedding for Efficient **Cross-Modal Retrieval** with Deep Feature Engineering, [[Paper]](https://arxiv.org/pdf/2110.11592.pdf)

- (arXiv 2021.10) SOFT: Softmax-free Transformer with **Linear Complexity**, [[Paper]](https://arxiv.org/pdf/2110.11945.pdf), [[Code]](https://fudan-zvg.github.io/SOFT)

- (arXiv 2021.10) Deep Two-Stream Video Inference for Human Body **Pose** and **Shape Estimation**, [[Paper]](https://arxiv.org/pdf/2110.11680.pdf)

- (arXiv 2021.10) TRANSFORMER **ACCELERATION** WITH DYNAMIC SPARSE ATTENTION, [[Paper]](https://arxiv.org/pdf/2110.11299.pdf)

- (arXiv 2021.10) CLOOB: MODERN **HOPFIELD** NETWORKS WITH INFOLOOB OUTPERFORM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11316.pdf), [[Code]](https://github.com/ml-jku/cloob)

- (arXiv 2021.10) Integrating Visuospatial, Linguistic and Commonsense Structure into **Story Visualization**, [[Paper]](https://arxiv.org/pdf/2110.10834.pdf)

- (arXiv 2021.10) StructFormer: Learning Spatial Structure for **Language-Guided** Semantic **Rearrangement** of Novel Objects, [[Paper]](https://arxiv.org/pdf/2110.10189.pdf), [[Project]](https://sites.google.com/view/structformer)

- (arXiv 2021.10) Gophormer: Ego-**Graph** Transformer for **Node Classification**, [[Paper]](https://arxiv.org/pdf/2110.13094.pdf)

- (arXiv 2021.10) STRANSGAN: AN EMPIRICAL STUDY ON TRANSFORMER IN **GANS**, [[Paper]](https://arxiv.org/pdf/2110.13107.pdf), [[Code]](https://nbei.github.io/stransgan.html)

- (arXiv 2021.10) MVT: Multi-view Vision Transformer for **3D Object Recognition**, [[Paper]](https://arxiv.org/pdf/2110.13083.pdf)

- (arXiv 2021.10) DocTr: **Document Image** Transformer for Geometric Unwarping and Illumination Correction, [[Paper]](https://arxiv.org/pdf/2110.12942.pdf), [[Code]](https://github.com/fh2019ustc/DocTr)

- (arXiv 2021.10) Bangla Image **Caption** Generation through CNN-Transformer based Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2110.12442.pdf)

- (arXiv 2021.10) WAV2CLIP: LEARNING ROBUST **AUDIO REPRESENTATIONS** FROM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11499.pdf), [[Code]](https://github.com/descriptinc/lyrebird-wav2clip)

- (arXiv 2021.10) AFTer-UNet: Axial Fusion Transformer UNet for **Medical Image Segmentation**, [[Paper]](https://arxiv.org/pdf/2110.10403.pdf)

- (arXiv 2021.10) CLOOB: MODERN HOPFIELD NETWORKS WITH INFOLOOB OUTPERFORM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11316.pdf), [[Code]](https://github.com/ml-jku/cloob)

- (arXiv 2021.10) AniFormer: Data-driven **3D Animation** with Transformer, [[Paper]](https://arxiv.org/pdf/2110.10533.pdf), [[Code]](https://github.com/mikecheninoulu/AniFormer)

- (arXiv 2021.10) **Few-Shot Temporal Action Localization** with Query Adaptive Transformer, [[Paper]](https://arxiv.org/pdf/2110.10552.pdf), [[Code]](https://github.com/sauradip/fewshotQAT)

- (arXiv 2021.10) 3D-ANAS v2: Grafting Transformer Module on Automatically Designed ConvNet for **Hyperspectral Image Classification**, [[Paper]](https://arxiv.org/pdf/2110.11084.pdf), [[Code]](https://github.com/xmm/3D-ANAS-V2)

- (arXiv 2021.10) CMTR: Cross-modality Transformer for Visible-infrared **Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2110.08994.pdf)

- (arXiv 2021.10) 3D-RETR: End-to-End **Single and Multi-View 3D Reconstruction** with Transformers, [[Paper]](https://arxiv.org/pdf/2110.08861.pdf), [[Code]](https://github.com/FomalhautB/3D-RETR)

- (arXiv 2021.10) HRFormer: **High-Resolution** Transformer for **Dense Prediction**, [[Paper]](https://arxiv.org/pdf/2110.09408.pdf), [[Code]](https://github.com/HRNet/HRFormer)

- (arXiv 2021.10) Leveraging MoCap Data for **Human Mesh Recovery**, [[Paper]](https://arxiv.org/pdf/2110.09243.pdf)

- (arXiv 2021.10) A Good **Prompt** Is Worth Millions of Parameters? Low-resource Prompt-based Learning for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2110.08484.pdf)

- (arXiv 2021.10) ASFormer: Transformer for **Action Segmentation**, [[Paper]](https://arxiv.org/pdf/2110.08568.pdf), [[Code]](https://github.com/ChinaYi/ASFormer)

- (arXiv 2021.10) Multimodal **Dialogue Response Generation**, [[Paper]](https://arxiv.org/pdf/2110.08515.pdf)

- (arXiv 2021.10) Understanding **Procedural Knowledge** by Sequencing Multimodal Instructional Manuals, [[Paper]](https://arxiv.org/pdf/2110.08486.pdf)

- (arXiv 2021.10) COMPOSITIONAL **ATTENTION**: DISENTANGLING SEARCH AND RETRIEVAL, [[Paper]](https://arxiv.org/pdf/2110.09419.pdf), [[Code]](https://github.com/sarthmit/Compositional-Attention)

- (arXiv 2021.10) Spatial-Temporal Transformer for 3D **Point Cloud Sequences**, [[Paper]](https://arxiv.org/pdf/2110.09783.pdf)

- (arXiv 2021.10) TransFusion: Cross-view Fusion with Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2110.09554.pdf), [[Code]](https://github.com/HowieMa/TransFusion-Pose)

- (arXiv 2021.10) Unifying Multimodal Transformer for **Bi-directional Image and Text Generation**, [[Paper]](https://arxiv.org/pdf/2110.09753.pdf)

- (arXiv 2021.10) Transformer with a Mixture of **Gaussian Keys**, [[Paper]](https://arxiv.org/pdf/2110.08678.pdf)

- (arXiv 2021.10) DIFFUSIONCLIP: **TEXT-GUIDED IMAGE MANIPULATION** USING DIFFUSION MODELS, [[Paper]](https://arxiv.org/pdf/2110.02711.pdf)

- (arXiv 2021.10) Adversarial **Robustness** Comparison of Vision Transformer and MLP-Mixer to CNNs, [[Paper]](https://arxiv.org/pdf/2110.02797.pdf), [[Code]](https://github.com/phibenz/robustness_comparison_vit_mlp-mixer_cnn)

- (arXiv 2021.10) RIPPLE ATTENTION FOR VISUAL PERCEPTION WITH **SUB-QUADRATIC COMPLEXITY**, [[Paper]](https://arxiv.org/pdf/2110.02453.pdf)

- (arXiv 2021.10) Certified Patch **Robustness** via Smoothed Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.07719.pdf), [[Code]](https://github.com/MadryLab/smoothed-vit)

- (arXiv 2021.10) CLIP-Forge: Towards Zero-Shot **Text-to-Shape** Generation, [[Paper]](https://arxiv.org/pdf/2110.02624.pdf)

- (arXiv 2021.10) Understanding and Improving **Robustness** of Vision Transformers through Patch-based Negative Augmentation, [[Paper]](https://arxiv.org/pdf/2110.07858.pdf)

- (arXiv 2021.10) SPARSE MOES MEET **EFFICIENT ENSEMBLES**, [[Paper]](https://arxiv.org/pdf/2110.03360.pdf)

- (arXiv 2021.10) Shared **Visual Representations** of Drawing for Communication: How do different **biases** affect human interpretability and intent? [[Paper]](https://arxiv.org/pdf/2110.08203.pdf)

- (arXiv 2021.10) SignBERT: Pre-Training of Hand-Model-Aware Representation for **Sign Language Recognition**, [[Paper]](https://arxiv.org/pdf/2110.05382.pdf)

- (arXiv 2021.10) Revitalizing CNN Attentions via Transformers in **Self-Supervised** Visual Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.05340.pdf)

- (arXiv 2021.10) Investigating **Transfer Learning Capabilities** of Vision Transformers and CNNs by Fine-Tuning a Single Trainable Block, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2110/2110.05270.pdf)

- (arXiv 2021.10) SUPERVISION EXISTS EVERYWHERE: A DATA EFFICIENT CONTRASTIVE **LANGUAGE-IMAGE** PRE-TRAINING PARADIGM, [[Paper]](https://arxiv.org/pdf/2110.05208.pdf), [[Code]](https://github.com/Sense-GVT/)

- (arXiv 2021.10) CLIP4Caption ++: Multi-CLIP for **Video Caption**, [[Paper]](https://arxiv.org/pdf/2110.05204.pdf)

- (arXiv 2021.10) Transformer-based Dual Relation Graph for **Multi-label Image Recognition**, [[Paper]](https://arxiv.org/pdf/2110.04722.pdf)

- (arXiv 2021.10) VECTOR-QUANTIZED **IMAGE MODELING** WITH IMPROVED VQGAN, [[Paper]](https://arxiv.org/pdf/2110.04627.pdf)

- (arXiv 2021.10) Adaptively Multi-view and Temporal Fusing Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2110.05092.pdf), [[Code]](https://github.com/lelexx/MTF-Transformer)

- (arXiv 2021.10) NVIT: VISION TRANSFORMER **COMPRESSION** AND **PARAMETER REDISTRIBUTION**, [[Paper]](https://arxiv.org/pdf/2110.04869.pdf)

- (arXiv 2021.10) 6D-ViT: Category-Level **6D Object Pose Estimation** via Transformer-based Instance Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.04792.pdf)

- (arXiv 2021.10) CLIP-Adapter: Better **Vision-Language** Models with Feature Adapters, [[Paper]](https://arxiv.org/pdf/2110.04544.pdf), [[Code]](https://github.com/gaopengcuhk/CLIP-Adapter)

- (arXiv 2021.10) ATISS: Autoregressive Transformers for **Indoor Scene Synthesis**, [[Paper]](https://arxiv.org/pdf/2110.03675.pdf), [[Code]](https://nv-tlabs.github.io/ATISS)
， 
- (arXiv 2021.10) MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND **MOBILE**-FRIENDLY VISION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2110.02178.pdf)

- (arXiv 2021.10) **TOKEN POOLING** IN VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.03860.pdf)

- (arXiv 2021.10) VIDT: AN EFFICIENT AND EFFECTIVE FULLY TRANSFORMER-BASED **OBJECT DETECTOR**, [[Paper]](https://arxiv.org/pdf/2110.03921.pdf), [[Code]](https://github.com/naver-ai/vidt)

- (arXiv 2021.10) CLIP4Caption: CLIP for **Video Caption**, [[Paper]](https://arxiv.org/pdf/2110.06615.pdf)

- (arXiv 2021.10) **OBJECT**-REGION **VIDEO** TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.06915.pdf), [[Code]](https://roeiherz.github.io/ORViT/)

- (arXiv 2021.10) LEVERAGING **REDUNDANCY** IN ATTENTION WITH REUSE TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.06821.pdf)

- (arXiv 2021.10) **Dynamic Inference** with Neural Interpreters, [[Paper]](https://arxiv.org/pdf/2110.06399.pdf)

- (arXiv 2021.10) A CLIP-Enhanced Method for **Video-Language** Understanding, [[Paper]](https://arxiv.org/pdf/2110.07137.pdf)

- (arXiv 2021.10) **Visual Relationship Detection** Using Part-and-Sum Transformers with Composite Queries, [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Dong_Visual_Relationship_Detection_Using_Part-and-Sum_Transformers_With_Composite_Queries_ICCV_2021_paper.pdf)

- (arXiv 2021.10) Discovering Human **Interactions** with Large-Vocabulary Objects via Query and Multi-Scale Detection, [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Discovering_Human_Interactions_With_Large-Vocabulary_Objects_via_Query_and_Multi-Scale_ICCV_2021_paper.pdf)

- (arXiv 2021.10) Learning Structural Representations for **Recipe Generation** and **Food Retrieval**, [[Paper]](https://arxiv.org/pdf/2110.01209.pdf)

- (arXiv 2021.10) A FREE LUNCH FROM VIT: ADAPTIVE ATTENTION MULTI-SCALE FUSION TRANSFORMER FOR **FINE-GRAINED VISUAL RECOGNITION**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2110/2110.01240.pdf)

### 2021.09
- (arXiv 2021.09) Joint Multimedia **Event Extraction** from Video and Article, [[Paper]](https://arxiv.org/pdf/2109.12776.pdf)

- (arXiv 2021.09) Long-Range Transformers for **Dynamic Spatiotemporal Forecasting**, [[Paper]](https://arxiv.org/pdf/2109.12218.pdf)

- (arXiv 2021.09) **Visually Grounded Concept** Composition, [[Paper]](https://arxiv.org/pdf/2109.14115.pdf)

- (arXiv 2021.09) CoSeg: Cognitively Inspired Unsupervised Generic **Event Segmentation**, [[Paper]](https://arxiv.org/pdf/2109.15170.pdf)

- (arXiv 2021.09) CCTrans: Simplifying and Improving **Crowd Counting** with Transformer, [[Paper]](https://arxiv.org/pdf/2109.14483.pdf)

- (arXiv 2021.09) UFO-ViT: High Performance **Linear** Vision Transformer **without Softmax**, [[Paper]](https://arxiv.org/pdf/2109.14382.pdf)

- (arXiv 2021.09) **Infrared Small-Dim Target Detection** with Transformer under Complex Backgrounds, [[Paper]](https://arxiv.org/pdf/2109.14379.pdf)

- (arXiv 2021.09) **Localizing Objects** with Self-Supervised Transformers and no Labels, [[Paper]](https://arxiv.org/pdf/2109.14279.pdf), [[Code]](https://github.com/valeoai/LOST)

- (arXiv 2021.09) Geometry-Entangled Visual Semantic Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2109.14137.pdf)

- (arXiv 2021.09) VideoCLIP: Contrastive Pre-training for **Zero-shot Video-Text Understanding**, [[Paper]](https://arxiv.org/pdf/2109.14084.pdf), [[Code]](https://github.com/pytorch/fairseq/examples/MMPT)

- (arXiv 2021.09) Fine-tuning Vision Transformers for the Prediction of **State Variables in Ising Models**, [[Paper]](https://arxiv.org/pdf/2109.13925.pdf)

- (arXiv 2021.09) CLIP-It! Language-Guided **Video Summarization**, [[Paper]](https://arxiv.org/pdf/2107.00650.pdf), [[Project]](https://medhini.github.io/clip_it)

- (arXiv 2021.09) MFEVIT: A ROBUST LIGHTWEIGHT TRANSFORMER-BASED NETWORK FOR MULTIMODAL 2D+3D **FACIAL EXPRESSION RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2109.13086.pdf)

- (arXiv 2021.09) Sparse Spatial Transformers for **Few-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2109.12932.pdf), [[Code]](https://github.com/chenhaoxing/SSFormers)

- (arXiv 2021.09) Vision Transformer Hashing for **Image Retrieval**, [[Paper]](https://arxiv.org/pdf/2109.12564.pdf)

- (arXiv 2021.09) PETA: **Photo Albums Event Recognition** using Transformers Attention, [[Paper]](https://arxiv.org/pdf/2109.12499.pdf)

- (arXiv 2021.09) MLIM: **VISION-AND-LANGUAGE** MODEL PRE-TRAINING WITH MASKED LANGUAGE AND IMAGE MODELING, [[Paper]](https://arxiv.org/pdf/2109.12178.pdf)

- (arXiv 2021.09) Dense Contrastive **Visual-Linguistic** Pretraining, [[Paper]](https://arxiv.org/pdf/2109.11778.pdf)

- (arXiv 2021.09) CPT: COLORFUL **PROMPT TUNING** FOR PRE-TRAINED VISION-LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2109.11797.pdf)

- (arXiv 2021.09) Localizing ∞-shaped fishes: **Sketch-guided object localization** in the wild, [[Paper]](https://arxiv.org/pdf/2109.11874.pdf), [[Code]](https://github.com/priba/sgol_wild)

- (arXiv 2021.09) CLIPORT: What and Where Pathways for **Robotic Manipulation**, [[Paper]](https://arxiv.org/pdf/2109.12098.pdf), [[Project]](https://cliport.github.io/), [[Code]](https://github.com/cliport/cliport)

- (arXiv 2021.09) GraFormer: Graph Convolution Transformer for **3D Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2109.08364.pdf), [[Code]](https://github.com/Graformer/GraFormer)

- (arXiv 2021.09) Multimodal Incremental Transformer with Visual Grounding for **Visual Dialogue Generation**, [[Paper]](https://arxiv.org/pdf/2109.08478.pdf)

- (arXiv 2021.09) Expression Snippet Transformer for Robust Video-based **Facial Expression Recognition**, [[Paper]](https://arxiv.org/pdf/2109.08409.pdf), [[Code]](https://anonymous.4open.science/r/ATSE-C58B)

- (arXiv 2021.09) LOTR: **Face Landmark Localization** Using Localization Transformer, [[Paper]](https://arxiv.org/pdf/2109.10057.pdf)

- (arXiv 2021.09) Dyadformer: A **Multi-modal** Transformer for Long-Range Modeling of Dyadic Interactions, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2109/2109.09487.pdf)

- (arXiv 2021.09) SDTP: Semantic-aware Decoupled Transformer Pyramid for **Dense Image Prediction**, [[Paper]](https://arxiv.org/pdf/2109.08963.pdf)

- (arXiv 2021.09) KD-VLP: Improving End-to-End **Vision-and-Language Pretraining** with Object Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2109.10504.pdf)

- (arXiv 2021.09) T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression, [[Paper]](https://arxiv.org/pdf/2109.10948.pdf)

- (arXiv 2021.09) OH-Former: Omni-Relational High-Order Transformer for **Person Re-Identification**, [[Paper]](https://arxiv.org/pdf/2109.11159.pdf)

- (arXiv 2021.09) PIX2SEQ: A LANGUAGE MODELING FRAMEWORK FOR **OBJECT DETECTION**, [[Paper]](https://arxiv.org/pdf/2109.10852.pdf)

- (arXiv 2021.09) ActionCLIP: A New Paradigm for **Video Action Recognition**, [[Paper]](https://arxiv.org/pdf/2109.08472.pdf)

- (arXiv 2021.09) BGT-Net: Bidirectional GRU Transformer Network for **Scene Graph Generation**, [[Paper]](https://arxiv.org/pdf/2109.05346.pdf)

- (arXiv 2021.09) Neural Human Performer: Learning Generalizable Radiance Fields for **Human Performance Rendering**, [[Paper]](https://arxiv.org/pdf/2109.07448.pdf), [[Code]](https://youngjoongunc.github.io/nhp/)

- (arXiv 2021.09) **Anchor DETR**: Query Design for Transformer-Based Detector, [[Paper]](https://arxiv.org/pdf/2109.07107.pdf), [[Code]](https://github.com/megvii-model/AnchorDETR)

- (arXiv 2021.09) An End-to-End Transformer Model for **3D Object Detection**, [[Paper]](https://arxiv.org/pdf/2109.08141.pdf), [[Code]](https://facebookresearch.github.io/3detr)

- (arXiv 2021.09) Hybrid Local-Global Transformer for **Image Dehazing**, [[Paper]](https://arxiv.org/pdf/2109.07100.pdf)

- (arXiv 2021.09) Semi-Supervised Wide-Angle **Portraits Correction** by Multi-Scale Transformer, [[Paper]](https://arxiv.org/pdf/2109.08024.pdf)

- (arXiv 2021.09) Label-Attention Transformer with Geometrically Coherent Objects for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2109.07799.pdf)

- (arXiv 2021.09) Pose Transformers (POTR): **Human Motion Prediction** with Non-Autoregressive Transformers, [[Paper]](https://arxiv.org/pdf/2109.07531.pdf), [[Code]](https://github.com/idiap/potr)

- (arXiv 2021.09) PnP-DETR: Towards **Efficient** Visual Analysis with Transformers, [[Paper]](https://arxiv.org/pdf/2109.07036.pdf), [[Code]](https://github.com/twangnh/pnp-detr)

- (arXiv 2021.09) Learning to **Ground** Visual Objects for Visual Dialog, [[Paper]](https://arxiv.org/pdf/2109.06013.pdf)

- (arXiv 2021.09) On Pursuit of Designing Multi-modal Transformer for **Video Grounding**, [[Paper]](https://arxiv.org/pdf/2109.06085.pdf), [[Code]](https://sites.google.com/view/mengcao/publication/gtr)

- (arXiv 2021.09) CDTrans: Cross-domain Transformer for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2109.06165.pdf)

- (arXiv 2021.09) IS ATTENTION BETTER THAN **MATRIX DECOMPOSITION**? [[Paper]](https://arxiv.org/pdf/2109.04553.pdf), [[Code]](https://github.com/Gsunshine/Enjoy-Hamburger)

- (arXiv 2021.09) Temporal Pyramid Transformer with Multimodal Interaction for **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2109.04735.pdf)

- (arXiv 2021.09) Line as a Visual Sentence: Context-aware **Line Descriptor** for Visual Localization, [[Paper]](https://arxiv.org/pdf/2109.04753.pdf)

- (arXiv 2021.09) Negative Sample Matters: A Renaissance of Metric Learning for **Temporal Grounding**, [[Paper]](https://arxiv.org/pdf/2109.04872.pdf)

- (arXiv 2021.09) LAViTeR: Learning Aligned **Visual and Textual** Representations Assisted by Image and Caption Generation, [[Paper]](https://arxiv.org/pdf/2109.04993.pdf), [[Code]](https://github.com/mshaikh2/LaViTeR)

- (arXiv 2021.09) Panoptic Narrative **Grounding**, [[Paper]](https://arxiv.org/pdf/2109.04988.pdf)

- (arXiv 2021.09) An Empirical Study of GPT-3 for Few-Shot Knowledge-Based **VQA**, [[Paper]](https://arxiv.org/pdf/2109.05014.pdf)

- (arXiv 2021.09) PlaTe: **Visually-Grounded Planning** with Transformers in Procedural Tasks, [[Paper]](https://arxiv.org/pdf/2109.04869.pdf), [[Project]](https://www.pair.toronto.edu/plate-planner/)

- (arXiv 2021.09) **EfficientCLIP**: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling, [[Paper]](https://arxiv.org/pdf/2109.04699.pdf)

- (arXiv 2021.09) **Scaled ReLU** Matters for **Training** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.03810.pdf)

- (arXiv 2021.09) FuseFormer: Fusing Fine-Grained Information in Transformers for **Video Inpainting**, [[Paper]](https://arxiv.org/pdf/2109.02974.pdf), [[Code]](https://github.com/ruiliu-ai/FuseFormer)

- (arXiv 2021.09) GCsT: Graph Convolutional Skeleton Transformer for **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2109.02860.pdf)

- (arXiv 2021.09) WHYACT: Identifying **Action Reasons** in Lifestyle **Vlogs**, [[Paper]](https://arxiv.org/pdf/2109.02747.pdf)

- (arXiv 2021.09) Zero-Shot **Open Set Detection** by Extending **CLIP**, [[Paper]](https://arxiv.org/pdf/2109.02748.pdf)

- (arXiv 2021.09) Towards Transferable **Adversarial Attacks** on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.04176.pdf)

- (arXiv 2021.09) Learning to **Prompt** for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2109.01134), [[Code]](https://github.com/KaiyangZhou/CoOp)

- (arXiv 2021.09) Improving **Video-Text Retrieval** by Multi-Stream Corpus Alignment and Dual Softmax Loss, [[Paper]](https://arxiv.org/pdf/2109.04290.pdf), [[Code]](https://github.com/starmemda/CAMoW/)

- (arXiv 2021.09) UCTransNet: Rethinking the **Skip Connections in U-Net** from a Channel-wise Perspective with Transformer, [[Paper]](https://arxiv.org/pdf/2109.04335.pdf), [[Code]](https://github.com/McGregorWwww/UCTransNet)

- (arXiv 2021.09) ConvMLP: Hierarchical Convolutional **MLPs** for Vision, [[Paper]](https://arxiv.org/pdf/2109.04454.pdf), [[Code]](https://github.com/SHI-Labs/Convolutional-MLPs)

- (arXiv 2021.09) TxT: **Crossmodal** End-to-End Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2109.04422.pdf)

- (arXiv 2021.09) Vision-and-Language or Vision-for-Language? On **Cross-Modal Influence** in Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2109.04448.pdf)

- (arXiv 2021.09) **Sparse**-MLP: A Fully-**MLP** Architecture with Conditional Computation, [[Paper]](https://arxiv.org/pdf/2109.02008.pdf)

- (arXiv 2021.09) SORNet: Spatial Object-Centric Representations for Sequential **Manipulation**, [[Paper]](https://arxiv.org/pdf/2109.03891.pdf), [[Project]](https://wentaoyuan.github.io/sornet)

- (arXiv 2021.09) Audio-Visual Transformer Based **Crowd Counting**, [[Paper]](https://arxiv.org/pdf/2109.01926.pdf)

- (arXiv 2021.09) Weakly Supervised Relative Spatial Reasoning for **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2109.01934.pdf), [[Code]](https://github.com/pratyay-banerjee/weak_sup_vqa)

- (arXiv 2021.09) FUSFORMER: A TRANSFORMER-BASED FUSION APPROACH FOR HYPERSPECTRAL IMAGE **SUPER-RESOLUTION**, [[Paper]](https://arxiv.org/pdf/2109.02079.pdf)

- (arXiv 2021.09) CTRL-C: **Camera calibration** TRansformer with Line-Classification, [[Paper]](https://arxiv.org/pdf/2109.02259.pdf), [[Code]](https://github.com/jwlee-vcl/CTRL-C)

- (arXiv 2021.09) Learning to Generate **Scene Graph** from Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2109.02227.pdf), [[Code]](https://github.com/YiwuZhong/SGG_from_NLS)

- (arXiv 2021.09) The Animation Transformer: Visual **Correspondence** via Segment Matching, [[Paper]](https://arxiv.org/pdf/2109.02614.pdf)

- (arXiv 2021.09) Voxel Transformer for **3D Object Detection**, [[Paper]](https://arxiv.org/pdf/2109.02497.pdf)

- (ICCV 2021.09) **3D Human Texture Estimation** from a Single Image with Transformers, [[Paper]](http://personal.ie.cuhk.edu.hk/~ccloy/files/iccv_2021_texformer.pdf), [[Code]](https://github.com/xuxy09/Texformer)

- (arXiv 2021.09) Encoder-decoder with Multi-level Attention for **3D Human Shape and Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2109.02303.pdf), [[Code]](https://github.com/ziniuwan/maed)

- (arXiv 2021.09) Joint Graph Learning and Matching for **Semantic Feature Correspondence**, [[Paper]](https://arxiv.org/pdf/2109.00240.pdf)

- (arXiv 2021.09) Searching for **Efficient** Multi-Stage Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.00642.pdf), [[Code]](https://github.com/yilunliao/vit-search)

### 2021.08
- (arXiv 2021.08) SIGN: Spatial-information Incorporated Generative Network for **Generalized Zero-shot Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2108.12517.pdf)

- (arXiv 2021.08) GroupFormer: **Group Activity Recognition** with Clustered Spatial-Temporal Transformer, [[Paper]](https://arxiv.org/pdf/2108.12630.pdf), [[Code]](https://github.com/xueyee/GroupFormer)

- (arXiv 2021.08) **A Battle of Network Structures**: An Empirical Study of CNN, Transformer, and MLP, [[Paper]](https://arxiv.org/pdf/2108.13002.pdf)

- (arXiv 2021.08) Exploring and Improving **Mobile** Level Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.13015.pdf)

- (arXiv 2021.08) Cross-category **Video Highlight Detection** via Set-based Learning, [[Paper]](https://arxiv.org/pdf/2108.11770.pdf), [[Code]](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)

- (arXiv 2021.08) Shifted Chunk Transformer for **Spatio-Temporal** Representational Learning, [[Paper]](https://arxiv.org/pdf/2108.11575.pdf)

- (arXiv 2021.08) SASRA: Semantically-aware Spatio-temporal Reasoning Agent for **Vision-and-Language Navigation** in Continuous Environments, [[Paper]](https://arxiv.org/pdf/2108.11945.pdf)

- (arXiv 2021.08) LocTex: Learning **Data-Efficient** Visual **Representations** from Localized Textual Supervision, [[Paper]](https://arxiv.org/pdf/2108.11950.pdf), [[Project]](https://loctex.mit.edu/)

- (arXiv 2021.08) Guiding Query Position and Performing Similar Attention for Transformer-Based **Detection** Heads, [[Paper]](https://arxiv.org/pdf/2108.09691.pdf)

- (arXiv 2021.08) SIMVLM: SIMPLE **VISUAL LANGUAGE** MODEL PRETRAINING WITH WEAK SUPERVISION, [[Paper]](https://arxiv.org/pdf/2108.10904.pdf)

- (arXiv 2021.08) TransFER: Learning Relation-aware **Facial Expression Representations** with Transformers, [[Paper]](https://
