# Transformer-in-Vision
Recent Transformer-based CV and related works. Welcome to comment/contribute!

Keep updated.

## Resource

- Attention is all you need, [[Paper]](https://arxiv.org/pdf/1706.03762.pdf)

- OpenAI CLIP [[Page]](https://openai.com/blog/clip/), [[Paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf), [[Code]](https://github.com/openai/CLIP), [[arXiv]](https://arxiv.org/pdf/2103.00020.pdf)

- OpenAI DALL·E [[Page]](https://openai.com/blog/dall-e/), [[Code]](https://github.com/openai/DALL-E), [[Paper]](https://arxiv.org/pdf/2102.12092.pdf)

- [huggingface/transformers](https://github.com/huggingface/transformers)

- [Kyubyong/transformer](https://github.com/Kyubyong/transformer), TF

- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch), Torch

- [krasserm/fairseq-image-captioning](https://github.com/krasserm/fairseq-image-captioning)

- [PyTorch Transformers Tutorials](https://github.com/abhimishra91/transformers-tutorials)

- [ictnlp/awesome-transformer](https://github.com/ictnlp/awesome-transformer)

- [basicv8vc/awesome-transformer](https://github.com/basicv8vc/awesome-transformer)

- [dk-liang/Awesome-Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer)

- [yuewang-cuhk/awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)

## Survey

- (arXiv 2021.09) Survey: Transformer based Video-Language Pre-training, [[Paper]](https://arxiv.org/pdf/2109.09920.pdf)

- (arXiv 2021.06) A Survey of Transformers, [[Paper]](https://arxiv.org/pdf/2106.04554.pdf)

- (arXiv 2021.06) Attention mechanisms and deep learning for machine vision: A survey of the state of the art, [[Paper]](https://arxiv.org/pdf/2106.07550.pdf)

- (arXiv 2021.06) Pre-Trained Models: Past, Present and Future, [[Paper]](https://arxiv.org/pdf/2106.07139.pdf)

- (arXiv 2021.05) Can Attention Enable MLPs To Catch Up With CNNs? [[Paper]](https://arxiv.org/pdf/2105.15078.pdf)

- (arXiv 2021.03) A Practical Survey on Faster and Lighter Transformers, [[Paper]](https://arxiv.org/pdf/2103.14636.pdf)

- (arXiv 2021.03) Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision, [[Paper]](https://arxiv.org/pdf/2103.04037.pdf)

- (arXiv 2021.01) A Survey on Visual Transformer, [[Paper]](https://arxiv.org/pdf/2012.12556.pdf)

- (arXiv 2020.9) Efficient Transformers: A Survey, [[Paper]](https://arxiv.org/pdf/2009.06732.pdf)

- (arXiv 2020.1) Transformers in Vision: A Survey, [[Paper]](https://arxiv.org/pdf/2101.01169.pdf)

## Recent Papers
<!-- ### 2021.11 -->
<!-- - (arXiv 2021.11) , [[Paper]]() -->

- (arXiv 2021.11) VLMO: Unified **Vision-Language** Pre-Training with Mixture-of-Modality-Experts, [[Paper]](https://arxiv.org/pdf/2111.02358.pdf), [[Code]](https://aka.ms/vlmo)

- (arXiv 2021.11) LAION-400M: Open **Dataset** of **CLIP**-Filtered 400 Million **Image-Text** Pairs, [[Paper]](https://arxiv.org/pdf/2111.02114.pdf), [[Project]](https://laion.ai/laion-400-open-dataset/)

- (arXiv 2021.11) An Empirical Study of **Training** End-to-End **Vision-and-Language** Transformers, [[Paper]](https://arxiv.org/pdf/2111.02387.pdf), [[Code]](https://github.com/zdou0830/METER)

- (arXiv 2021.11) CAN VISION TRANSFORMERS PERFORM **CONVOLUTION**? [[Paper]](https://arxiv.org/pdf/2111.01353.pdf)

- (arXiv 2021.11) HRViT: **Multi-Scale High-Resolution** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.01236.pdf)

### 2021.10
- (arXiv 2021.10) Gophormer: Ego-**Graph** Transformer for **Node Classification**, [[Paper]](https://arxiv.org/pdf/2110.13094.pdf)

- (arXiv 2021.10) STRANSGAN: AN EMPIRICAL STUDY ON TRANSFORMER IN **GANS**, [[Paper]](https://arxiv.org/pdf/2110.13107.pdf), [[Code]](https://nbei.github.io/stransgan.html)

- (arXiv 2021.10) MVT: Multi-view Vision Transformer for **3D Object Recognition**, [[Paper]](https://arxiv.org/pdf/2110.13083.pdf)

- (arXiv 2021.10) DocTr: **Document Image** Transformer for Geometric Unwarping and Illumination Correction, [[Paper]](https://arxiv.org/pdf/2110.12942.pdf), [[Code]](https://github.com/fh2019ustc/DocTr)

- (arXiv 2021.10) Bangla Image **Caption** Generation through CNN-Transformer based Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2110.12442.pdf)

- (arXiv 2021.10) WAV2CLIP: LEARNING ROBUST **AUDIO REPRESENTATIONS** FROM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11499.pdf), [[Code]](https://github.com/descriptinc/lyrebird-wav2clip)

- (arXiv 2021.10) AFTer-UNet: Axial Fusion Transformer UNet for **Medical Image Segmentation**, [[Paper]](https://arxiv.org/pdf/2110.10403.pdf)

- (arXiv 2021.10) CLOOB: MODERN HOPFIELD NETWORKS WITH INFOLOOB OUTPERFORM **CLIP**, [[Paper]](https://arxiv.org/pdf/2110.11316.pdf), [[Code]](https://github.com/ml-jku/cloob)

- (arXiv 2021.10) AniFormer: Data-driven **3D Animation** with Transformer, [[Paper]](https://arxiv.org/pdf/2110.10533.pdf), [[Code]](https://github.com/mikecheninoulu/AniFormer)

- (arXiv 2021.10) **Few-Shot Temporal Action Localization** with Query Adaptive Transformer, [[Paper]](https://arxiv.org/pdf/2110.10552.pdf), [[Code]](https://github.com/sauradip/fewshotQAT)

- (arXiv 2021.10) 3D-ANAS v2: Grafting Transformer Module on Automatically Designed ConvNet for **Hyperspectral Image Classification**, [[Paper]](https://arxiv.org/pdf/2110.11084.pdf), [[Code]](https://github.com/xmm/3D-ANAS-V2)

- (arXiv 2021.10) CMTR: Cross-modality Transformer for Visible-infrared **Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2110.08994.pdf)

- (arXiv 2021.10) 3D-RETR: End-to-End **Single and Multi-View 3D Reconstruction** with Transformers, [[Paper]](https://arxiv.org/pdf/2110.08861.pdf), [[Code]](https://github.com/FomalhautB/3D-RETR)

- (arXiv 2021.10) HRFormer: **High-Resolution** Transformer for **Dense Prediction**, [[Paper]](https://arxiv.org/pdf/2110.09408.pdf), [[Code]](https://github.com/HRNet/HRFormer)

- (arXiv 2021.10) Leveraging MoCap Data for **Human Mesh Recovery**, [[Paper]](https://arxiv.org/pdf/2110.09243.pdf)

- (arXiv 2021.10) A Good **Prompt** Is Worth Millions of Parameters? Low-resource Prompt-based Learning for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2110.08484.pdf)

- (arXiv 2021.10) ASFormer: Transformer for **Action Segmentation**, [[Paper]](https://arxiv.org/pdf/2110.08568.pdf), [[Code]](https://github.com/ChinaYi/ASFormer)

- (arXiv 2021.10) Multimodal **Dialogue Response Generation**, [[Paper]](https://arxiv.org/pdf/2110.08515.pdf)

- (arXiv 2021.10) Understanding **Procedural Knowledge** by Sequencing Multimodal Instructional Manuals, [[Paper]](https://arxiv.org/pdf/2110.08486.pdf)

- (arXiv 2021.10) COMPOSITIONAL **ATTENTION**: DISENTANGLING SEARCH AND RETRIEVAL, [[Paper]](https://arxiv.org/pdf/2110.09419.pdf), [[Code]](https://github.com/sarthmit/Compositional-Attention)

- (arXiv 2021.10) Spatial-Temporal Transformer for 3D **Point Cloud Sequences**, [[Paper]](https://arxiv.org/pdf/2110.09783.pdf)

- (arXiv 2021.10) TransFusion: Cross-view Fusion with Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2110.09554.pdf), [[Code]](https://github.com/HowieMa/TransFusion-Pose)

- (arXiv 2021.10) Unifying Multimodal Transformer for **Bi-directional Image and Text Generation**, [[Paper]](https://arxiv.org/pdf/2110.09753.pdf)

- (arXiv 2021.10) Transformer with a Mixture of **Gaussian Keys**, [[Paper]](https://arxiv.org/pdf/2110.08678.pdf)

- (arXiv 2021.10) DIFFUSIONCLIP: **TEXT-GUIDED IMAGE MANIPULATION** USING DIFFUSION MODELS, [[Paper]](https://arxiv.org/pdf/2110.02711.pdf)

- (arXiv 2021.10) Adversarial **Robustness** Comparison of Vision Transformer and MLP-Mixer to CNNs, [[Paper]](https://arxiv.org/pdf/2110.02797.pdf), [[Code]](https://github.com/phibenz/robustness_comparison_vit_mlp-mixer_cnn)

- (arXiv 2021.10) RIPPLE ATTENTION FOR VISUAL PERCEPTION WITH **SUB-QUADRATIC COMPLEXITY**, [[Paper]](https://arxiv.org/pdf/2110.02453.pdf)

- (arXiv 2021.10) Certified Patch **Robustness** via Smoothed Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.07719.pdf), [[Code]](https://github.com/MadryLab/smoothed-vit)

- (arXiv 2021.10) CLIP-Forge: Towards Zero-Shot **Text-to-Shape** Generation, [[Paper]](https://arxiv.org/pdf/2110.02624.pdf)

- (arXiv 2021.10) Understanding and Improving **Robustness** of Vision Transformers through Patch-based Negative Augmentation, [[Paper]](https://arxiv.org/pdf/2110.07858.pdf)

- (arXiv 2021.10) SPARSE MOES MEET **EFFICIENT ENSEMBLES**, [[Paper]](https://arxiv.org/pdf/2110.03360.pdf)

- (arXiv 2021.10) Shared **Visual Representations** of Drawing for Communication: How do different **biases** affect human interpretability and intent? [[Paper]](https://arxiv.org/pdf/2110.08203.pdf)

- (arXiv 2021.10) SignBERT: Pre-Training of Hand-Model-Aware Representation for **Sign Language Recognition**, [[Paper]](https://arxiv.org/pdf/2110.05382.pdf)

- (arXiv 2021.10) Revitalizing CNN Attentions via Transformers in **Self-Supervised** Visual Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.05340.pdf)

- (arXiv 2021.10) Investigating **Transfer Learning Capabilities** of Vision Transformers and CNNs by Fine-Tuning a Single Trainable Block, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2110/2110.05270.pdf)

- (arXiv 2021.10) SUPERVISION EXISTS EVERYWHERE: A DATA EFFICIENT CONTRASTIVE **LANGUAGE-IMAGE** PRE-TRAINING PARADIGM, [[Paper]](https://arxiv.org/pdf/2110.05208.pdf), [[Code]](https://github.com/Sense-GVT/)

- (arXiv 2021.10) CLIP4Caption ++: Multi-CLIP for **Video Caption**, [[Paper]](https://arxiv.org/pdf/2110.05204.pdf)

- (arXiv 2021.10) Transformer-based Dual Relation Graph for **Multi-label Image Recognition**, [[Paper]](https://arxiv.org/pdf/2110.04722.pdf)

- (arXiv 2021.10) VECTOR-QUANTIZED **IMAGE MODELING** WITH IMPROVED VQGAN, [[Paper]](https://arxiv.org/pdf/2110.04627.pdf)

- (arXiv 2021.10) Adaptively Multi-view and Temporal Fusing Transformer for **3D Human Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2110.05092.pdf), [[Code]](https://github.com/lelexx/MTF-Transformer)

- (arXiv 2021.10) NVIT: VISION TRANSFORMER **COMPRESSION** AND **PARAMETER REDISTRIBUTION**, [[Paper]](https://arxiv.org/pdf/2110.04869.pdf)

- (arXiv 2021.10) 6D-ViT: Category-Level **6D Object Pose Estimation** via Transformer-based Instance Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.04792.pdf)

- (arXiv 2021.10) CLIP-Adapter: Better **Vision-Language** Models with Feature Adapters, [[Paper]](https://arxiv.org/pdf/2110.04544.pdf), [[Code]](https://github.com/gaopengcuhk/CLIP-Adapter)

- (arXiv 2021.10) ATISS: Autoregressive Transformers for **Indoor Scene Synthesis**, [[Paper]](https://arxiv.org/pdf/2110.03675.pdf), [[Code]](https://nv-tlabs.github.io/ATISS)
， 
- (arXiv 2021.10) MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND **MOBILE**-FRIENDLY VISION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2110.02178.pdf)

- (arXiv 2021.10) **TOKEN POOLING** IN VISION TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.03860.pdf)

- (arXiv 2021.10) VIDT: AN EFFICIENT AND EFFECTIVE FULLY TRANSFORMER-BASED **OBJECT DETECTOR**, [[Paper]](https://arxiv.org/pdf/2110.03921.pdf), [[Code]](https://github.com/naver-ai/vidt)

- (arXiv 2021.10) CLIP4Caption: CLIP for **Video Caption**, [[Paper]](https://arxiv.org/pdf/2110.06615.pdf)

- (arXiv 2021.10) **OBJECT**-REGION **VIDEO** TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.06915.pdf), [[Code]](https://roeiherz.github.io/ORViT/)

- (arXiv 2021.10) LEVERAGING **REDUNDANCY** IN ATTENTION WITH REUSE TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2110.06821.pdf)

- (arXiv 2021.10) **Dynamic Inference** with Neural Interpreters, [[Paper]](https://arxiv.org/pdf/2110.06399.pdf)

- (arXiv 2021.10) A CLIP-Enhanced Method for **Video-Language** Understanding, [[Paper]](https://arxiv.org/pdf/2110.07137.pdf)

- (arXiv 2021.10) **Visual Relationship Detection** Using Part-and-Sum Transformers with Composite Queries, [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Dong_Visual_Relationship_Detection_Using_Part-and-Sum_Transformers_With_Composite_Queries_ICCV_2021_paper.pdf)

- (arXiv 2021.10) Discovering Human **Interactions** with Large-Vocabulary Objects via Query and Multi-Scale Detection, [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Discovering_Human_Interactions_With_Large-Vocabulary_Objects_via_Query_and_Multi-Scale_ICCV_2021_paper.pdf)

- (arXiv 2021.10) Learning Structural Representations for **Recipe Generation** and **Food Retrieval**, [[Paper]](https://arxiv.org/pdf/2110.01209.pdf)

- (arXiv 2021.10) A FREE LUNCH FROM VIT: ADAPTIVE ATTENTION MULTI-SCALE FUSION TRANSFORMER FOR **FINE-GRAINED VISUAL RECOGNITION**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2110/2110.01240.pdf)

### 2021.09
- (arXiv 2021.09) Joint Multimedia **Event Extraction** from Video and Article, [[Paper]](https://arxiv.org/pdf/2109.12776.pdf)

- (arXiv 2021.09) Long-Range Transformers for **Dynamic Spatiotemporal Forecasting**, [[Paper]](https://arxiv.org/pdf/2109.12218.pdf)

- (arXiv 2021.09) **Visually Grounded Concept** Composition, [[Paper]](https://arxiv.org/pdf/2109.14115.pdf)

- (arXiv 2021.09) CoSeg: Cognitively Inspired Unsupervised Generic **Event Segmentation**, [[Paper]](https://arxiv.org/pdf/2109.15170.pdf)

- (arXiv 2021.09) CCTrans: Simplifying and Improving **Crowd Counting** with Transformer, [[Paper]](https://arxiv.org/pdf/2109.14483.pdf)

- (arXiv 2021.09) UFO-ViT: High Performance **Linear** Vision Transformer **without Softmax**, [[Paper]](https://arxiv.org/pdf/2109.14382.pdf)

- (arXiv 2021.09) **Infrared Small-Dim Target Detection** with Transformer under Complex Backgrounds, [[Paper]](https://arxiv.org/pdf/2109.14379.pdf)

- (arXiv 2021.09) **Localizing Objects** with Self-Supervised Transformers and no Labels, [[Paper]](https://arxiv.org/pdf/2109.14279.pdf), [[Code]](https://github.com/valeoai/LOST)

- (arXiv 2021.09) Geometry-Entangled Visual Semantic Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2109.14137.pdf)

- (arXiv 2021.09) VideoCLIP: Contrastive Pre-training for **Zero-shot Video-Text Understanding**, [[Paper]](https://arxiv.org/pdf/2109.14084.pdf), [[Code]](https://github.com/pytorch/fairseq/examples/MMPT)

- (arXiv 2021.09) Fine-tuning Vision Transformers for the Prediction of **State Variables in Ising Models**, [[Paper]](https://arxiv.org/pdf/2109.13925.pdf)

- (arXiv 2021.09) CLIP-It! Language-Guided **Video Summarization**, [[Paper]](https://arxiv.org/pdf/2107.00650.pdf), [[Project]](https://medhini.github.io/clip_it)

- (arXiv 2021.09) MFEVIT: A ROBUST LIGHTWEIGHT TRANSFORMER-BASED NETWORK FOR MULTIMODAL 2D+3D **FACIAL EXPRESSION RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2109.13086.pdf)

- (arXiv 2021.09) Sparse Spatial Transformers for **Few-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2109.12932.pdf), [[Code]](https://github.com/chenhaoxing/SSFormers)

- (arXiv 2021.09) Vision Transformer Hashing for **Image Retrieval**, [[Paper]](https://arxiv.org/pdf/2109.12564.pdf)

- (arXiv 2021.09) PETA: **Photo Albums Event Recognition** using Transformers Attention, [[Paper]](https://arxiv.org/pdf/2109.12499.pdf)

- (arXiv 2021.09) MLIM: **VISION-AND-LANGUAGE** MODEL PRE-TRAINING WITH MASKED LANGUAGE AND IMAGE MODELING, [[Paper]](https://arxiv.org/pdf/2109.12178.pdf)

- (arXiv 2021.09) Dense Contrastive **Visual-Linguistic** Pretraining, [[Paper]](https://arxiv.org/pdf/2109.11778.pdf)

- (arXiv 2021.09) CPT: COLORFUL **PROMPT TUNING** FOR PRE-TRAINED VISION-LANGUAGE MODELS, [[Paper]](https://arxiv.org/pdf/2109.11797.pdf)

- (arXiv 2021.09) Localizing ∞-shaped fishes: **Sketch-guided object localization** in the wild, [[Paper]](https://arxiv.org/pdf/2109.11874.pdf), [[Code]](https://github.com/priba/sgol_wild)

- (arXiv 2021.09) CLIPORT: What and Where Pathways for **Robotic Manipulation**, [[Paper]](https://arxiv.org/pdf/2109.12098.pdf), [[Project]](https://cliport.github.io/), [[Code]](https://github.com/cliport/cliport)

- (arXiv 2021.09) GraFormer: Graph Convolution Transformer for **3D Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2109.08364.pdf), [[Code]](https://github.com/Graformer/GraFormer)

- (arXiv 2021.09) Multimodal Incremental Transformer with Visual Grounding for **Visual Dialogue Generation**, [[Paper]](https://arxiv.org/pdf/2109.08478.pdf)

- (arXiv 2021.09) Expression Snippet Transformer for Robust Video-based **Facial Expression Recognition**, [[Paper]](https://arxiv.org/pdf/2109.08409.pdf), [[Code]](https://anonymous.4open.science/r/ATSE-C58B)

- (arXiv 2021.09) LOTR: **Face Landmark Localization** Using Localization Transformer, [[Paper]](https://arxiv.org/pdf/2109.10057.pdf)

- (arXiv 2021.09) Dyadformer: A **Multi-modal** Transformer for Long-Range Modeling of Dyadic Interactions, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2109/2109.09487.pdf)

- (arXiv 2021.09) SDTP: Semantic-aware Decoupled Transformer Pyramid for **Dense Image Prediction**, [[Paper]](https://arxiv.org/pdf/2109.08963.pdf)

- (arXiv 2021.09) KD-VLP: Improving End-to-End **Vision-and-Language Pretraining** with Object Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2109.10504.pdf)

- (arXiv 2021.09) T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression, [[Paper]](https://arxiv.org/pdf/2109.10948.pdf)

- (arXiv 2021.09) OH-Former: Omni-Relational High-Order Transformer for **Person Re-Identification**, [[Paper]](https://arxiv.org/pdf/2109.11159.pdf)

- (arXiv 2021.09) PIX2SEQ: A LANGUAGE MODELING FRAMEWORK FOR **OBJECT DETECTION**, [[Paper]](https://arxiv.org/pdf/2109.10852.pdf)

- (arXiv 2021.09) ActionCLIP: A New Paradigm for **Video Action Recognition**, [[Paper]](https://arxiv.org/pdf/2109.08472.pdf)

- (arXiv 2021.09) BGT-Net: Bidirectional GRU Transformer Network for **Scene Graph Generation**, [[Paper]](https://arxiv.org/pdf/2109.05346.pdf)

- (arXiv 2021.09) Neural Human Performer: Learning Generalizable Radiance Fields for **Human Performance Rendering**, [[Paper]](https://arxiv.org/pdf/2109.07448.pdf), [[Code]](https://youngjoongunc.github.io/nhp/)

- (arXiv 2021.09) **Anchor DETR**: Query Design for Transformer-Based Detector, [[Paper]](https://arxiv.org/pdf/2109.07107.pdf), [[Code]](https://github.com/megvii-model/AnchorDETR)

- (arXiv 2021.09) An End-to-End Transformer Model for **3D Object Detection**, [[Paper]](https://arxiv.org/pdf/2109.08141.pdf), [[Code]](https://facebookresearch.github.io/3detr)

- (arXiv 2021.09) Hybrid Local-Global Transformer for **Image Dehazing**, [[Paper]](https://arxiv.org/pdf/2109.07100.pdf)

- (arXiv 2021.09) Semi-Supervised Wide-Angle **Portraits Correction** by Multi-Scale Transformer, [[Paper]](https://arxiv.org/pdf/2109.08024.pdf)

- (arXiv 2021.09) Label-Attention Transformer with Geometrically Coherent Objects for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2109.07799.pdf)

- (arXiv 2021.09) Pose Transformers (POTR): **Human Motion Prediction** with Non-Autoregressive Transformers, [[Paper]](https://arxiv.org/pdf/2109.07531.pdf), [[Code]](https://github.com/idiap/potr)

- (arXiv 2021.09) PnP-DETR: Towards **Efficient** Visual Analysis with Transformers, [[Paper]](https://arxiv.org/pdf/2109.07036.pdf), [[Code]](https://github.com/twangnh/pnp-detr)

- (arXiv 2021.09) Learning to **Ground** Visual Objects for Visual Dialog, [[Paper]](https://arxiv.org/pdf/2109.06013.pdf)

- (arXiv 2021.09) On Pursuit of Designing Multi-modal Transformer for **Video Grounding**, [[Paper]](https://arxiv.org/pdf/2109.06085.pdf), [[Code]](https://sites.google.com/view/mengcao/publication/gtr)

- (arXiv 2021.09) CDTrans: Cross-domain Transformer for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2109.06165.pdf)

- (arXiv 2021.09) IS ATTENTION BETTER THAN **MATRIX DECOMPOSITION**? [[Paper]](https://arxiv.org/pdf/2109.04553.pdf), [[Code]](https://github.com/Gsunshine/Enjoy-Hamburger)

- (arXiv 2021.09) Temporal Pyramid Transformer with Multimodal Interaction for **Video Question Answering**, [[Paper]](https://arxiv.org/pdf/2109.04735.pdf)

- (arXiv 2021.09) Line as a Visual Sentence: Context-aware **Line Descriptor** for Visual Localization, [[Paper]](https://arxiv.org/pdf/2109.04753.pdf)

- (arXiv 2021.09) Negative Sample Matters: A Renaissance of Metric Learning for **Temporal Grounding**, [[Paper]](https://arxiv.org/pdf/2109.04872.pdf)

- (arXiv 2021.09) LAViTeR: Learning Aligned **Visual and Textual** Representations Assisted by Image and Caption Generation, [[Paper]](https://arxiv.org/pdf/2109.04993.pdf), [[Code]](https://github.com/mshaikh2/LaViTeR)

- (arXiv 2021.09) Panoptic Narrative **Grounding**, [[Paper]](https://arxiv.org/pdf/2109.04988.pdf)

- (arXiv 2021.09) An Empirical Study of GPT-3 for Few-Shot Knowledge-Based **VQA**, [[Paper]](https://arxiv.org/pdf/2109.05014.pdf)

- (arXiv 2021.09) PlaTe: **Visually-Grounded Planning** with Transformers in Procedural Tasks, [[Paper]](https://arxiv.org/pdf/2109.04869.pdf), [[Project]](https://www.pair.toronto.edu/plate-planner/)

- (arXiv 2021.09) **EfficientCLIP**: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling, [[Paper]](https://arxiv.org/pdf/2109.04699.pdf)

- (arXiv 2021.09) **Scaled ReLU** Matters for **Training** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.03810.pdf)

- (arXiv 2021.09) FuseFormer: Fusing Fine-Grained Information in Transformers for **Video Inpainting**, [[Paper]](https://arxiv.org/pdf/2109.02974.pdf), [[Code]](https://github.com/ruiliu-ai/FuseFormer)

- (arXiv 2021.09) GCsT: Graph Convolutional Skeleton Transformer for **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2109.02860.pdf)

- (arXiv 2021.09) WHYACT: Identifying **Action Reasons** in Lifestyle **Vlogs**, [[Paper]](https://arxiv.org/pdf/2109.02747.pdf)

- (arXiv 2021.09) Zero-Shot **Open Set Detection** by Extending **CLIP**, [[Paper]](https://arxiv.org/pdf/2109.02748.pdf)

- (arXiv 2021.09) Towards Transferable **Adversarial Attacks** on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.04176.pdf)

- (arXiv 2021.09) Learning to **Prompt** for **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2109.01134), [[Code]](https://github.com/KaiyangZhou/CoOp)

- (arXiv 2021.09) Improving **Video-Text Retrieval** by Multi-Stream Corpus Alignment and Dual Softmax Loss, [[Paper]](https://arxiv.org/pdf/2109.04290.pdf), [[Code]](https://github.com/starmemda/CAMoW/)

- (arXiv 2021.09) UCTransNet: Rethinking the **Skip Connections in U-Net** from a Channel-wise Perspective with Transformer, [[Paper]](https://arxiv.org/pdf/2109.04335.pdf), [[Code]](https://github.com/McGregorWwww/UCTransNet)

- (arXiv 2021.09) ConvMLP: Hierarchical Convolutional **MLPs** for Vision, [[Paper]](https://arxiv.org/pdf/2109.04454.pdf), [[Code]](https://github.com/SHI-Labs/Convolutional-MLPs)

- (arXiv 2021.09) TxT: **Crossmodal** End-to-End Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2109.04422.pdf)

- (arXiv 2021.09) Vision-and-Language or Vision-for-Language? On **Cross-Modal Influence** in Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2109.04448.pdf)

- (arXiv 2021.09) **Sparse**-MLP: A Fully-**MLP** Architecture with Conditional Computation, [[Paper]](https://arxiv.org/pdf/2109.02008.pdf)

- (arXiv 2021.09) SORNet: Spatial Object-Centric Representations for Sequential **Manipulation**, [[Paper]](https://arxiv.org/pdf/2109.03891.pdf), [[Project]](https://wentaoyuan.github.io/sornet)

- (arXiv 2021.09) Audio-Visual Transformer Based **Crowd Counting**, [[Paper]](https://arxiv.org/pdf/2109.01926.pdf)

- (arXiv 2021.09) Weakly Supervised Relative Spatial Reasoning for **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2109.01934.pdf), [[Code]](https://github.com/pratyay-banerjee/weak_sup_vqa)

- (arXiv 2021.09) FUSFORMER: A TRANSFORMER-BASED FUSION APPROACH FOR HYPERSPECTRAL IMAGE **SUPER-RESOLUTION**, [[Paper]](https://arxiv.org/pdf/2109.02079.pdf)

- (arXiv 2021.09) CTRL-C: **Camera calibration** TRansformer with Line-Classification, [[Paper]](https://arxiv.org/pdf/2109.02259.pdf), [[Code]](https://github.com/jwlee-vcl/CTRL-C)

- (arXiv 2021.09) Learning to Generate **Scene Graph** from Natural Language Supervision, [[Paper]](https://arxiv.org/pdf/2109.02227.pdf), [[Code]](https://github.com/YiwuZhong/SGG_from_NLS)

- (arXiv 2021.09) The Animation Transformer: Visual **Correspondence** via Segment Matching, [[Paper]](https://arxiv.org/pdf/2109.02614.pdf)

- (arXiv 2021.09) Voxel Transformer for **3D Object Detection**, [[Paper]](https://arxiv.org/pdf/2109.02497.pdf)

- (ICCV 2021.09) **3D Human Texture Estimation** from a Single Image with Transformers, [[Paper]](http://personal.ie.cuhk.edu.hk/~ccloy/files/iccv_2021_texformer.pdf), [[Code]](https://github.com/xuxy09/Texformer)

- (arXiv 2021.09) Encoder-decoder with Multi-level Attention for **3D Human Shape and Pose Estimation**, [[Paper]](https://arxiv.org/pdf/2109.02303.pdf), [[Code]](https://github.com/ziniuwan/maed)

- (arXiv 2021.09) Joint Graph Learning and Matching for **Semantic Feature Correspondence**, [[Paper]](https://arxiv.org/pdf/2109.00240.pdf)

- (arXiv 2021.09) Searching for **Efficient** Multi-Stage Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.00642.pdf), [[Code]](https://github.com/yilunliao/vit-search)

### 2021.08
- (arXiv 2021.08) SIGN: Spatial-information Incorporated Generative Network for **Generalized Zero-shot Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2108.12517.pdf)

- (arXiv 2021.08) GroupFormer: **Group Activity Recognition** with Clustered Spatial-Temporal Transformer, [[Paper]](https://arxiv.org/pdf/2108.12630.pdf), [[Code]](https://github.com/xueyee/GroupFormer)

- (arXiv 2021.08) **A Battle of Network Structures**: An Empirical Study of CNN, Transformer, and MLP, [[Paper]](https://arxiv.org/pdf/2108.13002.pdf)

- (arXiv 2021.08) Exploring and Improving **Mobile** Level Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.13015.pdf)

- (arXiv 2021.08) Cross-category **Video Highlight Detection** via Set-based Learning, [[Paper]](https://arxiv.org/pdf/2108.11770.pdf), [[Code]](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)

- (arXiv 2021.08) Shifted Chunk Transformer for **Spatio-Temporal** Representational Learning, [[Paper]](https://arxiv.org/pdf/2108.11575.pdf)

- (arXiv 2021.08) SASRA: Semantically-aware Spatio-temporal Reasoning Agent for **Vision-and-Language Navigation** in Continuous Environments, [[Paper]](https://arxiv.org/pdf/2108.11945.pdf)

- (arXiv 2021.08) LocTex: Learning **Data-Efficient** Visual **Representations** from Localized Textual Supervision, [[Paper]](https://arxiv.org/pdf/2108.11950.pdf), [[Project]](https://loctex.mit.edu/)

- (arXiv 2021.08) Guiding Query Position and Performing Similar Attention for Transformer-Based **Detection** Heads, [[Paper]](https://arxiv.org/pdf/2108.09691.pdf)

- (arXiv 2021.08) SIMVLM: SIMPLE **VISUAL LANGUAGE** MODEL PRETRAINING WITH WEAK SUPERVISION, [[Paper]](https://arxiv.org/pdf/2108.10904.pdf)

- (arXiv 2021.08) TransFER: Learning Relation-aware **Facial Expression Representations** with Transformers, [[Paper]](https://arxiv.org/pdf/2108.11116.pdf)

- (arXiv 2021.08) Efficient Transformer for Single Image **Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2108.11084.pdf)

- (arXiv 2021.08) Discovering Spatial Relationships by Transformers for **Domain Generalization**, [[Paper]](https://arxiv.org/pdf/2108.10046.pdf)

- (arXiv 2021.08) TACo: Token-aware Cascade Contrastive Learning for **Video-Text Alignment**, [[Paper]](https://arxiv.org/pdf/2108.09980.pdf)

- (arXiv 2021.08) MM-ViT: Multi-Modal Video Transformer for **Compressed Video Action Recognition**, [[Paper]](https://arxiv.org/pdf/2108.09322.pdf)

- (arXiv 2021.08) SwinIR: **Image Restoration** Using Swin Transformer, [[Paper]](https://arxiv.org/pdf/2108.10257.pdf), [[Code]](https://github.com/JingyunLiang/SwinIR)

- (arXiv 2021.08) Grid-VLP: Revisiting Grid Features for **Vision-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2108.09479.pdf)

- (arXiv 2021.08) Improving **3D Object Detection** with Channel-wise Transformer, [[Paper]](https://arxiv.org/pdf/2108.10723.pdf)

- (arXiv 2021.08) No-Reference **Image Quality Assessment** via Transformers, Relative Ranking, and Self-Consistency, [[Paper]](https://arxiv.org/pdf/2108.06858.pdf), [[Code]](https://github.com/isalirezag/TReS)

- (arXiv 2021.08) SOTR: **Segmenting** Objects with Transformers, [[Paper]](https://arxiv.org/pdf/2108.06747.pdf), [[Code]](https://github.com/easton-cau/SOTR)

- (arXiv 2021.08) ROSITA: Enhancing **Vision-and-Language** Semantic Alignments via Cross- and Intra-modal Knowledge Integration, [[Paper]](https://arxiv.org/pdf/2108.07073.pdf), [[Code]](https://github.com/MILVLG/rosita)

- (arXiv 2021.08) Escaping the **Gradient Vanishing**: Periodic Alternatives of Softmax in Attention Mechanism, [[Paper]](https://arxiv.org/pdf/2108.07153.pdf), [[Code]](https://github.com/slwang9353/Period-alternatives-of-Softmax)

- (arXiv 2021.08) End-to-End Dense **Video Captioning** with Parallel Decoding, [[Paper]](https://arxiv.org/pdf/2108.07781.pdf), [[Code]](https://github.com/ttengwang/PDVC)

- (arXiv 2021.08) Trans4Trans: **Efficient** Transformer for Transparent Object and Semantic Scene Segmentation in Real-World **Navigation** Assistance, [[Paper]](https://arxiv.org/pdf/2108.09174.pdf)

- (arXiv 2021.08) **Video Relation Detection** via Tracklet based Visual Transformer, [[Paper]](https://arxiv.org/pdf/2108.08669.pdf), [[Code]](https://github.com/Dawn-LX/VidVRD-tracklets)

- (arXiv 2021.08) PoinTr: Diverse **Point Cloud Completion** with Geometry-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2108.08839.pdf), [[Code]](https://github.com/yuxumin/PoinTr)

- (arXiv 2021.08) ImageBART: Bidirectional Context with Multinomial Diffusion for **Autoregressive Image Synthesis**, [[Paper]](https://arxiv.org/pdf/2108.08827.pdf), [[Project]](https://compvis.github.io/imagebart/)

- (arXiv 2021.08) Do Vision Transformers **See Like Convolutional Neural Networks?** [[Paper]](https://arxiv.org/pdf/2108.08810.pdf)

- (arXiv 2021.08) TVT: Transferable Vision Transformer for **Unsupervised Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2108.05988.pdf)

- (arXiv 2021.08) MUSIQ: Multi-scale **Image Quality** Transformer, [[Paper]](https://arxiv.org/pdf/2108.05997.pdf)

- (arXiv 2021.08) **Point-Voxel** Transformer: An Efficient Approach To 3D Deep Learning, [[Paper]](https://arxiv.org/pdf/2108.06076.pdf), [[Code]](https://github.com/2020zhangcheng/PVT)

- (arXiv 2021.08) Conditional **DETR** for **Fast** Training Convergence, [[Paper]](https://arxiv.org/pdf/2108.06152.pdf), [[Code]](https://git.io/ConditionalDETR)

- (arXiv 2021.08) Vision-Language Transformer and Query Generation for **Referring Segmentation**, [[Paper]](https://arxiv.org/pdf/2108.05565.pdf), [[Code]](https://github.com/henghuiding/Vision-Language-Transformer)

- (arXiv 2021.08) Mobile-Former: Bridging **MobileNet** and Transformer, [[Paper]](https://arxiv.org/pdf/2108.05895.pdf)

- (arXiv 2021.08) **Multiview Detection** with Shadow Transformer (and View-Coherent Data Augmentation), [[Paper]](https://arxiv.org/pdf/2108.05888.pdf), [[Code]](https://github.com/hou-yz/MVDeTr)

- (arXiv 2021.08) **Billion-Scale Pretraining** with Vision Transformers for Multi-Task Visual Representations, [[Paper]](https://arxiv.org/pdf/2108.05887.pdf)

- (arXiv 2021.08) Embodied BERT: A Transformer Model for **Embodied**, Language-guided Visual Task Completion, [[Paper]](https://arxiv.org/pdf/2108.04927.pdf), [[Code]](https://github.com/amazon-research/embert)

- (arXiv 2021.08) Video Transformer for **Deepfake Detection** with Incremental Learning, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2108/2108.05307.pdf)

- (arXiv 2021.08) **ConvNets vs. Transformers**: Whose Visual Representations are More **Transferable**? [[Paper]](https://arxiv.org/pdf/2108.05305.pdf)

- (arXiv 2021.08) A Transformer-based Math Language Model for **Handwritten Math Expression Recognition**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2108/2108.05002.pdf)

- (arXiv 2021.08) Optimizing Latency for **Online Video Captioning** Using Audio-Visual Transformers, [[Paper]](https://arxiv.org/pdf/2108.02147.pdf)

- (arXiv 2021.08) TransRefer3D: Entity-and-Relation Aware Transformer for **Fine-Grained 3D Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2108.02388.pdf)

- (arXiv 2021.08) Fast **Convergence** of **DETR** with Spatially Modulated Co-Attention. [[Paper]](https://arxiv.org/pdf/2108.02404.pdf), [[Code]](https://github.com/gaopengcuhk/SMCA-DETR)

- (arXiv 2021.08) Token Shift Transformer for **Video Classification**, [[Paper]](https://arxiv.org/pdf/2108.02432.pdf), [[Code]](https://github.com/VideoNetworks/TokShift-Transformer)

- (arXiv 2021.08) Simpler is Better: **Few-shot Semantic Segmentation** with Classifier Weight Transformer, [[Paper]](https://arxiv.org/pdf/2108.03032.pdf), [[Code]](https://github.com/zhiheLu/CWT-for-FSS)

- (arXiv 2021.08) Joint Inductive and Transductive Learning for **Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2108.03679.pdf), [[Code]](https://github.com/maoyunyao/JOINT)

- (arXiv 2021.08) OVIS: **Open-Vocabulary Visual Instance Search** via Visual-Semantic Aligned Representation Learning, [[Paper]](https://arxiv.org/pdf/2108.03704.pdf)

- (arXiv 2021.08) Paint Transformer: Feed Forward Neural Painting with Stroke Prediction, [[Paper]](https://arxiv.org/pdf/2108.03798.pdf), [[Code-1]](https://github.com/PaddlePaddle/PaddleGAN), [[Code-2]](https://github.com/Huage001/PaintTransformer)

- (arXiv 2021.08) TransForensics: **Image Forgery Localization** with Dense Self-Attention, [[Paper]](https://arxiv.org/pdf/2108.03871.pdf)

- (arXiv 2021.08) TriTransNet: **RGB-D Salient Object Detection** with a Triplet Transformer Embedding Network, [[Paper]](https://arxiv.org/pdf/2108.03990.pdf)

- (arXiv 2021.08) **Image Retrieval** on Real-life Images with Pre-trained Vision-and-Language Models, [[Paper]](https://arxiv.org/pdf/2108.04024.pdf), [[Code]](https://cuberick-orion.github.io/CIRR/)

- (arXiv 2021.08) The Right to Talk: An **Audio-Visual** Transformer Approach, [[Paper]](https://arxiv.org/pdf/2108.03256.pdf)

- (arXiv 2021.08) PSViT: Better Vision Transformer via **Token Pooling** and **Attention Sharing**, [[Paper]](https://arxiv.org/pdf/2108.03428.pdf)

- (arXiv 2021.08) Unifying Global-Local Representations in **Salient Object Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2108.02759.pdf), [[Code]](https://github.com/OliverRensu/GLSTR)

- (arXiv 2021.08) Boosting **Few-shot Semantic Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2108.02266.pdf), [[Code]](https://github.com/GuoleiSun/TRFS)

- (arXiv 2021.08) Vision Transformer with **Progressive Sampling**, [[Paper]](https://arxiv.org/pdf/2108.01684.pdf), [[Code]](https://github.com/yuexy/PS-ViT)

- (arXiv 2021.08) Armour: **Generalizable Compact Self-Attention** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.01778.pdf)

- (arXiv 2021.08) Evo-ViT: Slow-Fast Token Evolution for **Dynamic** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2108.01390.pdf)

- (arXiv 2021.08) S^2-MLPV2: IMPROVED SPATIAL-SHIFT **MLP** ARCHITECTURE FOR VISION, [[Paper]](https://arxiv.org/pdf/2108.01072.pdf)

- (arXiv 2021.08) Congested **Crowd Instance Localization** with Dilated Convolutional Swin Transformer, [[Paper]](https://arxiv.org/pdf/2108.00584.pdf)

- (arXiv 2021.08) Multi-Head Self-Attention via Vision Transformer for **Zero-Shot Learning**, [[Paper]](https://arxiv.org/pdf/2108.00045.pdf)

- (arXiv 2021.08) CROSSFORMER: A VERSATILE VISION TRANSFORMER BASED ON **CROSS-SCALE ATTENTION**, [[Paper]](https://arxiv.org/pdf/2108.00154.pdf), [[Code]](https://github.com/cheerss/CrossFormer)

- (arXiv 2021.08) Word2Pix: Word to Pixel Cross Attention Transformer in **Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2108.00205.pdf)

- (arXiv 2021.08) Transformer-based deep **imitation learning** for dual-arm robot manipulation, [[Paper]](https://arxiv.org/pdf/2108.00385.pdf)

- (arXiv 2021.08) GTNet:Guided Transformer Network for Detecting **Human-Object Interactions**, [[Paper]](https://arxiv.org/pdf/2108.00596.pdf), [[Code]](https://github.com/UCSB-VRL/GTNet)

### 2021.07
- (arXiv 2021.07) Perceiver IO: A **General Architecture** for Structured Inputs & Outputs, [[Paper]](https://arxiv.org/pdf/2107.14795.pdf), [[Code]](https://dpmd.ai/perceiver-code)

- (arXiv 2021.07) DPT: **Deformable Patch**-based Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2107.14467.pdf), [[Code]](https://github.com/CASIA-IVA-Lab/DPT)

- (arXiv 2021.07) Product1M: Towards Weakly Supervised Instance-Level **Product Retrieval** via **Cross-modal Pretraining**, [[Paper]](https://arxiv.org/pdf/2107.14572.pdf)

- (arXiv 2021.07) Exceeding the Limits of **Visual-Linguistic Multi-Task Learning**, [[Paper]](https://arxiv.org/pdf/2107.13054.pdf)

- (arXiv 2021.07) UIBert: Learning Generic Multimodal Representations for **UI Understanding**, [[Paper]](https://arxiv.org/pdf/2107.13731.pdf)

- (arXiv 2021.07) Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for **Video Anomaly Detection**, [[Paper]](https://arxiv.org/pdf/2107.13720.pdf)

- (arXiv 2021.07) A Unified Efficient Pyramid Transformer for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2107.14209.pdf)

- (arXiv 2021.07) PPT Fusion: Pyramid Patch Transformer for a Case Study in **Image Fusion**, [[Paper]](https://arxiv.org/pdf/2107.13967.pdf)

- (arXiv 2021.07) ReFormer: The Relational Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2107.14178.pdf)

- (arXiv 2021.07) Rethinking and Improving **Relative Position Encoding** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2107.14222.pdf), [[Code]](https://github.com/microsoft/Cream/tree/main/iRPE)

- (arXiv 2021.07) **Statistically** Meaningful Approximation: a Case Study on **Approximating Turing Machines** with Transformers, [[Paper]](https://arxiv.org/pdf/2107.13163.pdf)

- (arXiv 2021.07) PlaneTR: Structure-Guided Transformers for **3D Plane Recovery**, [[Paper]](https://arxiv.org/pdf/2107.13108.pdf), [[Code]](https://git.io/PlaneTR)

- (arXiv 2021.07) Is Object Detection Necessary for **Human-Object Interaction** Recognition? [[Paper]](https://arxiv.org/pdf/2107.13083.pdf)

- (arXiv 2021.07) Exceeding the Limits of **Visual-Linguistic** Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2107.13054.pdf)

- (arXiv 2021.07) Don’t Sweep your Learning Rate under the Rug: A Closer Look at **Cross-modal Transfer** of Pretrained Transformers, [[Paper]](https://arxiv.org/pdf/2107.12460.pdf)

- (arXiv 2021.07) Exploring Sequence Feature Alignment for **Domain Adaptive Detection** Transformers, [[Paper]](https://arxiv.org/pdf/2107.12636.pdf), [[Code]](https://github.com/encounter1997/SFA)

- (arXiv 2021.07) Go **Wider** Instead of Deeper, [[Paper]](https://arxiv.org/pdf/2107.11817.pdf)

- (arXiv 2021.07) **Contextual** Transformer Networks for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2107.12292.pdf), [[Code]](https://github.com/JDAI-CV/CoTNet)

- (arXiv 2021.07) Mixed SIGNals: **Sign Language Production** via a Mixture of Motion Primitives, [[Paper]](https://arxiv.org/pdf/2107.11317.pdf)

- (arXiv 2021.07) Query2Label: A Simple Transformer Way to **Multi-Label Classification**, [[Paper]](https://arxiv.org/pdf/2107.10834.pdf), [[Code]](https://github.com/SlongLiu/query2labels)

- (arXiv 2021.07) EAN: Event Adaptive Network for Enhanced **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2107.10771.pdf), [[Code]](https://github.com/tianyuan168326/EAN-Pytorch)

- (arXiv 2021.07) CycleMLP: A **MLP**-like Architecture for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2107.10224.pdf), [[Code]](https://github.com/ShoufaChen/CycleMLP)

- (arXiv 2021.07) Generative **Video** Transformer: Can Objects be the Words? [[Paper]](https://arxiv.org/pdf/2107.09240.pdf)

- (arXiv 2021.07) QVHIGHLIGHTS: Detecting Moments and Highlights in **Videos** via **Natural Language Queries**, [[Paper]](https://arxiv.org/pdf/2107.09609.pdf), [[Code]](https://github.com/jayleicn/moment_detr)

- (arXiv 2021.07) PICASO: Permutation-Invariant Cascaded Attentional **Set Operator**, [[Paper]](https://arxiv.org/pdf/2107.08305.pdf), [[Code]](https://github.com/samzare/PICASO)

- (arXiv 2021.07) RAMS-Trans: Recurrent Attention Multi-scale Transformer for **Fine-grained Image Recognition**, [[Paper]](https://arxiv.org/pdf/2107.08192.pdf)

- (arXiv 2021.07) OODformer: **Out-Of-Distribution Detection** Transformer, [[Paper]](https://arxiv.org/pdf/2107.08976.pdf), [[Code]](https://github.com/rajatkoner08/oodformer)

- (arXiv 2021.07) **Image Fusion** Transformer, [[Paper]](https://arxiv.org/pdf/2107.09011.pdf), [[Code]](https://github.com/Vibashan/Image-Fusion-Transformer)

- (arXiv 2021.07) ResT: An Efficient Transformer for **Visual Recognition**, [[Paper]](https://arxiv.org/pdf/2105.13677.pdf), [[Code]](https://github.com/wofmanaf/ResT)

- (arXiv 2021.07) STAR: Sparse Transformer-based **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2107.07089.pdf), [[Code]](https://github.com/imj2185/STAR)

- (arXiv 2021.07) Transformer with Peak Suppression and Knowledge Guidance for **Fine-grained Image Recognition**, [[Paper]](https://arxiv.org/pdf/2107.06538.pdf)

- (arXiv 2021.07) How Much Can **CLIP** Benefit Vision-and-Language Tasks? [[Paper]](https://arxiv.org/pdf/2107.06383.pdf)

- (arXiv 2021.07) Locally **Enhanced Self-Attention**: Rethinking Self-Attention as Local and Context Terms, [[Paper]](https://arxiv.org/pdf/2107.05637.pdf), [[Code]](https://github.com/Chenglin-Yang/LESA)

- (arXiv 2021.07) Visual Parser: Representing **Part-whole Hierarchies** with Transformers, [[Paper]](https://arxiv.org/pdf/2107.05790.pdf), [[Code]](https://github.com/kevin-ssy/ViP)

- (arXiv 2021.07) Combiner: Full Attention Transformer with **Sparse** Computation Cost, [[Paper]](https://arxiv.org/pdf/2107.05768.pdf)

- (arXiv 2021.07) Per-Pixel Classification is Not All You Need for **Semantic Segmentation**, [[Paper]](https://arxiv.org/pdf/2107.06278.pdf), [[Project]](https://bowenc0221.github.io/maskformer)

- (arXiv 2021.07) Learning Multi-Scene **Absolute Pose Regression** with Transformers, [[Paper]](https://arxiv.org/pdf/2103.11468.pdf)

- (arXiv 2021.07) CMT: **Convolutional** Neural Networks Meet Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.06263.pdf)

- (arXiv 2021.07) HAT: Hierarchical Aggregation Transformers for **Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2107.05946.pdf), [[Code]](https://github.com/AI-Zhpp/HAT)

- (arXiv 2021.07) THE **BROWNIAN MOTION** IN THE TRANSFORMER MODEL, [[Paper]](https://arxiv.org/pdf/2107.05264.pdf)

- (arXiv 2021.07) Local-to-Global **Self-Attention** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.04735.pdf), [[Code]](https://github.com/ljpadam/LG-Transformer)

- (arXiv 2021.07) Scenes and Surroundings: **Scene Graph Generation** using Relation Transformer, [[Paper]](https://arxiv.org/pdf/2107.05448.pdf)

- (arXiv 2021.07) ViTGAN: Training **GANs** with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.04589.pdf)

- (arXiv 2021.07) Long-Short Temporal **Contrastive Learning** of **Video** Transformers, [[Paper]](https://arxiv.org/pdf/2106.09212.pdf)

- (arXiv 2021.07) PVTv2: Improved Baselines with **Pyramid** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.13797.pdf), [[Code]](https://github.com/whai362/PVT)

- (arXiv 2021.07) Learning **Vision-Guided Quadrupedal Locomotion** End-to-End with Cross-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2107.03996.pdf), [[Code]](https://rchalyang.github.io/LocoTransformer)

- (arXiv 2021.07) LanguageRefer: Spatial-Language Model for **3D Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2107.03438.pdf)

- (arXiv 2021.07) EEG-CONVTRANSFORMER FOR SINGLE-TRIAL **EEG** BASED VISUAL STIMULI CLASSIFICATION, [[Paper]](https://arxiv.org/pdf/2107.03983.pdf)

- (arXiv 2021.07) Feature Fusion Vision Transformer for **Fine-Grained Visual Categorization**, [[Paper]](https://arxiv.org/pdf/2107.02341.pdf)

- (arXiv 2021.07) Long-Short Transformer: **Efficient** Transformers for Language and Vision, [[Paper]](https://arxiv.org/pdf/2107.02192.pdf)

- (arXiv 2021.07) TransformerFusion: Monocular RGB **Scene Reconstruction** using Transformers, [[Paper]](https://arxiv.org/pdf/2107.02191.pdf)

- (arXiv 2021.07) VIDLANKD: Improving **Language Understanding** via Video-Distilled Knowledge Transfer, [[Paper]](https://arxiv.org/pdf/2107.02681.pdf), [[Code]](https://github.com/zinengtang/VidLanKD)

- (arXiv 2021.07) GLiT: Neural **Architecture Search** for Global and Local Image Transformer, [[Paper]](https://arxiv.org/pdf/2107.02960.pdf)

- (arXiv 2021.07) LEARNING VISION TRANSFORMER WITH SQUEEZE AND EXCITATION FOR FACIAL **EXPRESSION** RECOGNITION, [[Paper]](https://arxiv.org/pdf/2107.03107.pdf)

- (arXiv 2021.07) Trans4Trans: Efficient Transformer for Transparent Object **Segmentation** to Help Visually Impaired People Navigate in the Real World, [[Paper]](https://arxiv.org/pdf/2107.03172.pdf)

- (arXiv 2021.07) Long Short-Term Transformer for Online **Action Detection**, [[Paper]](https://arxiv.org/pdf/2107.03377.pdf)

- (arXiv 2021.07) VISION XFORMERS: **EFFICIENT** ATTENTION FOR IMAGE CLASSIFICATION, [[Paper]](https://arxiv.org/pdf/2107.02239.pdf)

- (arXiv 2021.07) Test-Time Personalization with a Transformer for Human **Pose** Estimation, [[Paper]](https://arxiv.org/pdf/2107.02133.pdf), [[Code]](https://liyz15.github.io/TTP/)

- (arXiv 2021.07) What Makes for **Hierarchical** Vision Transformer? [[Paper]](https://arxiv.org/pdf/2107.02174.pdf)

- (arXiv 2021.07) **Efficient** Vision Transformers via Fine-Grained Manifold Distillation, [[Paper]](https://arxiv.org/pdf/2107.01378.pdf)

- (arXiv 2021.07) **Visual Relationship Forecasting** in Videos, [[Paper]](https://arxiv.org/pdf/2107.01181.pdf)

- (arXiv 2021.07) Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service **Robots**, [[Paper]](https://arxiv.org/pdf/2107.00811.pdf)

- (arXiv 2021.07) Case Relation Transformer: A Crossmodal Language Generation Model for **Fetching Instructions**, [[Paper]](https://arxiv.org/pdf/2107.00789.pdf)

- (arXiv 2021.07) CSWin Transformer: A General Vision Transformer **Backbone** with Cross-Shaped Windows, [[Paper]](https://arxiv.org/pdf/2107.00652.pdf), [[Code]](https://github.com/microsoft/CSWin-Transformer.)

- (arXiv 2021.07) CLIP-It! Language-Guided **Video Summarization**, [[Paper]](https://arxiv.org/pdf/2107.00650.pdf), [[Code]](https://medhini.github.io/clip_it)

- (arXiv 2021.07) AutoFormer: Searching Transformers for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2107.00651.pdf), [[Code]](https://github.com/microsoft/AutoML)

- (arXiv 2021.07) Focal Self-**attention** for Local-Global Interactions in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.00641.pdf)

- (arXiv 2021.07) Global Filter Networks for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2107.00645.pdf), [[Code]](https://github.com/raoyongming/GFNet)

- (arXiv 2021.07) VideoLightFormer: Lightweight **Action Recognition** using Transformers, [[Paper]](https://arxiv.org/pdf/2107.00451.pdf)

- (arXiv 2021.07) OPT: Omni-Perception Pre-Trainer for **Cross-Modal** Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2107.00249.pdf)

- (arXiv 2021.07) TransSC: Transformer-based Shape Completion for **Grasp Evaluation**, [[Paper]](https://arxiv.org/pdf/2107.00511.pdf)

- (arXiv 2021.07) Action Transformer: A Self-Attention Model for Short-Time Human **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2107.00606.pdf)

### 2021.06
- (arXiv 2021.06) Associating Objects with Transformers for **Video Object Segmentation**, [[Paper]](https://arxiv.org/pdf/2106.02638.pdf), [[Code]](https://github.com/z-x-yang/AOT)

- (arXiv 2021.06) Video **Super-Resolution** Transformer, [[Paper]](https://arxiv.org/pdf/2106.06847.pdf), [[Code]](https://github.com/caojiezhang/VSR-Transformer)

- (arXiv 2021.06) **Thinking** Like Transformers, [[Paper]](https://arxiv.org/pdf/2106.06981.pdf)

- (arXiv 2021.06) **Kernel Identification** Through Transformers, [[Paper]](https://arxiv.org/pdf/2106.08185.pdf)

- (arXiv 2021.06) XCiT: **Cross-Covariance** Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.09681.pdf)

- (arXiv 2021.06) THUNDR: Transformer-based **3D HUmaN Reconstruction** with Markers, [[Paper]](https://arxiv.org/pdf/2106.09336.pdf)

- (arXiv 2021.06) Probing **Image–Language** Transformers for Verb Understanding, [[Paper]](https://arxiv.org/pdf/2106.09141.pdf)

- (arXiv 2021.06) How to **train** your ViT? Data, Augmentation, and Regularization in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.10270.pdf), [[Code]](https://github.com/google-research/vision_transformer), [[Model]](https://github.com/rwightman/pytorch-image-models)

- (arXiv 2021.06) End-to-end Temporal **Action Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2106.10271.pdf), [[Code]](https://github.com/xlliu7/TadTR)

- (arXiv 2021.06) **Efficient** Self-supervised Vision Transformers for Representation Learning, [[Paper]](https://arxiv.org/pdf/2106.09785.pdf)

- (arXiv 2021.06) CLIP2Video: Mastering Video-Text **Retrieval** via Image CLIP, [[Paper]](https://arxiv.org/pdf/2106.11097.pdf), [[Code]](https://github.com/CryhanFang/CLIP2Video)

- (arXiv 2021.06) Keeping Your Eye on the Ball: **Trajectory Attention** in Video Transformers, [[Paper]](https://arxiv.org/pdf/2106.05392.pdf), [[Code]](https://github.com/facebookresearch/Motionformer)

- (arXiv 2021.06) Transformed **ROIs** for Capturing Visual Transformations in Videos, [[Paper]](https://arxiv.org/pdf/2106.03162.pdf)

- (arXiv 2021.06) Transformer in **Convolutional** Neural Networks, [[Paper]](https://arxiv.org/pdf/2106.03180.pdf), [[Code]](https://github.com/yun-liu/TransCNN)

- (arXiv 2021.06) **Video Instance Segmentation** using Inter-Frame Communication Transformers, [[Paper]](https://arxiv.org/pdf/2106.03299.pdf)

- (arXiv 2021.06) Patch Slimming for **Efficient** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02852.pdf)

- (arXiv 2021.06) CAPE: Encoding Relative **Positions** with Continuous Augmented Positional Embeddings, [[Paper]](https://arxiv.org/pdf/2106.03143.pdf)

- (arXiv 2021.06) RegionViT: **Regional-to-Local Attention** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02689.pdf)

- (arXiv 2021.06) **Motion Planning** Transformers: One Model to Plan Them All, [[Paper]](https://arxiv.org/pdf/2106.02791.pdf)

- (arXiv 2021.06) Oriented Object **Detection** with Transformer, [[Paper]](https://arxiv.org/pdf/2106.03146.pdf)

- (arXiv 2021.06) Referring Transformer: A One-step Approach to Multi-task **Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2106.03089.pdf)

- (arXiv 2021.06) Grounding **inductive biases** in natural images: invariance stems from variations in data, [[Paper]](https://arxiv.org/pdf/2106.05121.pdf)

- (arXiv 2021.06) CoAtNet: Marrying **Convolution and Attention** for All Data Sizes, [[Paper]](https://arxiv.org/pdf/2106.04803.pdf)

- (arXiv 2021.06) **Scaling** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04560.pdf)

- (arXiv 2021.06) Uformer: A General U-Shaped Transformer for **Image Restoration**, [[Paper]](https://arxiv.org/pdf/2106.03106.pdf), [[Code]](https://github.com/ZhendongWang6/Uformer)

- (arXiv 2021.06) Visual Transformer for Task-aware **Active Learning**, [[Paper]](https://arxiv.org/pdf/2106.03801.pdf), [[Code]](https://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning)

- (arXiv 2021.06) Chasing **Sparsity** in Vision Transformers: An End-to-End Exploration, [[Paper]](https://arxiv.org/pdf/2106.04533.pdf), [[Code]](https://github.com/VITA-Group/SViTE)

- (arXiv 2021.06) DETReg: Unsupervised Pretraining with Region Priors for Object **Detection**, [[Paper]](https://arxiv.org/pdf/2106.04550.pdf), [[Code]](https://amirbar.net/detreg)

- (arXiv 2021.06) MVT: MASK VISION TRANSFORMER FOR FACIAL **EXPRESSION** RECOGNITION IN THE WILD, [[Paper]](https://arxiv.org/pdf/2106.04520.pdf)

- (arXiv 2021.06) **Demystifying** Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight, [[Paper]](https://arxiv.org/pdf/2106.04263.pdf)

- (arXiv 2021.06) Diverse Part Discovery: Occluded Person **Re-identification** with Part-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2106.04095.pdf)

- (arXiv 2021.06) MlTr: **Multi-label Classification** with Transformer, [[Paper]](https://arxiv.org/pdf/2106.06195.pdf), [[Code]](https://github.com/starmemda/MlTr/)

- (arXiv 2021.06) Going Beyond **Linear** Transformers with Recurrent Fast Weight Programmers, [[Paper]](https://arxiv.org/pdf/2106.06295.pdf), [[Code]](https://github.com/IDSIA/recurrent-fwp)

- (arXiv 2021.06) On Improving **Adversarial Transferability** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04169.pdf), [[Code]](https://git.io/JZmG3)

- (arXiv 2021.06) Fully Transformer Networks for Semantic Image **Segmentation**, [[Paper]](https://arxiv.org/pdf/2106.04108.pdf)

- (arXiv 2021.06) MST: Masked Self-Supervised Transformer for **Visual Representation**, [[Paper]](https://arxiv.org/pdf/2106.05656.pdf)

- (arXiv 2021.06) Space-time Mixing **Attention** for Video Transformer, [[Paper]](https://arxiv.org/pdf/2106.05968.pdf)

- (arXiv 2021.06) VIT-INCEPTION-GAN FOR **IMAGE COLOURISING**, [[Paper]](https://arxiv.org/pdf/2106.06321.pdf)

- (arXiv 2021.06) HYBRID **GENERATIVE-CONTRASTIVE REPRESENTATION** LEARNING, [[Paper]](https://arxiv.org/pdf/2106.06162.pdf), [[Code]](https://github.com/kakaobrain/gcrl)

- (arXiv 2021.06) OadTR: Online **Action Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.11149.pdf), [[Code]](https://github.com/wangxiang1230/OadTR)

- (arXiv 2021.06) VIMPAC: **Video** Pre-Training via Masked Token Prediction and Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2106.11250.pdf), [[Code]](https://github.com/airsplay/vimpac)

- (arXiv 2021.06) Delving Deep into the **Generalization** of Vision Transformers under **Distribution Shifts**, [[Paper]](https://arxiv.org/pdf/2106.07617.pdf), [[Code]](https://github.com/Phoenix1153/ViT_OOD_generalization)

- (arXiv 2021.06) Improved Transformer for **High-Resolution GANs**, [[Paper]](https://arxiv.org/pdf/2106.07631.pdf)

- (arXiv 2021.06) Towards Long-Form **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2106.11310.pdf), [[Code]](https://github.com/chaoyuaw/lvu)

- (arXiv 2021.06) TokenLearner: What Can 8 Learned **Tokens** Do for Images and Videos? [[Paper]](https://arxiv.org/pdf/2106.11297.pdf)

- (arXiv 2021.06) More than Encoder: Introducing Transformer Decoder to **Upsample**, [[Paper]](https://arxiv.org/pdf/2106.10637.pdf)

- (arXiv 2021.06) A Picture May Be Worth a Hundred Words for **Visual Question Answering**, [[Paper]](https://arxiv.org/pdf/2106.13445.pdf)

- (arXiv 2021.06) Probing Inter-modality: Visual Parsing with Self-Attention for **Vision-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2106.13488.pdf)

- (arXiv 2021.06) **Shape registration** in the time of transformers, [[Paper]](https://arxiv.org/pdf/2106.13679.pdf)

- (arXiv 2021.06) Vision Transformer **Architecture Search**, [[Paper]](https://arxiv.org/pdf/2106.13700.pdf), [[Code]](https://github.com/xiusu/ViTAS)

- (arXiv 2021.06) Unified Questioner Transformer for **Descriptive Question Generation** in Goal-Oriented Visual Dialogue, [[Paper]](https://arxiv.org/pdf/2106.15550.pdf)

- (arXiv 2021.06) **Multi-Exit** Vision Transformer for Dynamic Inference, [[Paper]](https://arxiv.org/pdf/2106.15183.pdf)

- (arXiv 2021.06) Early **Convolutions** Help Transformers See Better, [[Paper]](https://arxiv.org/pdf/2106.14881.pdf)

- (arXiv 2021.06) Rethinking Token-Mixing **MLP** for MLP-based Vision Backbone, [[Paper]](https://arxiv.org/pdf/2106.14882.pdf)

- (arXiv 2021.06) Augmented **Shortcuts** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.15941.pdf)

- (arXiv 2021.06) CAT: **Cross Attention** in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.05786.pdf), [[Code]](https://github.com/linhezheng19/CAT)

- (arXiv 2021.06) Post-Training **Quantization** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.14156.pdf)

- (arXiv 2021.06) Attention Bottlenecks for **Multimodal** Fusion, [[Paper]](https://arxiv.org/pdf/2107.00135.pdf)

- (arXiv 2021.06) Improving the **Efficiency** of Transformers for Resource-Constrained Devices, [[Paper]](https://arxiv.org/pdf/2106.16006.pdf)

- (arXiv 2021.06) **Multimodal** Few-Shot Learning with Frozen Language Models, [[Paper]](https://arxiv.org/pdf/2106.13884.pdf)

- (arXiv 2021.06) Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object **Detection** and **Segmentation**, [[Paper]](https://arxiv.org/pdf/2106.11401.pdf)

- (arXiv 2021.06) Exploring Vision Transformers for Fine-grained **Classification**, [[Paper]](https://arxiv.org/pdf/2106.10587.pdf), [[Code]](https://github.com/mv-lab/ViT-FGVC8)

- (arXiv 2021.06) S^2-MLP: Spatial-Shift **MLP** Architecture for Vision, [[Paper]](https://arxiv.org/pdf/2106.07477.pdf)

- (arXiv 2021.06) Styleformer: Transformer based **Generative Adversarial Networks** with Style Vector, [[Paper]](https://arxiv.org/pdf/2106.07023.pdf), [[Code]](https://github.com/Jeeseung-Park/Styleformer)

- (arXiv 2021.06) ViTAE: Vision Transformer Advanced by Exploring **Intrinsic Inductive Bias**, [[Paper]](https://arxiv.org/pdf/2106.03348.pdf), [[Code]](https://github.com/Annbless/ViTAE)

- (arXiv 2021.06) Shuffle Transformer: Rethinking **Spatial Shuffle** for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.03650.pdf)

- (arXiv 2021.06) Refiner: **Refining Self-attention** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.03714.pdf), [[Code]](https://github.com/zhoudaquan/Refiner_ViT)

- (arXiv 2021.06) Person **Re-Identification** with a Locally Aware Transformer, [[Paper]](https://arxiv.org/pdf/2106.03720.pdf)

- (arXiv 2021.06) **Efficient** Training of Visual Transformers with **Small-Size Datasets**, [[Paper]](https://arxiv.org/pdf/2106.03746.pdf)

- (arXiv 2021.06) **Glance-and-Gaze** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.02277.pdf), [[Code]](https://github.com/yucornetto/GG-Transformer)

- (arXiv 2021.06) **Few-Shot Segmentation** via Cycle-Consistent Transformer, [[Paper]](https://arxiv.org/pdf/2106.02320.pdf)

- (arXiv 2021.06) Semantic **Correspondence** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.02520.pdf), [[Code]](https://github.com/SunghwanHong/CATs)

- (arXiv 2021.06) THE **IMAGE LOCAL AUTOREGRESSIVE** TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2106.02514.pdf)

- (arXiv 2021.06) MERLOT: Multimodal **Neural Script Knowledge** Models, [[Paper]](https://arxiv.org/pdf/2106.02636.pdf), [[Project]](https://rowanzellers.com/merlot)

- (arXiv 2021.06) SOLQ: **Segmenting** Objects by Learning Queries, [[Paper]](https://arxiv.org/pdf/2106.02351.pdf), [[Code]](https://github.com/megvii-research/SOLQ)

- (arXiv 2021.06) Personalizing **Pre-trained Models**, [[Paper]](https://arxiv.org/pdf/2106.01499.pdf), [[Code]](https://github.com/PAL-ML/CLIPPER)

- (arXiv 2021.06) E2E-VLP: End-to-End **Vision-Language Pre-training** Enhanced by Visual Learning, [[Paper]](https://arxiv.org/pdf/2106.01804.pdf)

- (arXiv 2021.06) VOLO: Vision Outlooker for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2106.13112.pdf), [[Code]](https://github.com/sail-sg/volo)

- (arXiv 2021.06) Container: **Context Aggregation** Network, [[Paper]](https://arxiv.org/pdf/2106.01401.pdf)

- (arXiv 2021.06) Exploring Corruption **Robustness**: Inductive Biases in Vision Transformers and MLP-Mixers, [[Paper]](https://arxiv.org/pdf/2106.13122.pdf)

- (arXiv 2021.06) Video **Swin** Transformer, [[Paper]](https://arxiv.org/pdf/2106.13230.pdf), [[Code]](https://github.com/SwinTransformer/Video-Swin-Transformer)

- (arXiv 2021.06) IA-RED^2: Interpretability-Aware **Redundancy Reduction** for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.12620.pdf), [[Code]](http://people.csail.mit.edu/bpan/ia-red/)

- (arXiv 2021.06) AudioCLIP: Extending **CLIP** to Image, Text and Audio, [[Paper]](https://arxiv.org/pdf/2106.13043.pdf)

- (arXiv 2021.06) VISION PERMUTATOR: A PERMUTABLE MLP-LIKE ARCHITECTURE FOR VISUAL **RECOGNITION**, [[Paper]](https://arxiv.org/pdf/2106.12368.pdf), [[Code]](https://github.com/Andrew-Qibin/VisionPermutator)

- (arXiv 2021.06) Co-advise: Cross Inductive Bias **Distillation**, [[Paper]](https://arxiv.org/pdf/2106.12378.pdf)

- (arXiv 2021.06) Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2106.12023.pdf)

- (arXiv 2021.06) P2T: Pyramid Pooling Transformer for **Scene Understanding**, [[Paper]](https://arxiv.org/pdf/2106.12011.pdf), [[Code]](https://github.com/yuhuan-wu/P2T)

- (arXiv 2021.06) LegoFormer: Transformers for Block-by-Block **Multi-view 3D Reconstruction**, [[Paper]](https://arxiv.org/pdf/2106.12102.pdf)

- (arXiv 2021.06) Stable, Fast and Accurate: Kernelized Attention with Relative **Positional Encoding**, [[Paper]](https://arxiv.org/pdf/2106.12566.pdf)

- (arXiv 2021.06) MODETR: Moving Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.11422.pdf)

- (arXiv 2021.06) ResMLP: **Feedforward networks** for image classification with data-efficient training, [[Paper]](https://arxiv.org/pdf/2105.03404.pdf)

- (arXiv 2021.06) Multi-**head** or Single-**head**? An Empirical Comparison for Transformer Training, [[Paper]](https://arxiv.org/pdf/2106.09650.pdf)

- (arXiv 2021.06) Dynamic Head: Unifying Object **Detection** Heads with Attentions, [[Paper]](https://arxiv.org/pdf/2106.08322v1.pdf), [[Code]](https://github.com/microsoft/DynamicHead)

- (arXiv 2021.06) MLP-Mixer: An all-**MLP** Architecture for Vision, [[Paper]](https://arxiv.org/pdf/2105.01601.pdf), [[Code]](https://github.com/google-research/vision_transformer)

- (arXiv 2021.06) BEIT: BERT **Pre-Training** of Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.08254.pdf), [[Code]](https://aka.ms/beit)

- (arXiv 2021.06) Scaling Vision with **Sparse** Mixture of Experts, [[Paper]](https://arxiv.org/pdf/2106.05974.pdf)

- (arXiv 2021.06) Towards Training Stronger Video Vision Transformers for EPIC-KITCHENS-100 **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2106.05058.pdf)

- (arXiv 2021.06) Semi-Supervised 3D **Hand-Object Poses Estimation** with Interactions in Time, [[Paper]](https://arxiv.org/pdf/2106.05266.pdf), [[Code]](https://stevenlsw.github.io/Semi-Hand-Object)

- (arXiv 2021.06) DynamicViT: **Efficient** Vision Transformers with Dynamic Token Sparsification, [[Paper]](https://arxiv.org/pdf/2106.02034.pdf), [[Code]](https://github.com/raoyongming/DynamicViT)

- (arXiv 2021.06) SCTN: Sparse Convolution-Transformer Network for **Scene Flow Estimation**, [[Paper]](https://arxiv.org/pdf/2105.04447.pdf)

- (arXiv 2021.06) **Anticipative Video** Transformer, [[Paper]](https://arxiv.org/pdf/2106.02036.pdf), [[Project]](https://facebookresearch.github.io/AVT/)

- (arXiv 2021.06) Pay Attention to **MLPs**, [[Paper]](https://arxiv.org/pdf/2105.08050.pdf)

- (arXiv 2021.06) When Vision Transformers Outperform **ResNets** without Pretraining or Strong Data Augmentations, [[Paper]](https://arxiv.org/pdf/2106.01548.pdf)

- (arXiv 2021.06) StyTr^2: Unbiased Image **Style Transfer** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.14576.pdf)

- (arXiv 2021.06) THG:Transformer with **Hyperbolic Geometry**, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2106/2106.07350.pdf)

- (arXiv 2021.06) You Only Look at One Sequence: Rethinking Transformer in Vision through Object **Detection**, [[Paper]](https://arxiv.org/pdf/2106.00666.pdf), [[Code]](https://github.com/hustvl/YOLOS)

- (arXiv 2021.06) TransVOS: **Video Object Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2106.00588.pdf)

- (2021.06) **Reinforcement Learning** as One Big Sequence Modeling Problem, [[Paper]](https://people.eecs.berkeley.edu/~janner/trajectory-transformer/files/trajectory-transformer.pdf), [[Project]](https://trajectory-transformer.github.io/)

- (arXiv 2021.06) Less is More: Pay Less **Attention** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.14217.pdf), [[Code]](https://github.com/MonashAI/LIT)

- (arXiv 2021.06) SegFormer: Simple and Efficient Design for **Semantic Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.15203.pdf), [[Code]](https://github.com/NVlabs/SegFormer)

### 2021.05
- (arXiv 2021.05) KVT: k-NN **Attention** for Boosting Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.00515.pdf)

- (arXiv 2021.05) Memory-Efficient Differentiable Transformer **Architecture Search**, [[Paper]](https://arxiv.org/pdf/2105.14669.pdf)

- (arXiv 2021.05) An **Attention Free** Transformer, [[Paper]](https://arxiv.org/pdf/2105.14103.pdf)

- (arXiv 2021.05) On the Bias Against **Inductive Biases**, [[Paper]](https://arxiv.org/pdf/2105.14077.pdf)

- (arXiv 2021.05) MixerGAN: An MLP-Based Architecture for **Unpaired Image-to-Image Translation**, [[Paper]](https://arxiv.org/pdf/2105.14110.pdf)

- (arXiv 2021.05) Transformer-Based Source-Free **Domain Adaptation**, [[Paper]](https://arxiv.org/pdf/2105.14138.pdf), [[Code]](https://github.com/ygjwd12345/TransDA)

- (arXiv 2021.05) FoveaTer: Foveated Transformer for **Image Classification**, [[Paper]](https://arxiv.org/pdf/2105.14173.pdf)

- (arXiv 2021.05) UFC-BERT: Unifying Multi-Modal Controls for Conditional **Image Synthesis**, [[Paper]](https://arxiv.org/pdf/2105.14211.pdf)

- (arXiv 2021.05) **Gaze Estimation** using Transformer, [[Paper]](https://arxiv.org/pdf/2105.14424.pdf), [[Code]](https://github.com/yihuacheng/GazeTR)

- (arXiv 2021.05) Transformer-Based Deep Image Matching for Generalizable Person Re-identification, [[Paper]](https://arxiv.org/pdf/2105.14432.pdf), [[Project]](https://liaosc.wordpress.com/)

- (arXiv 2021.05) Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with **Adaptive Sequence Length**, [[Paper]](https://arxiv.org/pdf/2105.15075.pdf)

- (arXiv 2021.05) Analogous to **Evolutionary Algorithm**: Designing a Unified Sequence Model, [[Paper]](https://arxiv.org/pdf/2105.15089.pdf)

- (arXiv 2021.05) MSG-Transformer: Exchanging **Local Spatial Information** by Manipulating Messenger Tokens, [[Paper]](https://arxiv.org/pdf/2105.15168.pdf), [[Code]](https://github.com/hustvl/MSG-Transformer)

- (arXiv 2021.05) Sequence Parallelism: Making 4D **Parallelism** Possible, [[Paper]](https://arxiv.org/pdf/2105.13120.pdf)

- (arXiv 2021.05) CogView: Mastering **Text-to-Image Generation** via Transformers, [[Paper]](https://arxiv.org/pdf/2105.13290.pdf), [[Code]](https://github.com/THUDM/CogView)

- (arXiv 2021.05) TrTr: Visual **Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2105.03817.pdf), [[Code]](https://github.com/tongtybj/TrTr)

- (arXiv 2021.05) Conformer: Local Features Coupling Global Representations for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2105.03889.pdf), [[Code]](https://github.com/pengzhiliang/Conformer)

- (arXiv 2021.05) Visual **Grounding** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.04281.pdf)

- (arXiv 2021.05) **Self-Supervised** Learning with Swin Transformers, [[Paper]](https://arxiv.org/pdf/2105.04553.pdf), [[Code]](https://github.com/SwinTransformer/Transformer-SSL)

- (arXiv 2021.05) Are Pre-trained **Convolutions** Better than Pre-trained Transformers? [[Paper]](https://arxiv.org/pdf/2105.03322.pdf)

- (arXiv 2021.05) MOTR: End-to-End Multiple-Object **Tracking** with TRansformer, [[Paper]](https://arxiv.org/pdf/2105.03247.pdf), [[Code]](https://github.com/megvii-model/MOTR)

- (arXiv 2021.05) Attention for **Image Registration** (AiR): an unsupervised Transformer approach, [[Paper]](https://arxiv.org/pdf/2105.02282.pdf), [[Code]](https://gitlab.inria.fr/zihwang/transformer-for-image-registration)

- (arXiv 2021.05) EXPLORING EXPLICIT AND IMPLICIT VISUAL RELATIONSHIPS FOR IMAGE **CAPTIONING**, [[Paper]](https://arxiv.org/pdf/2105.02391.pdf)

- (arXiv 2021.05) **Computer-Aided Design** as Language, [[Paper]](https://arxiv.org/pdf/2105.02769.pdf)

- (arXiv 2021.05) FLEX: Parameter-free Multi-view 3D Human **Motion Reconstruction**, [[Paper]](https://arxiv.org/pdf/2105.01937.pdf), [[Project]](https://briang13.github.io/FLEX/)

- (arXiv 2021.05) TransHash: Transformer-based Hamming Hashing for Efficient Image **Retrieval**, [[Paper]](https://arxiv.org/pdf/2105.01823.pdf)

- (arXiv 2021.05) High-Resolution Complex **Scene Synthesis** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.06458.pdf)

- (arXiv 2021.05) Episodic Transformer for Vision-and-Language **Navigation**, [[Paper]](https://arxiv.org/pdf/2105.06453.pdf)

- (arXiv 2021.05) Towards **Robust** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2105.07926.pdf), [[Code]](https://git.io/Jswdk)

- (arXiv 2021.05) Vision Transformers are **Robust** Learners, [[Paper]](https://arxiv.org/pdf/2105.07581.pdf), [[Code]](https://git.io/J3VO0)

- (arXiv 2021.05) ISTR: End-to-End Instance **Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.00637.pdf), [[Code]](https://github.com/hujiecpp/ISTR)

- (arXiv 2021.05) SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale **Place Recognition**, [[Paper]](https://arxiv.org/pdf/2105.00149.pdf)

- (arXiv 2021.05) Rethinking **Skip** Connection with Layer Normalization in Transformers and ResNets, [[Paper]](https://arxiv.org/pdf/2105.07205.pdf)

- (arXiv 2021.05) IntFormer: **Predicting pedestrian intention** with the aid of the Transformer architecture, [[Paper]](https://arxiv.org/pdf/2105.08647.pdf)

- (arXiv 2021.05) Parallel Attention Network with Sequence Matching for Video **Grounding**, [[Paper]](https://arxiv.org/pdf/2105.08481.pdf), [[Code]](https://github.com/IsaacChanghau/SeqPAN)

- (arXiv 2021.05) Relative **Positional Encoding** for Transformers with Linear Complexity, [[Paper]](https://arxiv.org/pdf/2105.08399.pdf)

- (arXiv 2021.05) VTNET: VISUAL TRANSFORMER NETWORK FOR OBJECT GOAL **NAVIGATION**, [[Paper]](https://arxiv.org/pdf/2105.09447.pdf)

- (arXiv 2021.05) DeepCAD: A Deep Generative Network for **Computer-Aided Design** Models, [[Paper]](https://arxiv.org/pdf/2105.09492.pdf)

- (arXiv 2021.05) Single-Layer Vision Transformers for More Accurate **Early Exits** with Less Overhead, [[Paper]](https://arxiv.org/pdf/2105.09121.pdf)

- (arXiv 2021.05) An **Attention Free** Transformer, [[Paper]](https://arxiv.org/pdf/2105.14103.pdf)

- (arXiv 2021.05) Beyond Self-attention: **External Attention** using Two Linear Layers for Visual Tasks, [[Paper]](https://arxiv.org/pdf/2105.02358.pdf), [[Code]](https://github.com/MenghaoGuo/-EANet)

- (arXiv 2021.05) Combining Transformer Generators with Convolutional **Discriminators**, [[Paper]](https://arxiv.org/pdf/2105.10189.pdf)

- (arXiv 2021.05) VLM: Task-agnostic **Video-Language** Model Pre-training for Video Understanding, [[Paper]](https://arxiv.org/pdf/2105.09996.pdf)

- (arXiv 2021.05) Improving Generation and Evaluation of **Visual Stories** via Semantic Consistency, [[Paper]](https://arxiv.org/pdf/2105.10026.pdf), [[Code]](https://github.com/adymaharana/StoryViz)

- (arXiv 2021.05) BELT: Blockwise **Missing Embedding Learning** Transfomer, [[Paper]](https://arxiv.org/pdf/2105.10360.pdf)

- (arXiv 2021.05) End-to-End Video Object **Detection** with Spatial-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2105.10920.pdf), [[Code]](https://github.com/SJTU-LuHe/TransVOD)

- (arXiv 2021.05) SAT: 2D Semantics Assisted Training for 3D Visual **Grounding**, [[Paper]](https://arxiv.org/pdf/2105.11450.pdf)

- (arXiv 2021.05) Aggregating **Nested** Transformers, [[Paper]](https://arxiv.org/pdf/2105.12723.pdf)

- (arXiv 2021.05) Intriguing **Properties** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.10497.pdf), [[Code]](https://git.io/Js15X)

- (arXiv 2021.05) Temporal **Action Proposal Generation** with Transformers, [[Paper]](https://arxiv.org/pdf/2105.12043.pdf)

- (arXiv 2021.05) Learning Better **Visual Dialog Agents** with Pretrained Visual-Linguistic Representation, [[Paper]](https://arxiv.org/pdf/2105.11541.pdf), [[Code]](https://github.com/amazon-research/read-up)

- (arXiv 2021.05) Perceptual **Image Quality Assessment** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.14730.pdf)

- (arXiv 2021.05) Do You Even Need Attention? A Stack of **Feed-Forward Layers** Does Surprisingly Well on ImageNet, [[Paper]](https://arxiv.org/pdf/2105.02723v1.pdf), [[Code]](https://github.com/lukemelas/do-you-even-need-attention)

- (arXiv 2021.05) Pay Attention to **MLPs**, [[Paper]](https://arxiv.org/pdf/2105.08050.pdf)

- (arXiv 2021.05) ResMLP: Feedforward networks for image **classification** with data-efficient training, [[Paper]](https://arxiv.org/pdf/2105.03404v1.pdf)

- (arXiv 2021.05) RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image **Recognition**, [[Paper]](https://arxiv.org/pdf/2105.01883v1.pdf), [[Code]](https://github.com/DingXiaoH/RepMLP)

- (arXiv 2021.05) Are Convolutional Neural Networks or Transformers more like **human vision**? [[Paper]](https://arxiv.org/pdf/2105.07197.pdf)

- (arXiv 2021.05) FNet: Mixing Tokens with **Fourier** Transforms, [[Paper]](https://arxiv.org/pdf/2105.03824.pdf)

- (arXiv 2021.05) Segmenter: Transformer for Semantic **Segmentation**, [[Paper]](https://arxiv.org/pdf/2105.05633.pdf), [[Code]](https://github.com/rstrudel/segmenter)

- (arXiv 2021.05) TransHash: Transformer-based Hamming Hashing for **Efficient** Image Retrieval, [[Paper]](https://arxiv.org/pdf/2105.01823.pdf)

- (arXiv 2021.05) Visual **Composite Set Detection** Using Part-and-Sum Transformers, [[Paper]](https://arxiv.org/pdf/2105.02170.pdf)

### 2021.04
- (arXiv 2021.04) HandsFormer: Keypoint Transformer for Monocular 3D **Pose** Estimation of Hands and Object in Interaction, [[Paper]](https://arxiv.org/pdf/2104.14639.pdf)

- (arXiv 2021.04) Chop Chop BERT: **Visual Question Answering** by Chopping VisualBERT’s Heads, [[Paper]](https://arxiv.org/pdf/2104.14741.pdf)

- (arXiv 2021.04) CoSformer: **Detecting** Co-Salient Object with Transformers, [[Paper]](https://arxiv.org/pdf/2104.14729.pdf)

- (arXiv 2021.04) CAT: Cross-Attention Transformer for One-Shot Object **Detection**, [[Paper]](https://arxiv.org/pdf/2104.14984.pdf)

- (arXiv 2021.04) Dual Transformer for **Point Cloud** Analysis, [[Paper]](https://arxiv.org/pdf/2104.13044.pdf)

- (arXiv 2021.04) Playing **Lottery Tickets** with Vision and Language, [[Paper]](https://arxiv.org/pdf/2104.11832.pdf)

- (arXiv 2021.04) M3DETR: Multi-representation, Multi-scale, Mutual-relation 3D Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.11896.pdf)

- (arXiv 2021.04) RelTransformer: Balancing the **Visual Relationship** Detection from Local Context, Scene and Memory, [[Paper]](https://arxiv.org/pdf/2104.11934.pdf), [[Code]](https://github.com/Vision-CAIR/RelTransformer)

- (arXiv 2021.04) MDETR-Modulated Detection for End-to-End **Multi-Modal** Understanding, [[Paper]](https://arxiv.org/pdf/2104.12763.pdf), [[Code]](https://github.com/ashkamath/mdetr)

- (arXiv 2021.04) Rich Semantics Improve **Few-shot** Learning, [[Paper]](https://arxiv.org/pdf/2104.12709.pdf), [[Code]](https://github.com/MohamedAfham/RS_FSL)

- (arXiv 2021.04) Effect of Vision-and-Language Extensions on Natural Language Understanding in **Vision-and-Language** Models, [[Paper]](https://arxiv.org/pdf/2104.08066.pdf)

- (arXiv 2021.04) Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with **56M Parameters** on ImageNet, [[Paper]](https://arxiv.org/pdf/2104.10858.pdf), [[Code]](https://github.com/zihangJiang/TokenLabeling)

- (arXiv 2021.04) So-ViT: Mind Visual **Tokens** for Vision Transforme, [[Paper]](https://arxiv.org/pdf/2104.10935.pdf)

- (arXiv 2021.04) **Multiscale** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.11227.pdf), [[Code]](https://github.com/facebookresearch/SlowFast)

- (arXiv 2021.04) M2TR: Multi-modal Multi-scale Transformers for **Deepfake** Detection, [[Paper]](https://arxiv.org/pdf/2104.09770.pdf)

- (arXiv 2021.04) Transformer Transforms Salient Object Detection and Camouflaged Object **Detection**, [[Paper]](https://arxiv.org/pdf/2104.10127.pdf)

- (arXiv 2021.04) T2VLAD: Global-Local Sequence Alignment for Text-Video **Retrieval**, [[Paper]](https://arxiv.org/pdf/2104.10054.pdf)

- (arXiv 2021.04) VT-ADL: A Vision Transformer Network for Image **Anomaly** Detection and Localization, [[Paper]](https://arxiv.org/pdf/2104.10036.pdf)

- (arXiv 2021.04) Multi-Modal Fusion Transformer for End-to-End **Autonomous Driving**, [[Paper]](https://arxiv.org/pdf/2104.09224.pdf), [[Code]](https://github.com/autonomousvision/transfuser)

- (arXiv 2021.04) TransVG: End-to-End Visual **Grounding** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.08541.pdf)

- (arXiv 2021.04) Visual Transformer **Pruning**, [[Paper]](https://arxiv.org/pdf/2104.08500.pdf)

- (arXiv 2021.04) Higher Order Recurrent **Space-Time** Transformer, [[Paper]](https://arxiv.org/pdf/2104.08665.pdf), [[Code]](https://github.com/CorcovadoMing/HORST)

- (arXiv 2021.04) CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip **Retrieval**, [[Paper]](https://arxiv.org/pdf/2104.08860.pdf), [[Code]](https://github.com/ArrowLuo/CLIP4Clip)

- (arXiv 2021.04) Lessons on **Parameter Sharing** across Layers in Transformers, [[Paper]](https://arxiv.org/pdf/2104.06022.pdf)

- (arXiv 2021.04) Disentangled Motif-aware **Graph** Learning for Phrase Grounding, [[Paper]](https://arxiv.org/pdf/2104.06008.pdf)

- (arXiv 2021.04) Co-**Scale** Conv-**Attentional** Image Transformers, [[Paper]](https://arxiv.org/pdf/2104.06399.pdf), [[Code]](https://github.com/mlpc-ucsd/CoaT)

- (arXiv 2021.04) Cloth Interactive Transformer for **Virtual Try-On**, [[Paper]](https://arxiv.org/pdf/2104.05519.pdf), [[Code]](https://github.com/Amazingren/CIT)

- (arXiv 2021.04) LocalViT: Bringing **Locality** to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.05707.pdf), [[Code]](https://github.com/ofsoundof/LocalViT)

- (arXiv 2021.04) Seeing Out of tHe bOx: End-to-End Pre-training for **Vision-Language** Representation Learning, [[Paper]](https://arxiv.org/pdf/2104.03135.pdf)

- (arXiv 2021.04) Facial Attribute Transformers for Precise and Robust **Makeup Transfer**, [[Paper]](https://arxiv.org/pdf/2104.02894.pdf)

- (arXiv 2021.04) Emerging Properties in **Self-Supervised** Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.14294.pdf), [[Code]](https://github.com/facebookresearch/dino?fbclid=IwAR3tq8GzNrR4BaZjjSqGyWe6qsFK1XenBZNIZCCgjJmMi0ghettvIiDRAxM)

- (arXiv 2021.04) ConTNet: Why not use **convolution** and transformer at the same time? [[Paper]](https://arxiv.org/pdf/2104.13497.pdf), [[Code]](https://github.com/yan-hao-tian/ConTNet)

- (arXiv 2021.04) **Point Cloud** Learning with Transformer, [[Paper]](https://arxiv.org/pdf/2104.13636.pdf)

- (arXiv 2021.04) Twins: Revisiting the Design of **Spatial Attention** in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.13840.pdf), [[Code]](https://github.com/Meituan-AutoML/Twins)

- (arXiv 2021.04) Inpainting Transformer for **Anomaly** Detection, [[Paper]](https://arxiv.org/pdf/2104.13897.pdf)

- (arXiv 2021.04) Shot Contrastive Self-Supervised Learning for **Scene Boundary Detection**, [[Paper]](https://arxiv.org/pdf/2104.13537.pdf)

- (arXiv 2021.04) HOTR: End-to-End **Human-Object Interaction** Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2104.13682.pdf)

- (arXiv 2021.04) Visual **Saliency** Transformer, [[Paper]](https://arxiv.org/pdf/2104.12099.pdf)

- (arXiv 2021.04) Improve Vision Transformers Training by Suppressing **Over-smoothing**, [[Paper]](https://arxiv.org/pdf/2104.12753.pdf), [[Code]](https://github.com/ChengyueGongR/PatchVisionTransformer)

- (arXiv 2021.04) Visformer: The Vision-**friendly** Transformer, [[Paper]](https://arxiv.org/pdf/2104.12533.pdf), [[Code]](https://github.com/danczs/Visformer)

- (arXiv 2021.04) TransMOT: Spatial-Temporal Graph Transformer for Multiple Object **Tracking**, [[Paper]](https://arxiv.org/pdf/2104.00194.pdf)

- (arXiv 2021.04) **Mesh** Graphormer, [[Paper]](https://arxiv.org/pdf/2104.00272.pdf), [[Code]](https://github.com/microsoft/meshgraphormer)

- (arXiv 2021.04) TRAJEVAE - Controllable Human **Motion Generation** from Trajectories, [[Paper]](https://arxiv.org/pdf/2104.00351.pdf)

- (arXiv 2021.04) UC^2: Universal Cross-lingual Cross-modal **Vision-and-Language** Pre-training, [[Paper]](https://arxiv.org/pdf/2104.00332.pdf)

- (arXiv 2021.04) Learning to Cluster **Faces** via Transformer, [[Paper]](https://arxiv.org/pdf/2104.11502.pdf)

- (arXiv 2021.04) Skeletor: Skeletal Transformers for Robust Body-**Pose** Estimation, [[Paper]](https://arxiv.org/pdf/2104.11712.pdf)

- (arXiv 2021.04) VidTr: Video Transformer **Without Convolutions**, [[Paper]](https://arxiv.org/pdf/2104.11746.pdf)

- (arXiv 2021.04) VATT: Transformers for Multimodal Self-Supervised Learning from Raw **Video, Audio and Text**, [[Paper]](https://arxiv.org/pdf/2104.11178.pdf)

- (arXiv 2021.04) Going **deeper** with Image Transformers, [[Paper]](https://arxiv.org/pdf/2103.17239.pdf)

- (arXiv 2021.04) **EFFICIENT** PRE-TRAINING OBJECTIVES FOR TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2104.09694.pdf), [[Code]](https://github.com/iKernels/efficient-pre-training-objectives-for-transformers)

- (arXiv 2021.04) ROFORMER: ENHANCED TRANSFORMER WITH ROTARY **POSITION EMBEDDING**, [[Paper]](https://arxiv.org/pdf/2104.09864.pdf)

- (arXiv 2021.04) VideoGPT: **Video Generation** using VQ-VAE and Transformers, [[Paper]](https://arxiv.org/pdf/2104.10157.pdf), [[Code]](https://wilson1yan.github.io/videogpt/index.html)

- (arXiv 2021.04) DODRIO: Exploring Transformer Models with **Interactive Visualization**, [[Paper]](https://arxiv.org/pdf/2103.14625.pdf), [[Code]](https://poloclub.github.io/dodrio/)

- (arXiv 2021.04) Lifting Transformer for 3D Human **Pose** Estimation in Video, [[Paper]](https://arxiv.org/pdf/2103.14304.pdf)

- (arXiv 2021.04) Demystifying the Better Performance of **Position Encoding** Variants for Transformer, [[Paper]](https://arxiv.org/pdf/2104.08698.pdf)

- (arXiv 2021.04) Consistent Accelerated Inference via **Confident Adaptive** Transformers, [[Paper]](https://arxiv.org/pdf/2104.08803.pdf), [[Code]](https://github.com/TalSchuster/CATs)

- (arXiv 2021.04) Temporal Query Networks for Fine-grained **Video** Understanding, [[Paper]](https://arxiv.org/pdf/2104.09496.pdf), [[Code]](http://www.robots.ox.ac.uk/~vgg/research/tqn/)

- (arXiv 2021.04) Face Transformer for **Recognition**, [[Paper]](https://arxiv.org/pdf/2103.14803.pdf), [[Code]](https://github.com/zhongyy/Face-Transformer)

- (arXiv 2021.04) VGNMN: Video-grounded Neural Module Network to **Video-Grounded Language** Tasks, [[Paper]](https://arxiv.org/pdf/2104.07921.pdf)

- (arXiv 2021.04) Self-supervised Video **Retrieval** Transformer Network, [[Paper]](https://arxiv.org/pdf/2104.07993.pdf)

- (arXiv 2021.04) Cross-Modal Retrieval Augmentation for Multi-Modal **Classification**, [[Paper]](https://arxiv.org/pdf/2104.08108.pdf)

- (arXiv 2021.04) Point-Based Modeling of **Human Clothing**, [[Paper]](https://arxiv.org/pdf/2104.08230.pdf)

- (arXiv 2021.04) Points as Queries: Weakly Semi-supervised Object **Detection** by Points, [[Paper]](https://arxiv.org/pdf/2104.07434.pdf)

- (arXiv 2021.04) Geometry-Free **View Synthesis**: Transformers and no 3D Priors, [[Paper]](https://arxiv.org/pdf/2104.07652.pdf), [[Code]](https://git.io/JOnwn)

- (arXiv 2021.04) Self-supervised Video Object **Segmentation** by Motion Grouping, [[Paper]](https://arxiv.org/pdf/2104.07658.pdf), [[Project]](https://charigyang.github.io/motiongroup/)

- (arXiv 2021.04) Decoupled Spatial-Temporal Transformer for Video **Inpainting**, [[Paper]](https://arxiv.org/pdf/2104.06637.pdf), [[Code]](https://github.com/ruiliu-ai/DSTT)

- (arXiv 2021.04) **Pose** Recognition with Cascade Transformers, [[Paper]](https://arxiv.org/pdf/2104.06976.pdf), [[Code]](https://github.com/mlpc-ucsd/PRTR)

- (arXiv 2021.04) Action-Conditioned **3D Human Motion Synthesis** with Transformer VAE, [[Paper]](https://arxiv.org/pdf/2104.05670.pdf), [[Project]](https://imagine.enpc.fr/~petrovim/actor)

- (arXiv 2021.04) Escaping the Big Data Paradigm with **Compact** Transformers, [[Paper]](https://arxiv.org/pdf/2104.05704.pdf), [[Code]](https://github.com/SHI-Labs/Compact-Transformers)

- (arXiv 2021.04) Know What and Know Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language **Navigation**, [[Paper]](https://arxiv.org/pdf/2104.04167.pdf)

- (arXiv 2021.04) **Handwriting** Transformers, [[Paper]](https://arxiv.org/pdf/2104.03964.pdf)

- (arXiv 2021.04) SiT: **Self-supervised** vIsion Transformer, [[Paper]](https://arxiv.org/pdf/2104.03602.pdf)

- (arXiv 2021.04) EFFICIENT TRANSFORMERS IN **REINFORCEMENT LEARNING** USING ACTOR-LEARNER DISTILLATION, [[Paper]](https://arxiv.org/pdf/2104.01655.pdf)

- (arXiv 2021.04) **Compressing** Visual-linguistic Model via Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2104.02096.pdf)

- (arXiv 2021.04) When Pigs Fly: Contextual **Reasoning** in Synthetic and Natural Scenes, [[Paper]](https://arxiv.org/pdf/2104.02215.pdf)

- (arXiv 2021.04) Variational Transformer Networks for **Layout Generation**, [[Paper]](https://arxiv.org/pdf/2104.02416.pdf)

- (arXiv 2021.04) Few-Shot Transformation of Common **Actions** into Time and Space, [[Paper]](https://arxiv.org/pdf/2104.02439.pdf)

- (arXiv 2021.04) **Fourier** Image Transformer, [[Paper]](https://arxiv.org/pdf/2104.02555.pdf)

- (arXiv 2021.04) **Efficient** DETR: Improving End-to-End Object Detector with Dense Prior, [[Paper]](https://arxiv.org/pdf/2104.01318.pdf)

- (arXiv 2021.04) A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person **Re-identification**, [[Paper]](https://arxiv.org/pdf/2104.01745.pdf)

- (arXiv 2021.04) An Empirical Study of **Training** Self-Supervised Visual Transformers, [[Paper]](https://arxiv.org/pdf/2104.02057.pdf)

- (arXiv 2021.04) Multitarget **Tracking** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00734.pdf)

- (arXiv 2021.04) TFill: **Image Completion** via a Transformer-Based Architecture, [[Paper]](https://arxiv.org/pdf/2104.00845.pdf), [[Code]](https://github.com/lyndonzheng/TFill)

- (arXiv 2021.04) AAformer: Auto-Aligned Transformer for Person **Re-Identification**, [[Paper]](https://arxiv.org/pdf/2104.00921.pdf)

- (arXiv 2021.04) VisQA: X-raying Vision and Language **Reasoning** in Transformers, [[Paper]](https://arxiv.org/pdf/2104.00926.pdf)

- (arXiv 2021.04) TubeR: Tube-Transformer for **Action Detection**, [[Paper]](https://arxiv.org/pdf/2104.00969.pdf)

- (arXiv 2021.04) Language-based **Video Editing** via Multi-Modal Multi-Level Transformer, [[Paper]](https://arxiv.org/pdf/2104.01122.pdf)

- (arXiv 2021.04) LeViT: a Vision Transformer in ConvNet’s Clothing for **Faster** Inference, [[Paper]](https://arxiv.org/pdf/2104.01136.pdf)

- (arXiv 2021.04) LoFTR: Detector-Free **Local Feature Matching** with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00680.pdf), [[Code]](https://zju3dv.github.io/loftr/)

- (arXiv 2021.04) Putting **NeRF** on a Diet: Semantically Consistent Few-Shot View Synthesis, [[Paper]](https://arxiv.org/pdf/2104.00677.pdf), [[Project]](https://www.ajayj.com/dietnerf)

- (arXiv 2021.04) Group-Free 3D Object **Detection** via Transformers, [[Paper]](https://arxiv.org/pdf/2104.00678.pdf), [[Code]](https://github.com/zeliu98/Group-Free-3D)

- (arXiv 2021.04) Frozen in Time: A Joint Video and Image Encoder for End-to-End **Retrieval**, [[Paper]](https://arxiv.org/pdf/2104.00650.pdf)

- (arXiv 2021.04) Composable Augmentation Encoding for Video **Representation** Learning, [[Paper]](https://arxiv.org/pdf/2104.00616.pdf)

### 2021.03
- (arXiv 2021.03) TransCenter: Transformers with Dense Queries for Multiple-Object **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.15145.pdf)

- (arXiv 2021.03) PixelTransformer: Sample Conditioned **Signal Generation**, [[Paper]](https://arxiv.org/pdf/2103.15813.pdf), [[Code]](https://shubhtuls.github.io/PixelTransformer/)

- (arXiv 2021.03) Augmented Transformer with Adaptive Graph for **Temporal Action Proposal Generation**, [[Paper]](https://arxiv.org/pdf/2103.16024.pdf)

- (arXiv 2021.03) DA-DETR: Domain Adaptive **Detection** Transformer by Hybrid Attention, [[Paper]](https://arxiv.org/pdf/2103.17084.pdf)

- (arXiv 2021.03) Learning Spatio-Temporal Transformer for Visual **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.17154.pdf), [[Code]](https://github.com/researchmm/Stark)

- (arXiv 2021.03) StyleCLIP: Text-Driven Manipulation of **StyleGAN** Imagery, [[Paper]](https://arxiv.org/pdf/2103.17249.pdf), [[Code]](https://github.com/orpatashnik/StyleCLIP)

- (arXiv 2021.03) Multimodal **Motion Prediction** with Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)

- (arXiv 2021.03) Robust Facial **Expression** Recognition with Convolutional Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.16854.pdf)

- (arXiv 2021.03) **Describing and Localizing** Multiple Changes with Transformers, [[Paper]](https://arxiv.org/pdf/2103.14146.pdf), [[Project]](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers)

- (arXiv 2021.03) COTR: **Correspondence** Transformer for Matching Across Images, [[Paper]](https://arxiv.org/pdf/2103.14167.pdf)

- (arXiv 2021.03) nderstanding Robustness of Transformers for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2103.14586.pdf)

- (arXiv 2021.03) CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image **Classification**, [[Paper]](https://arxiv.org/pdf/2103.14899.pdf)

- (arXiv 2021.03) Looking Beyond Two Frames: End-to-End Multi-Object **Tracking** Using Spatial and Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2103.14829.pdf)

- (arXiv 2021.03) HiT: Hierarchical Transformer with Momentum Contrast for Video-Text **Retrieval**, [[Paper]](https://arxiv.org/pdf/2103.15049.pdf)

- (arXiv 2021.03) TFPose: Direct Human **Pose** Estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2103.15320.pdf), [[Code]](https://git.io/AdelaiDet)

- (arXiv 2021.03) Multi-Scale Vision Longformer: A New Vision Transformer for **High-Resolution Image** Encoding, [[Paper]](https://arxiv.org/pdf/2103.15358.pdf)

- (arXiv 2021.03) Transformer **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.15436.pdf), [[Code]](https://github.com/chenxin-dlut/TransT)

- (arXiv 2021.03) ViViT: A **Video** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.15691.pdf)

- (arXiv 2021.03) CvT: Introducing **Convolutions** to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2103.15808.pdf), [[Code]](https://github.com/leoxiaobin/CvT)

- (arXiv 2021.03) Generic Attention-model **Explainability** for Interpreting Bi-Modal and Encoder-Decoder Transformers, [[Paper]](https://arxiv.org/pdf/2103.15679.pdf), [[Code]](https://github.com/hila-chefer/Transformer-MM-Explainability)

- (arXiv 2021.03) On the Adversarial **Robustness** of Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.15670.pdf)

- (arXiv 2021.03) Rethinking **Spatial Dimensions** of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2103.16302.pdf), [[Code]](https://github.com/naver-ai/pit)

- (arXiv 2021.03) Spatiotemporal Transformer for Video-based **Person Re-identification**, [[Paper]](https://arxiv.org/pdf/2103.16469.pdf)

- (arXiv 2021.03) Read and Attend: **Temporal Localisation** in Sign Language Videos, [[Paper]](https://arxiv.org/pdf/2103.16481.pdf), [[Benchmark]](https://github.com/visipedia/newt)

- (arXiv 2021.03) Thinking Fast and Slow: **Efficient** Text-to-Visual Retrieval with Transformers, [[Paper]](https://arxiv.org/pdf/2103.16553.pdf)

- (arXiv 2021.03) An Image is Worth 16x16 Words, What is a **Video** Worth? [[Paper]](https://arxiv.org/pdf/2103.13915.pdf)

- (arXiv 2021.03) High-Fidelity Pluralistic **Image Completion** with Transformers, [[Paper]](https://arxiv.org/pdf/2103.14031.pdf), [[Code]](http://raywzy.com/ICT)

- (arXiv 2021.03) Swin Transformer: **Hierarchical** Vision Transformer using Shifted Windows, [[Paper]](https://arxiv.org/pdf/2103.14030.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer)

- (arXiv 2021.03) Revamping Cross-Modal Recipe **Retrieval** with Hierarchical Transformers and Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2103.13061.pdf), [[Code]](https://github.com/amzn/image-to-recipe-transformers)

- (arXiv 2021.03) Multi-view 3D **Reconstruction** with Transformer, [[Paper]](https://arxiv.org/pdf/2103.12957.pdf)

- (arXiv 2021.03) Scene-Intuitive Agent for Remote Embodied Visual **Grounding**, [[Paper]](https://arxiv.org/pdf/2103.12944.pdf)

- (arXiv 2021.03) Can Vision Transformers Learn **without Natural Images**? [[Paper]](https://arxiv.org/pdf/2103.13023.pdf)

- (arXiv 2021.03) On the **Robustness** of Vision Transformers to Adversarial Examples, [[Paper]](https://arxiv.org/pdf/2104.02610.pdf)

- (arXiv 2021.03) Kaleido-BERT: **Vision-Language** Pre-training on Fashion Domain, [[Paper]](https://arxiv.org/pdf/2103.16110.pdf), [[Code]](http://dpfan.net/Kaleido-BERT)

- (arXiv 2021.03) End-to-End Trainable Multi-Instance **Pose Estimation** with Transformers, [[Paper]](https://arxiv.org/pdf/2103.12115.pdf)

- (arXiv 2021.03) Transformers Solve the Limited Receptive Field for **Monocular Depth Prediction**, [[Paper]](https://arxiv.org/pdf/2103.12091.pdf), [[Code]](https://github.com/ygjwd12345/TransDepth)

- (arXiv 2021.03) Meta-DETR: Few-Shot Object **Detection** via Unified Image-Level Meta-Learning, [[Paper]](https://arxiv.org/pdf/2103.11731.pdf)

- (arXiv 2021.03) Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual **Tracking**, [[Paper]](https://arxiv.org/pdf/2103.11681.pdf), [[Code]](https://github.com/594422814/TransformerTrack)

- (arXiv 2021.03) DeepViT: Towards **Deeper** Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.11886.pdf), [[Code]](https://github.com/zhoudaquan/dvit_repo)

- (arXiv 2021.03) Incorporating **Convolution** Designs into Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.11816.pdf)

- (arXiv 2021.03) Multimodal **Motion Prediction** with Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)

- (arXiv 2021.03) MaAST: Map Attention with Semantic Transformers for Efficient Visual **Navigation**, [[Paper]](https://arxiv.org/pdf/2103.11374.pdf)

- (arXiv 2021.03) Paying Attention to Multiscale Feature Maps in Multimodal **Image Matching**, [[Paper]](https://arxiv.org/pdf/2103.11247.pdf)

- (arXiv 2021.03) HOPPER: MULTI-HOP TRANSFORMER FOR **SPATIOTEMPORAL REASONING**, [[Paper]](https://arxiv.org/pdf/2103.10574.pdf), [[Code]](https://github.com/necla-ml/cater-h)

- (arXiv 2021.03) Scalable Visual Transformers with **Hierarchical Pooling**, [[Paper]](https://arxiv.org/pdf/2103.10619.pdf)

- (arXiv 2021.03) AgentFormer: Agent-Aware Transformers for Socio-Temporal **Multi-Agent Forecasting**, [[Paper]](https://arxiv.org/pdf/2103.14023.pdf), [[Code]](https://github.com/Khrylx/AgentFormer)

- (arXiv 2021.03) Vision Transformers for **Dense Prediction**, [[Paper]](https://arxiv.org/pdf/2103.13413.pdf), [[Code]](https://github.com/intel-isl/DPT)

- (arXiv 2021.03) **3D Human Pose** Estimation with Spatial and Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2103.10455.pdf), [[Code]](https://github.com/zczcwh/PoseFormer)

- (arXiv 2021.03) ConViT: Improving Vision Transformers ith **Soft Convolutional Inductive Biases**, [[Paper]](https://arxiv.org/pdf/2103.10697.pdf), [[Code]](https://github.com/facebookresearch/convit)

- (arXiv 2021.03) MDMMT: Multidomain Multimodal Transformer for Video **Retrieval**, [[Paper]](https://arxiv.org/pdf/2103.10699.pdf)

- (arXiv 2021.03) On the **Sentence Embeddings** from Pre-trained Language Models, [[Paper]](https://arxiv.org/pdf/2011.05864.pdf)

- (arXiv 2021.03) Enhancing Transformer for **Video** Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training, [[Paper]](https://arxiv.org/pdf/2103.10043.pdf)

- (arXiv 2021.03) DanceNet3D: Music Based **Dance Generation** with Parametric Motion Transformer, [[Paper]](https://arxiv.org/pdf/2103.10206.pdf)

- (arXiv 2021.03) Decoupled Spatial Temporal Graphs for Generic **Visual Grounding**, [[Paper]](https://arxiv.org/pdf/2103.10191.pdf)

- (arXiv 2021.03) Space-Time Crop & Attend: Improving Cross-modal **Video Representation** Learning, [[Paper]](https://arxiv.org/pdf/2103.10211.pdf)

- (arXiv 2021.03) Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2103.08849.pdf), [[Code]](http://github.com/berniebear/Mutli-HT100M)

- (arXiv 2021.03) TransFG: A Transformer Architecture for Fine-grained **Recognition**, [[Paper]](https://arxiv.org/pdf/2103.07976.pdf)

- (arXiv 2021.03) Causal Attention for **Vision-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2103.03493.pdf), [[Code]](https://github.com/yangxuntu/catt)

- (arXiv 2021.03) Continuous 3D Multi-Channel **Sign Language Production** via Progressive Transformers and Mixture Density Networks, [[Paper]](https://arxiv.org/pdf/2103.06982.pdf)

- (arXiv 2021.03) WenLan: Bridging Vision and Language by Large-Scale Multi-Modal **Pre-Training**, [[Paper]](https://arxiv.org/pdf/2103.06561.pdf)

- (arXiv 2021.03) **Attention** is not all you need: pure attention loses rank doubly exponentially with depth, [[Paper]](https://arxiv.org/pdf/2103.03404v1.pdf)

- (arXiv 2021.03) QPIC: Query-Based Pairwise **Human-Object Interaction** Detection with Image-Wide Contextual Information, [[Paper]](https://arxiv.org/pdf/2103.05399), [[Code]](https://github.com/hitachi-rd-cv/qpic)

- (arXiv 2021.03) Reformulating **HOI** Detection as Adaptive Set Prediction, [[Paper]](https://arxiv.org/pdf/2103.05983), [[Code]](https://github.com/yoyomimi/AS-Net)

- (arXiv 2021.03) End-to-End **Human Object Interaction** Detection with HOI Transformer, [[Paper]](https://arxiv.org/pdf/2103.04503), [[Code]](https://github.com/bbepoch/HoiTransformer)

- (arXiv 2021.03) Perceiver: General Perception with Iterative **Attention**, [[Paper]](https://arxiv.org/pdf/2103.03206.pdf)

- (arXiv 2021.03) Transformer **in** Transformer, [[Paper]](https://arxiv.org/pdf/2103.00112.pdf), [[Code]](https://github.com/huawei-noah/noah-research/tree/master/TNT)

- (arXiv 2021.03) **Generative Adversarial** Transformers, [[Paper]](https://arxiv.org/pdf/2103.01209.pdf), [[Code]](https://github.com/dorarad/gansformer)

- (arXiv 2021.03) OmniNet: Omnidirectional **Representations** from Transformers, [[Paper]](https://arxiv.org/pdf/2103.01075.pdf)

- (arXiv 2021.03) Single-Shot **Motion Completion** with Transformer, [[Paper]](https://arxiv.org/pdf/2103.00776.pdf), [[Code]](https://github.com/FuxiCV/SSMCT)

### 2021.02
- (arXiv 2021.02) Evolving Attention with **Residual** Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12895.pdf)

- (arXiv 2021.02) GEM: Glare or Gloom, I Can Still See You – End-to-End Multimodal Object **Detector**, [[Paper]](https://arxiv.org/pdf/2102.12319.pdf)

- (arXiv 2021.02) SparseBERT: Rethinking the Importance Analysis in **Self-attention**, [[Paper]](https://arxiv.org/pdf/2102.12871.pdf)

- (arXiv 2021.02) Investigating the **Limitations** of Transformers with Simple Arithmetic Tasks, [[Paper]](https://arxiv.org/pdf/2102.13019.pdf), [[Code]](https://github.com/castorini/transformers-arithmetic)

- (arXiv 2021.02) Do Transformer Modifications **Transfer** Across Implementations and Applications? [[Paper]](https://arxiv.org/pdf/2102.11972.pdf)

- (arXiv.2021.02) Do We Really Need Explicit **Position Encodings** for Vision Transformers? [[Paper]](https://arxiv.org/pdf/2102.10882.pdf), [[Code]](https://github.com/Meituan-AutoML/CPVT)

- (arXiv.2021.02) A Straightforward Framework For Video **Retrieval** Using CLIP, [[Paper]](https://arxiv.org/pdf/2102.12443.pdf), [[Code]](https://github.com/Deferf/CLIP_Video_Representation)

- (arXiv.2021.02) Pyramid Vision Transformer: A Versatile Backbone for **Dense Prediction** without Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12122.pdf), [[Code]](https://github.com/whai362/PVT)

- (arXiv.2021.02) VisualGPT: Data-efficient Image **Captioning** by Balancing Visual Input and Linguistic Knowledge from Pretraining, [[Paper]](https://arxiv.org/pdf/2102.10407.pdf), [[Code]](https://github.com/Vision-CAIR/VisualGPT)

- (arXiv.2021.02) Towards Accurate and **Compact** Architectures via Neural Architecture Transformer, [[Paper]](https://arxiv.org/pdf/2102.10301.pdf)

- (arXiv.2021.02) Centroid Transformer: Learning to **Abstract** with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) **Linear** Transformers Are Secretly Fast Weight Memory Systems, [[Paper]](https://arxiv.org/pdf/2102.11174.pdf)

- (arXiv.2021.02) **POSITION** INFORMATION IN TRANSFORMERS: AN OVERVIEW, [[Paper]](https://arxiv.org/pdf/2102.11090.pdf)

- (arXiv 2021.02) Transformer is All You Need: **Multimodal** Multitask Learning with a Unified Transformer, [[Paper]](https://arxiv.org/pdf/2102.10772.pdf), [[Project]](https://mmf.sh/), [[Code]](https://github.com/facebookresearch/mmf)

- (arXiv 2021.02) Centroid Transformer: Learning to **Abstract** with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) Conceptual 12M: Pushing Web-Scale **Image-Text** Pre-Training To Recognize Long-Tail Visual Concepts, [[Paper]](https://arxiv.org/pdf/2102.08981.pdf)

- (arXiv 2021.02) TransGAN: Two Transformers Can Make One Strong **GAN**, [[Paper]](https://arxiv.org/pdf/2102.07074.pdf), [[Code]](https://github.com/VITA-Group/TransGAN)

- (arXiv 2021.02) END-TO-END **AUDIO-VISUAL SPEECH RECOGNITION** WITH CONFORMERS, [[Paper]](https://arxiv.org/pdf/2102.06657.pdf)

- (arXiv 2021.02) Is Space-Time Attention All You Need for **Video** Understanding? [[Paper]](https://arxiv.org/pdf/2102.05095.pdf), [[Code]](https://github.com/lucidrains/TimeSformer-pytorch)

- (arXiv 2021.02) Less is More: CLIPBERT for **Video-and-Language** Learning via Sparse Sampling, [[Paper]](https://arxiv.org/pdf/2102.06183.pdf), [[Code]](https://github.com/jayleicn/ClipBERT)

- (arXiv 2021.02) **Video** Transformer Network, [[Paper]](https://arxiv.org/pdf/2102.00719.pdf)

- (arXiv 2021.02) Training Vision Transformers for Image **Retrieval**, [[Paper]](https://arxiv.org/pdf/2102.05644.pdf)

- (arXiv 2021.02) Relaxed Transformer Decoders for Direct **Action Proposal Generation**, [[Paper]](https://arxiv.org/pdf/2102.01894.pdf), [[Code]](https://github.com/MCG-NJU/RTD-Action)

- (arXiv 2021.02) TransReID: Transformer-based Object **Re-Identification**, [[Paper]](https://arxiv.org/pdf/2102.04378.pdf)

- (arXiv 2021.02) Improving Visual **Reasoning** by Exploiting The Knowledge in Texts, [[Paper]](https://arxiv.org/pdf/2102.04760.pdf)

### 2021.01
- (arXiv 2021.01) **Fast** Convergence of DETR with Spatially Modulated Co-Attention, [[Paper]](https://arxiv.org/pdf/2101.07448.pdf)

- (arXiv 2021.01) Dual-Level Collaborative Transformer for Image **Captioning**, [[Paper]](https://arxiv.org/pdf/2101.06462.pdf)

- (arXiv 2021.01) SSTVOS: Sparse Spatiotemporal Transformers for Video Object **Segmentation** (arXiv 2021.1), [[Paper]](https://arxiv.org/pdf/2101.08833.pdf)

- (arXiv 2021.01) CPTR: FULL TRANSFORMER NETWORK FOR IMAGE **CAPTIONING**, [[Paper]](https://arxiv.org/pdf/2101.10804.pdf)

- (arXiv 2021.01) Trans2Seg: Transparent Object **Segmentation** with Transformer, [[Paper]](https://arxiv.org/pdf/2101.08461), [[Code]](https://github.com/xieenze/Trans2Seg)

- (arXiv 2021.01) Scheduled Sampling in **Vision-Language** Pretraining with Decoupled Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2101.11562.pdf), [[Code]](https://github.com/YehLi/TDEN)

- (arXiv 2021.01) Trear: Transformer-based RGB-D Egocentric **Action Recognition**, [[Paper]](https://arxiv.org/pdf/2101.03904.pdf)

- (arXiv 2021.01) Learn to Dance with AIST++: Music Conditioned **3D Dance Generation**, [[Paper]](https://arxiv.org/pdf/2101.08779), [[Page]](https://google.github.io/aichoreographer/;)

- (arXiv 2021.01) Spherical Transformer: Adapting **Spherical** Signal to CNNs, [[Paper]](https://arxiv.org/pdf/2101.03848.pdf)

- (arXiv 2021.01) Are We There Yet? Learning to **Localize** in Embodied Instruction Following, [[Paper]](https://arxiv.org/pdf/2101.03431.pdf)

- (arXiv 2021.01) VinVL: Making Visual Representations Matter in **Vision-Language** Models, [[Paper]](https://arxiv.org/pdf/2101.00529.pdf)

- (arXiv 2021.01) Bottleneck Transformers for Visual **Recognition**, [[Paper]](https://arxiv.org/pdf/2101.11605.pdf)

- (arXiv 2021.01) Investigating the Vision Transformer Model for Image **Retrieval** Tasks, [[Paper]](https://arxiv.org/pdf/2101.03771)

- (arXiv 2021.01) ADDRESSING SOME LIMITATIONS OF TRANSFORMERS WITH **FEEDBACK MEMORY**, [[Paper]](https://arxiv.org/pdf/2002.09402.pdf)

- (arXiv 2021.01) Tokens-to-Token ViT: **Training** Vision Transformers from Scratch on ImageNet, [[Paper]](https://arxiv.org/pdf/2101.11986.pdf), [[Code]](https://github.com/yitu-opensource/T2T-ViT)

- (arXiv 2021.01) TrackFormer: Multi-Object **Tracking** with Transformers, [[Paper]](https://arxiv.org/pdf/2101.02702)

- (arXiv 2021.01) VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale **Text-to-Image Search**, [[Paper]](https://arxiv.org/pdf/2101.00265)

- (arXiv 2021.01) **Line Segment Detection** Using Transformers without Edges, [[Paper]](https://arxiv.org/pdf/2101.01909)

- (arXiv 2021.01) Decoupling the Role of Data, Attention, and Losses in **Multimodal** Transformers, [[Paper]](https://arxiv.org/pdf/2102.00529.pdf)

### 2020.12
- (arXiv 2020.12) **Cloud** Transformers, [[Paper]](https://arxiv.org/pdf/2007.11679.pdf)

- (arXiv 2020.12) Accurate Word **Representations** with Universal Visual Guidance, [[Paper]](https://arxiv.org/pdf/2012.15086.pdf)

- (arXiv 2020.12) DETR for **Pedestrian Detection**, [[Paper]](https://arxiv.org/pdf/2012.06785)

- (arXiv 2020.12) Transformer **Interpretability** Beyond Attention Visualization, [[Paper]](https://arxiv.org/pdf/2012.09838), [[Code]](https://github.com/hila-chefer/Transformer-Explainability)

- (arXiv 2020.12) PCT: **Point Cloud** Transformer, [[Paper]](https://arxiv.org/pdf/2012.09688)

- (arXiv 2020.12) TransPose: Towards Explainable Human **Pose** Estimation by Transformer, [[Paper]](https://arxiv.org/pdf/2012.14214)

- (arXiv 2020.12) Rethinking Semantic **Segmentation** from a Sequence-to-Sequence Perspective with Transformers, [[Paper]](https://arxiv.org/pdf/2012.15840), [[Code]](https://github.com/fudan-zvg/SETR)

- (arXiv 2020.12) Transformer Guided Geometry Model for Flow-Based Unsupervised **Visual Odometry**, [[Paper]](https://arxiv.org/pdf/2101.02143)

- (arXiv 2020.12) Transformer for **Image Quality Assessment**, [[Paper]](https://arxiv.org/pdf/2101.01097), [[Code]](https://github.com/junyongyou/triq)

- (arXiv 2020.12) TransTrack: Multiple-Object **Tracking** with Transformer, [[Paper]](https://arxiv.org/pdf/2012.15460), [[Code]](https://github.com/PeizeSun/TransTrack)

- (arXiv 2020.12) 3D Object **Detection** with Pointformer, [[Paper]](https://arxiv.org/pdf/2012.11409)

- (arXiv 2020.12) Training data-**efficient** image transformers & distillation through attention, [[Paper]](https://arxiv.org/pdf/2012.12877)

- (arXiv 2020.12) Toward Transformer-Based Object **Detection**, [[Paper]](https://arxiv.org/pdf/2012.09958)

- (arXiv 2020.12) SceneFormer: **Indoor Scene Generation** with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09793)

- (arXiv 2020.12) **Point** Transformer, [[Paper]](https://arxiv.org/pdf/2012.09164)

- (arXiv 2020.12) End-to-End **Human Pose and Mesh Reconstruction** with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09760)

- (arXiv 2020.12) Informer: Beyond Efficient Transformer for Long Sequence Time-Series **Forecasting**, [[Paper]](https://arxiv.org/pdf/2012.07436.pdf)

- (arXiv 2020.12) Pre-Trained **Image Processing** Transformer, [[Paper]](https://arxiv.org/pdf/2012.00364)

- (arXiv 2020.12) Taming Transformers for High-Resolution **Image Synthesis**, [[Paper]](https://arxiv.org/pdf/2012.09841.pdf), [[Code]](https://github.com/CompVis/taming-transformers)

### 2020.11
- (arXiv 2020.11) End-to-end **Lane Shape Prediction** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.04233), [[Code]](https://github.com/liuruijin17/LSTR)

- (arXiv 2020.11) UP-DETR: Unsupervised Pre-training for Object **Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.09094)

- (arXiv 2020.11) End-to-End Video Instance **Segmentation** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14503)

- (arXiv 2020.11) Rethinking Transformer-based Set Prediction for Object **Detection**, [[Paper]](https://arxiv.org/pdf/2011.10881)

- (arXiv 2020.11) General Multi-label Image **Classification** with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14027}

- (arXiv 2020.11) End-to-End Object **Detection** with Adaptive Clustering Transformer, [[Paper]](https://arxiv.org/pdf/2011.09315)

### before 2020.11
- (arXiv 2020.10) An Image is Worth 16x16 Words: Transformers for **Image Recognition** at Scale, [[Paper]](https://arxiv.org/pdf/2010.11929), [[Code]](https://github.com/google-research/vision_transformer)

- (arXiv 2020.07) Oscar: Object-Semantics Aligned Pre-training for **Vision-and-Language** Tasks, [[Paper]](https://arxiv.org/pdf/2004.06165.pdf), [[Code]](https://github.com/microsoft/Oscar)

- (arXiv 2020.07) Feature **Pyramid** Transformer, [[Paper]](https://arxiv.org/pdf/2007.09451), [[Code]](https://github.com/ZHANGDONG-NJUST/FPT)

- (arXiv 2020.06) Linformer: Self-Attention with **Linear Complexity**, [[Paper]](https://arxiv.org/pdf/2006.04768.pdf)

- (arXiv 2020.06) Visual Transformers: Token-based **Image Representation and Processing** for Computer Vision, [[Paper]](https://arxiv.org/pdf/2006.03677)

- (arXiv 2019.08) LXMERT: Learning **Cross-Modality** Encoder Representations from Transformers, [[Paper]](https://arxiv.org/pdf/1908.07490.pdf), [[Code]](https://github.com/airsplay/lxmert)

- (ICLR'21) IOT: INSTANCE-WISE LAYER REORDERING FOR TRANSFORMER **STRUCTURES**, [[Paper]](https://arxiv.org/pdf/2103.03457.pdf), [[Code]](https://github.com/instance-wise-ordered-transformer/IOT)

- (ICLR'21) UPDET: UNIVERSAL MULTI-AGENT **REINFORCEMENT LEARNING** VIA POLICY DECOUPLING WITH TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2101.08001.pdf), [[Code]](https://github.com/hhhusiyi-monash/UPDeT)

- (ICLR'21) Deformable DETR: Deformable Transformers for End-to-End **Object Detection**, [[Paper]](https://arxiv.org/pdf/2010.04159), [[Code]](https://github.com/fundamentalvision/Deformable-DETR)

- (ICLR'21) LAMBDANETWORKS: MODELING **LONG-RANGE INTERACTIONS** WITHOUT ATTENTION, [[Paper]](https://openreview.net/pdf?id=xTJEN-ggl1b), [[Code]](https://github.com/lucidrains/lambda-networks)

- (ICLR'21) SUPPORT-SET BOTTLENECKS FOR **VIDEO-TEXT REPRESENTATION** LEARNING, [[Paper]](https://arxiv.org/pdf/2010.02824.pdf)

- (ICLR'21) **COLORIZATION** TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2102.04432.pdf), [[Code]](https://github.com/google-research/google-research/tree/master/coltran)

- (ECCV'20) Multi-modal Transformer for **Video Retrieval**, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490205.pdf)

- (ECCV'20) Connecting **Vision and Language** with Localized Narratives, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500630.pdf)

- (ECCV'20) DETR: End-to-End **Object Detection** with Transformers, [[Paper]](https://arxiv.org/pdf/2005.12872), [[Code]](https://github.com/facebookresearch/detr)

- (CVPR'20) PaStaNet: Toward **Human Activity** Knowledge Engine, [[Paper]](https://arxiv.org/pdf/2004.00945.pdf), [[Code]](https://github.com/DirtyHarryLYL/HAKE-Action)

- (CVPR'20) Multi-Modality Cross Attention Network for **Image and Sentence Matching**, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Multi-Modality_Cross_Attention_Network_for_Image_and_Sentence_Matching_CVPR_2020_paper.pdf), [[Page]](https://www.robots.ox.ac.uk/~vgg/research/speech2action/)

- (CVPR'20) Learning Texture Transformer Network for **Image Super-Resolution**, [[Paper]](https://arxiv.org/pdf/2006.04139), [[Code]](https://github.com/researchmm/TTSR)

- (CVPR'20) Speech2Action: Cross-modal Supervision for **Action Recognition**, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagrani_Speech2Action_Cross-Modal_Supervision_for_Action_Recognition_CVPR_2020_paper.pdf)

- (ICPR'20) Transformer Encoder **Reasoning** Network, [[Paper]](https://arxiv.org/pdf/2004.09144.pdf), [[Code]](https://github.com/mesnico/TERN)

- (EMNLP'19) Effective Use of Transformer Networks for **Entity Tracking**, [[Paper]](https://arxiv.org/pdf/1909.02635), [[Code]](https://github.com/aditya2211/transformer-entity-tracking)

## TODO
- [ ] V-L representation learning (https://arxiv.org/pdf/2103.16110.pdf has provided a detailed table)
