# Transformer-in-Vision
Recent Transformer-based CV and related works. Welcome to comment/contribute!

Keep update.

## Resource
- Attention is all you need, [[Paper]](https://arxiv.org/pdf/1706.03762.pdf)

- OpenAI CLIP [[Page]](https://openai.com/blog/clip/), [[Paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf), [[Code]](https://github.com/openai/CLIP), [[arXiv]](https://arxiv.org/pdf/2103.00020.pdf)

- OpenAI DALL·E [[Page]](https://openai.com/blog/dall-e/), [[Code]](https://github.com/openai/DALL-E), [[Paper]](https://arxiv.org/pdf/2102.12092.pdf)

- [huggingface/transformers](https://github.com/huggingface/transformers)

- [Kyubyong/transformer](https://github.com/Kyubyong/transformer), TF

- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch), Torch

- [krasserm/fairseq-image-captioning](https://github.com/krasserm/fairseq-image-captioning)

- [PyTorch Transformers Tutorials](https://github.com/abhimishra91/transformers-tutorials)

- [ictnlp/awesome-transformer](https://github.com/ictnlp/awesome-transformer)

- [basicv8vc/awesome-transformer](https://github.com/basicv8vc/awesome-transformer)

- [dk-liang/Awesome-Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer)

- [yuewang-cuhk/awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)

## Survery: 
- (arXiv 2021.03) A Practical Survey on Faster and Lighter Transformers, [[Paper]](https://arxiv.org/pdf/2103.14636.pdf)

- (arXiv 2021.03) Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision, [[Paper]](https://arxiv.org/pdf/2103.04037.pdf)

- (arXiv 2020.9) Efficient Transformers: A Survey, [[Paper]](https://arxiv.org/pdf/2009.06732.pdf)

- (arXiv 2020.1) Transformers in Vision: A Survey, [[Paper]](https://arxiv.org/pdf/2101.01169.pdf)

## Recent Papers
- (arXiv 2021.04) Going deeper with Image Transformers, [[Paper]](https://arxiv.org/pdf/2103.17239.pdf)

- (arXiv 2021.04) EFFICIENT PRE-TRAINING OBJECTIVES FOR TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2104.09694.pdf), [[Code]](https://github.com/iKernels/efficient-pre-training-objectives-for-transformers)

- (arXiv 2021.04) ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING, [[Paper]](https://arxiv.org/pdf/2104.09864.pdf)

- (arXiv 2021.04) VideoGPT: Video Generation using VQ-VAE and Transformers, [[Paper]](https://arxiv.org/pdf/2104.10157.pdf), [[Code]](https://wilson1yan.github.io/videogpt/index.html)

- (arXiv 2021.04) DODRIO: Exploring Transformer Models with Interactive Visualization, [[Paper]](https://arxiv.org/pdf/2103.14625.pdf), [[Code]](https://poloclub.github.io/dodrio/)

- (arXiv 2021.04) Lifting Transformer for 3D Human Pose Estimation in Video, [[Paper]](https://arxiv.org/pdf/2103.14304.pdf)

- (arXiv 2021.04) Demystifying the Better Performance of Position Encoding Variants for Transformer, [[Paper]](https://arxiv.org/pdf/2104.08698.pdf)

- (arXiv 2021.04) Consistent Accelerated Inference via Confident Adaptive Transformers, [[Paper]](https://arxiv.org/pdf/2104.08803.pdf), [[Code]](https://github.com/TalSchuster/CATs)

- (arXiv 2021.04) Temporal Query Networks for Fine-grained Video Understanding, [[Paper]](https://arxiv.org/pdf/2104.09496.pdf), [[Code]](http://www.robots.ox.ac.uk/~vgg/research/tqn/)

- (arXiv 2021.04) Face Transformer for Recognition, [[Paper]](https://arxiv.org/pdf/2103.14803.pdf), [[Code]](https://github.com/zhongyy/Face-Transformer)

- (arXiv 2021.04) VGNMN: Video-grounded Neural Module Network to Video-Grounded Language Tasks, [[Paper]](https://arxiv.org/pdf/2104.07921.pdf)

- (arXiv 2021.04) Self-supervised Video Retrieval Transformer Network, [[Paper]](https://arxiv.org/pdf/2104.07993.pdf)

- (arXiv 2021.04) Cross-Modal Retrieval Augmentation for Multi-Modal Classification, [[Paper]](https://arxiv.org/pdf/2104.08108.pdf)

- (arXiv 2021.04) Point-Based Modeling of Human Clothing, [[Paper]](https://arxiv.org/pdf/2104.08230.pdf)

- (arXiv 2021.04) Points as Queries: Weakly Semi-supervised Object Detection by Points, [[Paper]](https://arxiv.org/pdf/2104.07434.pdf)

- (arXiv 2021.04) Geometry-Free View Synthesis: Transformers and no 3D Priors, [[Paper]](https://arxiv.org/pdf/2104.07652.pdf), [[Code]](https://git.io/JOnwn)

- (arXiv 2021.04) Self-supervised Video Object Segmentation by Motion Grouping, [[Paper]](https://arxiv.org/pdf/2104.07658.pdf), [[Project]](https://charigyang.github.io/motiongroup/)

- (arXiv 2021.04) Decoupled Spatial-Temporal Transformer for Video Inpainting, [[Paper]](https://arxiv.org/pdf/2104.06637.pdf), [[Code]](https://github.com/ruiliu-ai/DSTT)

- (arXiv 2021.04) Pose Recognition with Cascade Transformers, [[Paper]](https://arxiv.org/pdf/2104.06976.pdf), [[Code]](https://github.com/mlpc-ucsd/PRTR)

- (arXiv 2021.04) Action-Conditioned 3D Human Motion Synthesis with Transformer VAE, [[Paper]](https://arxiv.org/pdf/2104.05670.pdf), [[Project]](https://imagine.enpc.fr/~petrovim/actor)

- (arXiv 2021.04) Escaping the Big Data Paradigm with Compact Transformers, [[Paper]](https://arxiv.org/pdf/2104.05704.pdf), [[Code]](https://github.com/SHI-Labs/Compact-Transformers)

- (arXiv 2021.04) Know What and Know Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation, [[Paper]](https://arxiv.org/pdf/2104.04167.pdf)

- (arXiv 2021.04) Handwriting Transformers, [[Paper]](https://arxiv.org/pdf/2104.03964.pdf)

- (arXiv 2021.04) SiT: Self-supervised vIsion Transformer, [[Paper]](https://arxiv.org/pdf/2104.03602.pdf)

- (arXiv 2021.04) EFFICIENT TRANSFORMERS IN REINFORCEMENT LEARNING USING ACTOR-LEARNER DISTILLATION, [[Paper]](https://arxiv.org/pdf/2104.01655.pdf)

- (arXiv 2021.04) Compressing Visual-linguistic Model via Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2104.02096.pdf)

- (arXiv 2021.04) When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes, [[Paper]](https://arxiv.org/pdf/2104.02215.pdf)

- (arXiv 2021.04) Variational Transformer Networks for Layout Generation, [[Paper]](https://arxiv.org/pdf/2104.02416.pdf)

- (arXiv 2021.04) Few-Shot Transformation of Common Actions into Time and Space, [[Paper]](https://arxiv.org/pdf/2104.02439.pdf)

- (arXiv 2021.04) Fourier Image Transformer, [[Paper]](https://arxiv.org/pdf/2104.02555.pdf)

- (arXiv 2021.04) Efficient DETR: Improving End-to-End Object Detector with Dense Prior, [[Paper]](https://arxiv.org/pdf/2104.01318.pdf)

- (arXiv 2021.04) A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person Re-identification, [[Paper]](https://arxiv.org/pdf/2104.01745.pdf)

- (arXiv 2021.04) An Empirical Study of Training Self-Supervised Visual Transformers, [[Paper]](https://arxiv.org/pdf/2104.02057.pdf)

- (arXiv 2021.04) Multitarget Tracking with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00734.pdf)

- (arXiv 2021.04) TFill: Image Completion via a Transformer-Based Architecture, [[Paper]](https://arxiv.org/pdf/2104.00845.pdf), [[Code]](https://github.com/lyndonzheng/TFill)

- (arXiv 2021.04) AAformer: Auto-Aligned Transformer for Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2104.00921.pdf)

- (arXiv 2021.04) VisQA: X-raying Vision and Language Reasoning in Transformers, [[Paper]](https://arxiv.org/pdf/2104.00926.pdf)

- (arXiv 2021.04) TubeR: Tube-Transformer for Action Detection, [[Paper]](https://arxiv.org/pdf/2104.00969.pdf)

- (arXiv 2021.04) Language-based Video Editing via Multi-Modal Multi-Level Transformer, [[Paper]](https://arxiv.org/pdf/2104.01122.pdf)

- (arXiv 2021.04) LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference, [[Paper]](https://arxiv.org/pdf/2104.01136.pdf)

- (arXiv 2021.04) LoFTR: Detector-Free Local Feature Matching with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00680.pdf), [[Code]](https://zju3dv.github.io/loftr/)

- (arXiv 2021.04) Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis, [[Paper]](https://arxiv.org/pdf/2104.00677.pdf), [[Project]](https://www.ajayj.com/dietnerf)

- (arXiv 2021.04) Group-Free 3D Object Detection via Transformers, [[Paper]](https://arxiv.org/pdf/2104.00678.pdf), [[Code]](https://github.com/zeliu98/Group-Free-3D)

- (arXiv 2021.04) Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval, [[Paper]](https://arxiv.org/pdf/2104.00650.pdf)

- (arXiv 2021.03) Learning Spatio-Temporal Transformer for Visual Tracking, [[Paper]](https://arxiv.org/pdf/2103.17154.pdf), [[Code]](https://github.com/researchmm/Stark)

- (arXiv 2021.03) StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery, [[Paper]](https://arxiv.org/pdf/2103.17249.pdf), [[Code]](https://github.com/orpatashnik/StyleCLIP)

- (arXiv 2021.03) Multimodal Motion Prediction with Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)

- (arXiv 2021.04) Composable Augmentation Encoding for Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2104.00616.pdf)

- (arXiv 2021.03) Describing and Localizing Multiple Changes with Transformers, [[Paper]](https://arxiv.org/pdf/2103.14146.pdf), [[Project]](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers)

- (arXiv 2021.03) COTR: Correspondence Transformer for Matching Across Images, [[Paper]](https://arxiv.org/pdf/2103.14167.pdf)

- (arXiv 2021.03) nderstanding Robustness of Transformers for Image Classification, [[Paper]](https://arxiv.org/pdf/2103.14586.pdf)

- (arXiv 2021.03) CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification, [[Paper]](https://arxiv.org/pdf/2103.14899.pdf)

- (arXiv 2021.03) Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2103.14829.pdf)

- (arXiv 2021.03) HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval, [[Paper]](https://arxiv.org/pdf/2103.15049.pdf)

- (arXiv 2021.03) TFPose: Direct Human Pose Estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2103.15320.pdf), [[Code]](https://git.io/AdelaiDet)

- (arXiv 2021.03) Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding, [[Paper]](https://arxiv.org/pdf/2103.15358.pdf)

- (arXiv 2021.03) Transformer Tracking, [[Paper]](https://arxiv.org/pdf/2103.15436.pdf), [[Code]](https://github.com/chenxin-dlut/TransT)

- (arXiv 2021.03) ViViT: A Video Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.15691.pdf)

- (arXiv 2021.03) CvT: Introducing Convolutions to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2103.15808.pdf), [[Code]](https://github.com/leoxiaobin/CvT)

- (arXiv 2021.03) Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers, [[Paper]](https://arxiv.org/pdf/2103.15679.pdf), [[Code]](https://github.com/hila-chefer/Transformer-MM-Explainability)

- (arXiv 2021.03) On the Adversarial Robustness of Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.15670.pdf)

- (arXiv 2021.03) Rethinking Spatial Dimensions of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2103.16302.pdf), [[Code]](https://github.com/naver-ai/pit)

- (arXiv 2021.03) Spatiotemporal Transformer for Video-based Person Re-identification, [[Paper]](https://arxiv.org/pdf/2103.16469.pdf)

- (arXiv 2021.03) Read and Attend: Temporal Localisation in Sign Language Videos, [[Paper]](https://arxiv.org/pdf/2103.16481.pdf), [[Benchmark]](https://github.com/visipedia/newt)

- (arXiv 2021.03) Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers, [[Paper]](https://arxiv.org/pdf/2103.16553.pdf)

- (arXiv 2021.03) An Image is Worth 16x16 Words, What is a Video Worth? [[Paper]](https://arxiv.org/pdf/2103.13915.pdf)

- (arXiv 2021.03) High-Fidelity Pluralistic Image Completion with Transformers, [[Paper]](https://arxiv.org/pdf/2103.14031.pdf), [[Code]](http://raywzy.com/ICT)

- (arXiv 2021.03) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, [[Paper]](https://arxiv.org/pdf/2103.14030.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer)

- (arXiv 2021.03) Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2103.13061.pdf), [[Code]](https://github.com/amzn/image-to-recipe-transformers)

- (arXiv 2021.03) Multi-view 3D Reconstruction with Transformer, [[Paper]](https://arxiv.org/pdf/2103.12957.pdf)

- (arXiv 2021.03) Scene-Intuitive Agent for Remote Embodied Visual Grounding, [[Paper]](https://arxiv.org/pdf/2103.12944.pdf)

- (arXiv 2021.03) Can Vision Transformers Learn without Natural Images? [[Paper]](https://arxiv.org/pdf/2103.13023.pdf)

- (arXiv 2021.03) On the Robustness of Vision Transformers to Adversarial Examples, [[Paper]](https://arxiv.org/pdf/2104.02610.pdf)

- (arXiv 2021.03) Kaleido-BERT: Vision-Language Pre-training on Fashion Domain, [[Paper]](https://arxiv.org/pdf/2103.16110.pdf), [[Code]](http://dpfan.net/Kaleido-BERT)

- (arXiv 2021.03) End-to-End Trainable Multi-Instance Pose Estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2103.12115.pdf)

- (arXiv 2021.03) Transformers Solve the Limited Receptive Field for Monocular Depth Prediction, [[Paper]](https://arxiv.org/pdf/2103.12091.pdf), [[Code]](https://github.com/ygjwd12345/TransDepth)

- (arXiv 2021.03) Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning, [[Paper]](https://arxiv.org/pdf/2103.11731.pdf)

- (arXiv 2021.03) Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking, [[Paper]](https://arxiv.org/pdf/2103.11681.pdf), [[Code]](https://github.com/594422814/TransformerTrack)

- (arXiv 2021.03) DeepViT: Towards Deeper Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.11886.pdf), [[Code]](https://github.com/zhoudaquan/dvit_repo)

- (arXiv 2021.03) Incorporating Convolution Designs into Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.11816.pdf)

- (arXiv 2021.03) Multimodal Motion Prediction with Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)

- (arXiv 2021.03) MaAST: Map Attention with Semantic Transformers for Efficient Visual Navigation, [[Paper]](https://arxiv.org/pdf/2103.11374.pdf)

- (arXiv 2021.03) Paying Attention to Multiscale Feature Maps in Multimodal Image Matching, [[Paper]](https://arxiv.org/pdf/2103.11247.pdf)

- (arXiv 2021.03) Learning Multi-Scene Absolute Pose Regression with Transformers, [[Paper]](https://arxiv.org/pdf/2103.11468.pdf)

- (arXiv 2021.03) HOPPER: MULTI-HOP TRANSFORMER FOR SPATIOTEMPORAL REASONING, [[Paper]](https://arxiv.org/pdf/2103.10574.pdf), [[Code]](https://github.com/necla-ml/cater-h)

- (arXiv 2021.03) Scalable Visual Transformers with Hierarchical Pooling, [[Paper]](https://arxiv.org/pdf/2103.10619.pdf)

- (arXiv 2021.03) AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting, [[Paper]](https://arxiv.org/pdf/2103.14023.pdf), [[Code]](https://github.com/Khrylx/AgentFormer)

- (arXiv 2021.03) Vision Transformers for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2103.13413.pdf), [[Code]](https://github.com/intel-isl/DPT)

- (arXiv 2021.03) 3D Human Pose Estimation with Spatial and Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2103.10455.pdf), [[Code]](https://github.com/zczcwh/PoseFormer)

- (arXiv 2021.03) ConViT: Improving Vision Transformers ith Soft Convolutional Inductive Biases, [[Paper]](https://arxiv.org/pdf/2103.10697.pdf), [[Code]](https://github.com/facebookresearch/convit)

- (arXiv 2021.03) MDMMT: Multidomain Multimodal Transformer for Video Retrieval, [[Paper]](https://arxiv.org/pdf/2103.10699.pdf)

- (arXiv 2021.03) On the Sentence Embeddings from Pre-trained Language Models, [[Paper]](https://arxiv.org/pdf/2011.05864.pdf)

- (arXiv 2021.03) Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training, [[Paper]](https://arxiv.org/pdf/2103.10043.pdf)

- (arXiv 2021.03) DanceNet3D: Music Based Dance Generation with Parametric Motion Transformer, [[Paper]](https://arxiv.org/pdf/2103.10206.pdf)

- (arXiv 2021.03) Decoupled Spatial Temporal Graphs for Generic Visual Grounding, [[Paper]](https://arxiv.org/pdf/2103.10191.pdf)

- (arXiv 2021.03) Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2103.10211.pdf)

- (arXiv 2021.03) Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2103.08849.pdf), [[Code]](http://github.com/berniebear/Mutli-HT100M)

- (arXiv 2021.03) TransFG: A Transformer Architecture for Fine-grained Recognition, [[Paper]](https://arxiv.org/pdf/2103.07976.pdf)

- (arXiv 2021.03) Causal Attention for Vision-Language Tasks, [[Paper]](https://arxiv.org/pdf/2103.03493.pdf), [[Code]](https://github.com/yangxuntu/catt)

- (arXiv 2021.03) Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks, [[Paper]](https://arxiv.org/pdf/2103.06982.pdf)

- (arXiv 2021.03) WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training, [[Paper]](https://arxiv.org/pdf/2103.06561.pdf)

- (arXiv 2021.03) Attention is not all you need: pure attention loses rank doubly exponentially with depth, [[Paper]](https://arxiv.org/pdf/2103.03404v1.pdf)

- (arXiv 2021.03) QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information, [[Paper]](https://arxiv.org/pdf/2103.05399), [[Code]](https://github.com/hitachi-rd-cv/qpic)

- (arXiv 2021.03) Reformulating HOI Detection as Adaptive Set Prediction, [[Paper]](https://arxiv.org/pdf/2103.05983), [[Code]](https://github.com/yoyomimi/AS-Net)

- (arXiv 2021.03) End-to-End Human Object Interaction Detection with HOI Transformer, [[Paper]](https://arxiv.org/pdf/2103.04503), [[Code]](https://github.com/bbepoch/HoiTransformer)

- (arXiv 2021.03) Perceiver: General Perception with Iterative Attention, [[Paper]](https://arxiv.org/pdf/2103.03206.pdf)

- (arXiv 2021.03) Transformer in Transformer, [[Paper]](https://arxiv.org/pdf/2103.00112.pdf), [[Code]](https://github.com/huawei-noah/noah-research/tree/master/TNT)

- (arXiv 2021.03) Generative Adversarial Transformers, [[Paper]](https://arxiv.org/pdf/2103.01209.pdf), [[Code]](https://github.com/dorarad/gansformer)

- (arXiv 2021.03) OmniNet: Omnidirectional Representations from Transformers, [[Paper]](https://arxiv.org/pdf/2103.01075.pdf)

- (arXiv 2021.03) Single-Shot Motion Completion with Transformer, [[Paper]](https://arxiv.org/pdf/2103.00776.pdf), [[Code]](https://github.com/FuxiCV/SSMCT)

- (arXiv 2021.02) Evolving Attention with Residual Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12895.pdf)

- (arXiv 2021.02) GEM: Glare or Gloom, I Can Still See You – End-to-End Multimodal Object Detector, [[Paper]](https://arxiv.org/pdf/2102.12319.pdf)

- (arXiv 2021.02) SparseBERT: Rethinking the Importance Analysis in Self-attention, [[Paper]](https://arxiv.org/pdf/2102.12871.pdf)

- (arXiv 2021.02) Investigating the Limitations of Transformers with Simple Arithmetic Tasks, [[Paper]](https://arxiv.org/pdf/2102.13019.pdf), [[Code]](https://github.com/castorini/transformers-arithmetic)

- (arXiv 2021.02) Do Transformer Modifications Transfer Across Implementations and Applications? [[Paper]](https://arxiv.org/pdf/2102.11972.pdf)

- (arXiv.2021.02) Do We Really Need Explicit Position Encodings for Vision Transformers? [[Paper]](https://arxiv.org/pdf/2102.10882.pdf), [[Code]](https://github.com/Meituan-AutoML/CPVT)

- (arXiv.2021.02) A Straightforward Framework For Video Retrieval Using CLIP, [[Paper]](https://arxiv.org/pdf/2102.12443.pdf), [[Code]](https://github.com/Deferf/CLIP_Video_Representation)

- (arXiv.2021.02) Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12122.pdf), [[Code]](https://github.com/whai362/PVT)

- (arXiv.2021.02) VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining, [[Paper]](https://arxiv.org/pdf/2102.10407.pdf), [[Code]](https://github.com/Vision-CAIR/VisualGPT)

- (arXiv.2021.02) Towards Accurate and Compact Architectures via Neural Architecture Transformer, [[Paper]](https://arxiv.org/pdf/2102.10301.pdf)

- (arXiv.2021.02) Centroid Transformer: Learning to Abstract with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) Linear Transformers Are Secretly Fast Weight Memory Systems, [[Paper]](https://arxiv.org/pdf/2102.11174.pdf)

- (arXiv.2021.02) POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW, [[Paper]](https://arxiv.org/pdf/2102.11090.pdf)

- (arXiv 2021.02) Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer, [[Paper]](https://arxiv.org/pdf/2102.10772.pdf), [[Project]](https://mmf.sh/), [[Code]](https://github.com/facebookresearch/mmf)

- (arXiv 2021.02) Centroid Transformer: Learning to Abstract with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts, [[Paper]](https://arxiv.org/pdf/2102.08981.pdf)

- (arXiv 2021.02) TransGAN: Two Transformers Can Make One Strong GAN, [[Paper]](https://arxiv.org/pdf/2102.07074.pdf), [[Code]](https://github.com/VITA-Group/TransGAN)

- (arXiv 2021.02) END-TO-END AUDIO-VISUAL SPEECH RECOGNITION WITH CONFORMERS, [[Paper]](https://arxiv.org/pdf/2102.06657.pdf)

- (arXiv 2021.02) Is Space-Time Attention All You Need for Video Understanding? [[Paper]](https://arxiv.org/pdf/2102.05095.pdf), [[Code]](https://github.com/lucidrains/TimeSformer-pytorch)

- (arXiv 2021.02) Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling, [[Paper]](https://arxiv.org/pdf/2102.06183.pdf), [[Code]](https://github.com/jayleicn/ClipBERT)

- (arXiv 2021.02) Video Transformer Network, [[Paper]](https://arxiv.org/pdf/2102.00719.pdf)

- (arXiv 2021.02) Training Vision Transformers for Image Retrieval, [[Paper]](https://arxiv.org/pdf/2102.05644.pdf)

- (arXiv 2021.02) Relaxed Transformer Decoders for Direct Action Proposal Generation, [[Paper]](https://arxiv.org/pdf/2102.01894.pdf), [[Code]](https://github.com/MCG-NJU/RTD-Action)

- (arXiv 2021.02) TransReID: Transformer-based Object Re-Identification, [[Paper]](https://arxiv.org/pdf/2102.04378.pdf)

- (arXiv 2021.02) Improving Visual Reasoning by Exploiting The Knowledge in Texts, [[Paper]](https://arxiv.org/pdf/2102.04760.pdf)

- (arXiv 2021.01) Fast Convergence of DETR with Spatially Modulated Co-Attention, [[Paper]](https://arxiv.org/pdf/2101.07448.pdf)

- (arXiv 2021.01) Dual-Level Collaborative Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2101.06462.pdf)

- (arXiv 2021.01) SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation (arXiv 2021.1), [[Paper]](https://arxiv.org/pdf/2101.08833.pdf)

- (arXiv 2021.01) CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING, [[Paper]](https://arxiv.org/pdf/2101.10804.pdf)

- (arXiv 2021.01) Trans2Seg: Transparent Object Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2101.08461), [[Code]](https://github.com/xieenze/Trans2Seg)

- (arXiv 2021.01) Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2101.11562.pdf), [[Code]](https://github.com/YehLi/TDEN)

- (arXiv 2021.01) Trear: Transformer-based RGB-D Egocentric Action Recognition, [[Paper]](https://arxiv.org/pdf/2101.03904.pdf)

- (arXiv 2021.01) Learn to Dance with AIST++: Music Conditioned 3D Dance Generation, [[Paper]](https://arxiv.org/pdf/2101.08779), [[Page]](https://google.github.io/aichoreographer/;)

- (arXiv 2021.01) Spherical Transformer: Adapting Spherical Signal to CNNs, [[Paper]](https://arxiv.org/pdf/2101.03848.pdf)

- (arXiv 2021.01) Are We There Yet? Learning to Localize in Embodied Instruction Following, [[Paper]](https://arxiv.org/pdf/2101.03431.pdf)

- (arXiv 2021.01) VinVL: Making Visual Representations Matter in Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2101.00529.pdf)

- (arXiv 2021.01) Bottleneck Transformers for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2101.11605.pdf)

- (arXiv 2021.01) Investigating the Vision Transformer Model for Image Retrieval Tasks, [[Paper]](https://arxiv.org/pdf/2101.03771)

- (arXiv 2021.01) ADDRESSING SOME LIMITATIONS OF TRANSFORMERS WITH FEEDBACK MEMORY, [[Paper]](https://arxiv.org/pdf/2002.09402.pdf)

- (arXiv 2021.01) Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, [[Paper]](https://arxiv.org/pdf/2101.11986.pdf), [[Code]](https://github.com/yitu-opensource/T2T-ViT)

- (arXiv 2021.01) TrackFormer: Multi-Object Tracking with Transformers, [[Paper]](https://arxiv.org/pdf/2101.02702)

- (arXiv 2021.01) VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search, [[Paper]](https://arxiv.org/pdf/2101.00265)

- (arXiv 2021.01) Line Segment Detection Using Transformers without Edges, [[Paper]](https://arxiv.org/pdf/2101.01909)

- (arXiv 2021.01) Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2102.00529.pdf)

- (arXiv 2020.12) Cloud Transformers, [[Paper]](https://arxiv.org/pdf/2007.11679.pdf)

- (arXiv 2020.12) Accurate Word Representations with Universal Visual Guidance, [[Paper]](https://arxiv.org/pdf/2012.15086.pdf)

- (arXiv 2020.12) DETR for Pedestrian Detection, [[Paper]](https://arxiv.org/pdf/2012.06785)

- (arXiv 2020.12) Transformer Interpretability Beyond Attention Visualization, [[Paper]](https://arxiv.org/pdf/2012.09838), [[Code]](https://github.com/hila-chefer/Transformer-Explainability)

- (arXiv 2020.12) PCT: Point Cloud Transformer, [[Paper]](https://arxiv.org/pdf/2012.09688)

- (arXiv 2020.12) TransPose: Towards Explainable Human Pose Estimation by Transformer, [[Paper]](https://arxiv.org/pdf/2012.14214)

- (arXiv 2020.12) Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers, [[Paper]](https://arxiv.org/pdf/2012.15840), [[Code]](https://github.com/fudan-zvg/SETR)

- (arXiv 2020.12) Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry, [[Paper]](https://arxiv.org/pdf/2101.02143)

- (arXiv 2020.12) Transformer for Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2101.01097), [[Code]](https://github.com/junyongyou/triq)

- (arXiv 2020.12) TransTrack: Multiple-Object Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2012.15460), [[Code]](https://github.com/PeizeSun/TransTrack)

- (arXiv 2020.12) 3D Object Detection with Pointformer, [[Paper]](https://arxiv.org/pdf/2012.11409)

- (arXiv 2020.12) Training data-efficient image transformers & distillation through attention, [[Paper]](https://arxiv.org/pdf/2012.12877)

- (arXiv 2020.12) Toward Transformer-Based Object Detection, [[Paper]](https://arxiv.org/pdf/2012.09958)

- (arXiv 2020.12) SceneFormer: Indoor Scene Generation with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09793)

- (arXiv 2020.12) Point Transformer, [[Paper]](https://arxiv.org/pdf/2012.09164)

- (arXiv 2020.12) End-to-End Human Pose and Mesh Reconstruction with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09760)

- (arXiv 2020.12) Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting, [[Paper]](https://arxiv.org/pdf/2012.07436.pdf)

- (arXiv 2020.12) Pre-Trained Image Processing Transformer, [[Paper]](https://arxiv.org/pdf/2012.00364)

- (arXiv 2020.12) Taming Transformers for High-Resolution Image Synthesis, [[Paper]](https://arxiv.org/pdf/2012.09841.pdf), [[Code]](https://github.com/CompVis/taming-transformers)

- (arXiv 2020.11) End-to-end Lane Shape Prediction with Transformers, [[Paper]](https://arxiv.org/pdf/2011.04233), [[Code]](https://github.com/liuruijin17/LSTR)

- (arXiv 2020.11) UP-DETR: Unsupervised Pre-training for Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2011.09094)

- (arXiv 2020.11) End-to-End Video Instance Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14503)

- (arXiv 2020.11) Rethinking Transformer-based Set Prediction for Object Detection, [[Paper]](https://arxiv.org/pdf/2011.10881)

- (arXiv 2020.11) General Multi-label Image Classification with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14027}

- (arXiv 2020.11) End-to-End Object Detection with Adaptive Clustering Transformer, [[Paper]](https://arxiv.org/pdf/2011.09315)

- (arXiv 2020.10) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, [[Paper]](https://arxiv.org/pdf/2010.11929), [[Code]](https://github.com/google-research/vision_transformer)

- (arXiv 2020.07) Oscar: Object-Semantics Aligned Pre-training for Vision-and-Language Tasks, [[Paper]](https://arxiv.org/pdf/2004.06165.pdf), [[Code]](https://github.com/microsoft/Oscar)

- (arXiv 2020.07) Feature Pyramid Transformer, [[Paper]](https://arxiv.org/pdf/2007.09451), [[Code]](https://github.com/ZHANGDONG-NJUST/FPT)

- (arXiv 2020.06) Linformer: Self-Attention with Linear Complexity, [[Paper]](https://arxiv.org/pdf/2006.04768.pdf)

- (arXiv 2020.06) Visual Transformers: Token-based Image Representation and Processing for Computer Vision, [[Paper]](https://arxiv.org/pdf/2006.03677)

- (arXiv 2019.08) LXMERT: Learning Cross-Modality Encoder Representations from Transformers, [[Paper]](https://arxiv.org/pdf/1908.07490.pdf), [[Code]](https://github.com/airsplay/lxmert)

- (ICLR'21) IOT: INSTANCE-WISE LAYER REORDERING FOR TRANSFORMER STRUCTURES, [[Paper]](https://arxiv.org/pdf/2103.03457.pdf), [[Code]](https://github.com/instance-wise-ordered-transformer/IOT)

- (ICLR'21) UPDET: UNIVERSAL MULTI-AGENT REINFORCEMENT LEARNING VIA POLICY DECOUPLING WITH TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2101.08001.pdf), [[Code]](https://github.com/hhhusiyi-monash/UPDeT)

- (ICLR'21) Deformable DETR: Deformable Transformers for End-to-End Object Detection, [[Paper]](https://arxiv.org/pdf/2010.04159), [[Code]](https://github.com/fundamentalvision/Deformable-DETR)

- (ICLR'21) LAMBDANETWORKS: MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION, [[Paper]](https://openreview.net/pdf?id=xTJEN-ggl1b), [[Code]](https://github.com/lucidrains/lambda-networks)

- (ICLR'21) SUPPORT-SET BOTTLENECKS FOR VIDEO-TEXT REPRESENTATION LEARNING, [[Paper]](https://arxiv.org/pdf/2010.02824.pdf)

- (ICLR'21) COLORIZATION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2102.04432.pdf), [[Code]](https://github.com/google-research/google-research/tree/master/coltran)

- (ECCV'20) Multi-modal Transformer for Video Retrieval, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490205.pdf)

- (ECCV'20) Connecting Vision and Language with Localized Narratives, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500630.pdf)

- (ECCV'20) DETR: End-to-End Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2005.12872), [[Code]](https://github.com/facebookresearch/detr)

- (CVPR'20) PaStaNet: Toward Human Activity Knowledge Engine, [[Paper]](https://arxiv.org/pdf/2004.00945.pdf), [[Code]](https://github.com/DirtyHarryLYL/HAKE-Action)

- (CVPR'20) Multi-Modality Cross Attention Network for Image and Sentence Matching, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Multi-Modality_Cross_Attention_Network_for_Image_and_Sentence_Matching_CVPR_2020_paper.pdf), [[Page]](https://www.robots.ox.ac.uk/~vgg/research/speech2action/)

- (CVPR'20) Learning Texture Transformer Network for Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2006.04139), [[Code]](https://github.com/researchmm/TTSR)

- (CVPR'20) Speech2Action: Cross-modal Supervision for Action Recognition, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagrani_Speech2Action_Cross-Modal_Supervision_for_Action_Recognition_CVPR_2020_paper.pdf)

- (ICPR'20) Transformer Encoder Reasoning Network, [[Paper]](https://arxiv.org/pdf/2004.09144.pdf), [[Code]](https://github.com/mesnico/TERN)

- (EMNLP'19) Effective Use of Transformer Networks for Entity Tracking, [[Paper]](https://arxiv.org/pdf/1909.02635), [[Code]](https://github.com/aditya2211/transformer-entity-tracking)

## TODO
- [ ] V-L representation learning (https://arxiv.org/pdf/2103.16110.pdf has provided a detailed table)
