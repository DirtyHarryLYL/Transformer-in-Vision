# Transformer-in-Vision
Some recent Transformer-based CV works. Welcome to comment/contribute!

Updating.

## Resource
- Attention is all you need, [[Paper]](https://arxiv.org/pdf/1706.03762.pdf)

- OpenAI CLIP [[Page]](https://openai.com/blog/clip/), [[Paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf), [[Code]](https://github.com/openai/CLIP), [[arXiv]](https://arxiv.org/pdf/2103.00020.pdf)

- OpenAI DALLÂ·E [[Page]](https://openai.com/blog/dall-e/), [[Code]](https://github.com/openai/DALL-E), [[Paper]](https://arxiv.org/pdf/2102.12092.pdf)

- [huggingface/transformers](https://github.com/huggingface/transformers)

- [Kyubyong/transformer](https://github.com/Kyubyong/transformer), TF

- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch), Torch

- [krasserm/fairseq-image-captioning](https://github.com/krasserm/fairseq-image-captioning)

- [PyTorch Transformers Tutorials](https://github.com/abhimishra91/transformers-tutorials)

- [ictnlp/awesome-transformer](https://github.com/ictnlp/awesome-transformer)

- [basicv8vc/awesome-transformer](https://github.com/basicv8vc/awesome-transformer)

- [dk-liang/Awesome-Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer)

- [yuewang-cuhk/awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)

## Survery: 
- (arXiv 2020.9) Efficient Transformers: A Survey, [PDF](https://arxiv.org/pdf/2009.06732.pdf)

- (arXiv 2020.1) Transformers in Vision: A Survey, [PDF](https://arxiv.org/pdf/2101.01169.pdf)

## Recent Papers

- (arXiv 2021.03) Perceiver: General Perception with Iterative Attention, [[Paper]](https://arxiv.org/pdf/2103.03206.pdf)

- (arXiv 2021.03) Transformer in Transformer, [[Paper]](https://arxiv.org/pdf/2103.00112.pdf), [[Code]](https://github.com/huawei-noah/noah-research/tree/master/TNT)

- (arXiv 2021.03) Generative Adversarial Transformers, [[Paper]](https://arxiv.org/pdf/2103.01209.pdf), [[Code]](https://github.com/dorarad/gansformer)

- (arXiv 2021.03) OmniNet: Omnidirectional Representations from Transformers, [[Paper]](https://arxiv.org/pdf/2103.01075.pdf)

- (arXiv 2021.03) Single-Shot Motion Completion with Transformer, [[Paper]](https://arxiv.org/pdf/2103.00776.pdf), [[Code]](https://github.com/FuxiCV/SSMCT)

- (arXiv.2021.02) Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12122.pdf), [[Code]](https://github.com/whai362/PVT)

- (arXiv.2021.02) VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining, [[Paper]](https://arxiv.org/pdf/2102.10407.pdf), [[Code]](https://github.com/Vision-CAIR/VisualGPT)

- (arXiv.2021.02) Towards Accurate and Compact Architectures via Neural Architecture Transformer, [[Paper]](https://arxiv.org/pdf/2102.10301.pdf)

- (arXiv.2021.02) Centroid Transformer: Learning to Abstract with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) Linear Transformers Are Secretly Fast Weight Memory Systems, [[Paper]](https://arxiv.org/pdf/2102.11174.pdf)

- (arXiv.2021.02) POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW, [[Paper]](https://arxiv.org/pdf/2102.11090.pdf)

- (arXiv 2021.02) Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer, [[Paper]](https://arxiv.org/pdf/2102.10772.pdf), [[Project]](https://mmf.sh/), [[Code]](https://github.com/facebookresearch/mmf)

- (arXiv 2021.02) Centroid Transformer: Learning to Abstract with Attention, [[Paper]](https://arxiv.org/pdf/2102.08606.pdf)

- (arXiv 2021.02) Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts, [[Paper]](https://arxiv.org/pdf/2102.08981.pdf)

- (arXiv 2021.02) TransGAN: Two Transformers Can Make One Strong GAN, [[Paper]](https://arxiv.org/pdf/2102.07074.pdf), [[Code]](https://github.com/VITA-Group/TransGAN)

- (arXiv 2021.02) END-TO-END AUDIO-VISUAL SPEECH RECOGNITION WITH CONFORMERS, [[Paper]](https://arxiv.org/pdf/2102.06657.pdf)

- (arXiv 2021.02) Is Space-Time Attention All You Need for Video Understanding? [[Paper]](https://arxiv.org/pdf/2102.05095.pdf)

- (arXiv 2021.02) Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling, [[Paper]](https://arxiv.org/pdf/2102.06183.pdf), [[Code]](https://github.com/jayleicn/ClipBERT)

- (arXiv 2021.02) Video Transformer Network, [[Paper]](https://arxiv.org/pdf/2102.00719.pdf)

- (arXiv 2021.02) Training Vision Transformers for Image Retrieval, [[Paper]](https://arxiv.org/pdf/2102.05644.pdf)

- (arXiv 2021.02) Relaxed Transformer Decoders for Direct Action Proposal Generation, [[Paper]](https://arxiv.org/pdf/2102.01894.pdf), [[Code]](https://github.com/MCG-NJU/RTD-Action)

- (arXiv 2021.02) TransReID: Transformer-based Object Re-Identification, [[Paper]](https://arxiv.org/pdf/2102.04378.pdf)

- (arXiv 2021.02) Improving Visual Reasoning by Exploiting The Knowledge in Texts, [[Paper]](https://arxiv.org/pdf/2102.04760.pdf)

- (arXiv 2021.01) Fast Convergence of DETR with Spatially Modulated Co-Attention, [[Paper]](https://arxiv.org/pdf/2101.07448.pdf)

- (arXiv 2021.01) Dual-Level Collaborative Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2101.06462.pdf)

- (arXiv 2021.01) SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation (arXiv 2021.1), [[Paper]](https://arxiv.org/pdf/2101.08833.pdf)

- (arXiv 2021.01) CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING, [[Paper]](https://arxiv.org/pdf/2101.10804.pdf)

- (arXiv 2021.01) Trans2Seg: Transparent Object Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2101.08461), [[Code]](https://github.com/xieenze/Trans2Seg)

- (arXiv 2021.01) Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2101.11562.pdf), [[Code]](https://github.com/YehLi/TDEN)

- (arXiv 2021.01) Trear: Transformer-based RGB-D Egocentric Action Recognition, [[Paper]](https://arxiv.org/pdf/2101.03904.pdf)

- (arXiv 2021.01) Learn to Dance with AIST++: Music Conditioned 3D Dance Generation, [[Paper]](https://arxiv.org/pdf/2101.08779), [[Page]](https://google.github.io/aichoreographer/;)

- (arXiv 2021.01) Spherical Transformer: Adapting Spherical Signal to CNNs, [[Paper]](https://arxiv.org/pdf/2101.03848.pdf)

- (arXiv 2021.01) Are We There Yet? Learning to Localize in Embodied Instruction Following, [[Paper]](https://arxiv.org/pdf/2101.03431.pdf)

- (arXiv 2021.01) VinVL: Making Visual Representations Matter in Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2101.00529.pdf)

- (arXiv 2021.01) Bottleneck Transformers for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2101.11605.pdf)

- (arXiv 2021.01) Investigating the Vision Transformer Model for Image Retrieval Tasks, [[Paper]](https://arxiv.org/pdf/2101.03771)

- (arXiv 2021.01) ADDRESSING SOME LIMITATIONS OF TRANSFORMERS WITH FEEDBACK MEMORY, [[Paper]](https://arxiv.org/pdf/2002.09402.pdf)

- (arXiv 2021.01) Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, [[Paper]](https://arxiv.org/pdf/2101.11986.pdf), [[Code]](https://github.com/yitu-opensource/T2T-ViT)

- (arXiv 2021.01) TrackFormer: Multi-Object Tracking with Transformers, [[Paper]](https://arxiv.org/pdf/2101.02702)

- (arXiv 2021.01) VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search, [[Paper]](https://arxiv.org/pdf/2101.00265)

- (arXiv 2021.01) Line Segment Detection Using Transformers without Edges, [[Paper]](https://arxiv.org/pdf/2101.01909)

- (arXiv 2021.01) Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2102.00529.pdf)

- (arXiv 2020.12) Accurate Word Representations with Universal Visual Guidance, [[Paper]](https://arxiv.org/pdf/2012.15086.pdf)

- (arXiv 2020.12) DETR for Pedestrian Detection, [[Paper]](https://arxiv.org/pdf/2012.06785)

- (arXiv 2020.12) Transformer Interpretability Beyond Attention Visualization, [[Paper]](https://arxiv.org/pdf/2012.09838), [[Code]](https://github.com/hila-chefer/Transformer-Explainability)

- (arXiv 2020.12) PCT: Point Cloud Transformer, [[Paper]](https://arxiv.org/pdf/2012.09688)

- (arXiv 2020.12) TransPose: Towards Explainable Human Pose Estimation by Transformer, [[Paper]](https://arxiv.org/pdf/2012.14214)

- (arXiv 2020.12) Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers, [[Paper]](https://arxiv.org/pdf/2012.15840), [[Code]](https://github.com/fudan-zvg/SETR)

- (arXiv 2020.12) Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry, [[Paper]](https://arxiv.org/pdf/2101.02143)

- (arXiv 2020.12) Transformer for Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2101.01097), [[Code]](https://github.com/junyongyou/triq)

- (arXiv 2020.12) TransTrack: Multiple-Object Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2012.15460), [[Code]](https://github.com/PeizeSun/TransTrack)

- (arXiv 2020.12) 3D Object Detection with Pointformer, [[Paper]](https://arxiv.org/pdf/2012.11409)

- (arXiv 2020.12) Training data-efficient image transformers & distillation through attention, [[Paper]](https://arxiv.org/pdf/2012.12877)

- (arXiv 2020.12) Toward Transformer-Based Object Detection, [[Paper]](https://arxiv.org/pdf/2012.09958)

- (arXiv 2020.12) SceneFormer: Indoor Scene Generation with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09793)

- (arXiv 2020.12) Point Transformer, [[Paper]](https://arxiv.org/pdf/2012.09164)

- (arXiv 2020.12) End-to-End Human Pose and Mesh Reconstruction with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09760)

- (arXiv 2020.12) Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting, [[Paper]](https://arxiv.org/pdf/2012.07436.pdf)

- (arXiv 2020.12) Pre-Trained Image Processing Transformer, [[Paper]](https://arxiv.org/pdf/2012.00364)

- (arXiv 2020.12) Taming Transformers for High-Resolution Image Synthesis, [[Paper]](https://arxiv.org/pdf/2012.09841.pdf), [[Code]](https://github.com/CompVis/taming-transformers)

- (arXiv 2020.11) End-to-end Lane Shape Prediction with Transformers, [[Paper]](https://arxiv.org/pdf/2011.04233), [[Code]](https://github.com/liuruijin17/LSTR)

- (arXiv 2020.11) UP-DETR: Unsupervised Pre-training for Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2011.09094)

- (arXiv 2020.11) End-to-End Video Instance Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14503)

- (arXiv 2020.11) Rethinking Transformer-based Set Prediction for Object Detection, [[Paper]](https://arxiv.org/pdf/2011.10881)

- (arXiv 2020.11) General Multi-label Image Classification with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14027}

- (arXiv 2020.11) End-to-End Object Detection with Adaptive Clustering Transformer, [[Paper]](https://arxiv.org/pdf/2011.09315)

- (arXiv 2020.10) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, [[Paper]](https://arxiv.org/pdf/2010.11929), [[Code]](https://github.com/google-research/vision_transformer)

- (arXiv 2020.07) Oscar: Object-Semantics Aligned Pre-training for Vision-and-Language Tasks, [[Paper]](https://arxiv.org/pdf/2004.06165.pdf), [[Code]](https://github.com/microsoft/Oscar)

- (arXiv 2020.07) Feature Pyramid Transformer, [[Paper]](https://arxiv.org/pdf/2007.09451), [[Code]](https://github.com/ZHANGDONG-NJUST/FPT)

- (arXiv 2020.06) Visual Transformers: Token-based Image Representation and Processing for Computer Vision, [[Paper]](https://arxiv.org/pdf/2006.03677)

- (arXiv 2019.08) LXMERT: Learning Cross-Modality Encoder Representations from Transformers, [[Paper]](https://arxiv.org/pdf/1908.07490.pdf), [[Code]](https://github.com/airsplay/lxmert)

- (ICLR'21) UPDET: UNIVERSAL MULTI-AGENT REINFORCEMENT LEARNING VIA POLICY DECOUPLING WITH TRANSFORMERS, [[Paper]](https://arxiv.org/pdf/2101.08001.pdf), [[Code]](https://github.com/hhhusiyi-monash/UPDeT)

- (ICLR'21) Deformable DETR: Deformable Transformers for End-to-End Object Detection, [[Paper]](https://arxiv.org/pdf/2010.04159), [[Code]](https://github.com/fundamentalvision/Deformable-DETR)

- (ICLR'21) LAMBDANETWORKS: MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION, [[Paper]](https://openreview.net/pdf?id=xTJEN-ggl1b), [[Code]](https://github.com/lucidrains/lambda-networks)

- (ICLR'21) SUPPORT-SET BOTTLENECKS FOR VIDEO-TEXT REPRESENTATION LEARNING, [[Paper]](https://arxiv.org/pdf/2010.02824.pdf)

- (ICLR'21) COLORIZATION TRANSFORMER, [[Paper]](https://arxiv.org/pdf/2102.04432.pdf), [[Code]](https://github.com/google-research/google-research/tree/master/coltran)

- (ECCV'20) Multi-modal Transformer for Video Retrieval, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490205.pdf)

- (ECCV'20) Connecting Vision and Language with Localized Narratives, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500630.pdf)

- (ECCV'20) DETR: End-to-End Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2005.12872), [[Code]](https://github.com/facebookresearch/detr)

- (CVPR'20) Multi-Modality Cross Attention Network for Image and Sentence Matching, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Multi-Modality_Cross_Attention_Network_for_Image_and_Sentence_Matching_CVPR_2020_paper.pdf), [[Page]](https://www.robots.ox.ac.uk/~vgg/research/speech2action/)

- (CVPR'20) Learning Texture Transformer Network for Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2006.04139), [[Code]](https://github.com/researchmm/TTSR)

- (CVPR'20) Speech2Action: Cross-modal Supervision for Action Recognition, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagrani_Speech2Action_Cross-Modal_Supervision_for_Action_Recognition_CVPR_2020_paper.pdf)

- (ICPR'20) Transformer Encoder Reasoning Network, [[Paper]](https://arxiv.org/pdf/2004.09144.pdf), [[Code]](https://github.com/mesnico/TERN)

- (EMNLP'19) Effective Use of Transformer Networks for Entity Tracking, [[Paper]](https://arxiv.org/pdf/1909.02635), [[Code]](https://github.com/aditya2211/transformer-entity-tracking)

## TODO
- [ ] V-L representation learning
